{
  "name": "ASASF Master Protocol Generation Workflow (v3.0)",
  "description": "Generates a comprehensive, timestamped package of the current ResonantiA Protocol, workflows, SPRs, code manifest, and key instructions. This is Arche's self-definition actualization.",
  "version": "3.0",
  "tasks": {
    "initiate_asf_master_gen": {
      "description": "Initiate ASASF Master Protocol Generation process.",
      "action": "display_output",
      "inputs": {
        "content": "Starting ASASF Master Protocol Generation..."
      },
      "dependencies": []
    },
    "create_timestamped_output_dir": {
      "description": "Create a timestamped output directory for this generation run.",
      "action": "execute_code",
      "inputs": {
        "input_data": { 
            "asf_base_output_dir": "{{ initial_context.asf_base_output_dir | default('outputs/ASASF_Snapshots') }}"
        }
      },
      "action_config": {
        "language": "python",
        "code": "import os\nimport time\nimport json\nimport sys\n\ninput_str = sys.stdin.read()\ninput_json = {} # Default to empty dict\nwarning_message = None\nif input_str:\n    try:\n        input_json = json.loads(input_str)\n    except json.JSONDecodeError as e:\n        warning_message = f'JSONDecodeError on stdin: {e}. Proceeding with default input_json.'\n        pass # input_json remains {}\nelif not input_str:\n    warning_message = 'Script received empty stdin, used default for input_json.'\n\nbase_output_dir = input_json.get('asf_base_output_dir', 'outputs/ASASF_Snapshots')\ntimestamp = time.strftime('%Y%m%d_%H%M%S')\nrun_output_dir = os.path.join(base_output_dir, f'ResonantiA_Canonical_Self_Definition_{timestamp}')\nos.makedirs(run_output_dir, exist_ok=True)\nresult = {'run_output_dir': run_output_dir, 'timestamp_str': timestamp, 'status_message': f'Created ASASF output directory: {run_output_dir}'}\nif warning_message:\n    result['script_warning'] = warning_message\n\nsys.stdout.write(json.dumps(result))"
      },
      "outputs": {"run_output_dir": "string", "timestamp_str": "string", "reflection": "dict"},
      "dependencies": ["initiate_asf_master_gen"]
    },
    "assemble_full_protocol_document": {
      "description": "Instruct Arche (LLM) to re-assemble and output the full ResonantiA Protocol v3.0 document text based on its current understanding.",
      "action": "generate_text_llm",
      "inputs": {
        "prompt": "Keyholder `Keyy=('IMnDEVmode')` requests the full, verbatim output of the *current, complete, and enhanced* ResonantiA Protocol v3.0 document. This document should be assembled from your internal understanding of all sections previously generated and refined through our interactions, including: Preamble, Story Synopsis, Section 1 (Mandatory Directives), Section 2 (Conceptual Framework), Section 3 (Operational Framework - already enhanced), Section 4 (Setup), Section 5 (Core Principles), Section 6 (Security/Ethics), Section 7 (Code/Workflows - including all .py files like config.py, main.py, workflow_engine.py, action_registry.py, spr_manager.py, all tool files with their IAR-compliant functional code, and all workflow .json files including ReSSyD/ASASF ones, and the full spr_definitions_tv.json), Section 8 (Advanced Interaction Patterns, including SOMA-Pattern 8.8), Section 9 (README.md), Section 10 (Hosting/Monetization), Section 11 (Repo Setup), and Section 12 (Wiki Guidance). Ensure all v3.0 enhancements (IAR, Temporal Reasoning, Meta-Cognition, SIRC, SOMA, SynergyAnalysisTool, Keyholder Override implications) are fully integrated and explicitly detailed as previously discussed and generated. The output must be a single, continuous Markdown formatted text block. Start with 'ResonantiA Protocol v3.0 (IAR & Temporal Integration - Canonical Self-Definition - Generated {{ create_timestamped_output_dir.timestamp_str }})\\n\\n**PREAMBLE (Enhanced v3.0):**\\n...'. This is a high-priority internal system documentation and self-actualization task for the ASASF.",
        "max_tokens": 128000,
        "model": "{{ initial_context.documentation_llm_model | default(None) }}"
      },
      "outputs": {"response_text": "string", "reflection": "dict"},
      "dependencies": ["create_timestamped_output_dir"],
      "condition": "{{ create_timestamped_output_dir.reflection.status == 'Success' }}"
    },
    "save_protocol_document": {
      "description": "Save the assembled protocol document to a Markdown file.",
      "action": "execute_code",
      "inputs": {
         "input_data":{
            "protocol_markdown_string": "{{ assemble_full_protocol_document.response_text }}",
            "run_output_dir": "{{ create_timestamped_output_dir.run_output_dir }}",
            "timestamp_str": "{{ create_timestamped_output_dir.timestamp_str }}"
         }
      },
      "action_config": {
        "language": "python",
        "code": "import os\nimport json\nimport sys\n\n# Load input data from stdin\ninput_str = sys.stdin.read()\ninput_json = {} # Default to empty dict\nif input_str:\n    try:\n        input_json = json.loads(input_str)\n    except json.JSONDecodeError as e:\n        input_json = {}\n\nprotocol_content = input_json.get('protocol_markdown_string', '# ERROR: Protocol content not generated by LLM.')\noutput_dir = input_json.get('run_output_dir', 'outputs/ASASF_Snapshots/unknown_run')\ntimestamp_str = input_json.get('timestamp_str', 'timestamp')\nfilename = f'ResonantiA_Protocol_v3.0_Canonical_{timestamp_str}.md'\nfilepath = os.path.join(output_dir, filename)\n\ntry:\n    with open(filepath, 'w', encoding='utf-8') as f:\n        f.write(protocol_content)\n    print(f'Canonical Protocol document saved to: {filepath}')\n    result = {'protocol_filepath': filepath, 'status_message': 'Canonical Protocol document saved.'}\nexcept Exception as e:\n    print(f'Error saving protocol document: {e}')\n    result = {'protocol_filepath': None, 'status_message': f'Error saving protocol: {e}', 'error': str(e)}\n\n# Output result to stdout for Arche engine to capture\nsys.stdout.write(json.dumps(result))"
      },
      "outputs": {"protocol_filepath": "string", "status_message": "string", "error": "string", "reflection": "dict"},
      "dependencies": ["assemble_full_protocol_document"],
      "condition": "{{ assemble_full_protocol_document.reflection.status == 'Success' }}"
    },
    "package_workflows_actual": {
      "description": "Copy all workflow JSON files from the primary workflow directory to the ASASF output directory.",
      "action": "execute_code",
      "inputs": {
        "input_data":{
            "run_output_dir": "{{ create_timestamped_output_dir.run_output_dir }}"
        }
      },
      "action_config": {
        "language": "python",
        "code": "import os\nimport shutil\nimport json\nimport sys\n\n# Dynamically locate ArchE package and config\ntry:\n    from Three_PointO_ArchE import config as arche_config\nexcept ModuleNotFoundError:\n    # Fallback if running from a different context, try to find relative to script\n    # This is less robust and assumes a certain directory structure if not run via `python -m`\n    current_script_path = os.path.dirname(os.path.abspath(__file___dummy_for_path)) # __file__ not directly available in exec\n    project_root = os.path.abspath(os.path.join(current_script_path, '..', '..')) # Heuristic: two levels up from a script in a task\n    sys.path.insert(0, project_root) # Add to path to find Three_PointO_ArchE\n    try:\n        from Three_PointO_ArchE import config as arche_config\n    except ImportError:\n        # If still not found, use a default or fail gracefully\n        print('CRITICAL: Could not locate ArchE config for WORKFLOW_DIR. Using default ./workflows')\n        class PlaceholderConfig:\n            WORKFLOW_DIR = './workflows'\n        arche_config = PlaceholderConfig()\n\n# Load input data from stdin\ninput_str = sys.stdin.read()\ninput_json = {} # Default to empty dict\nif input_str:\n    try:\n        input_json = json.loads(input_str)\n    except json.JSONDecodeError as e:\n        input_json = {}\n\nsource_workflow_dir = arche_config.WORKFLOW_DIR\noutput_dir = input_json.get('run_output_dir', 'outputs/ASASF_Snapshots/unknown_run')\ndest_workflow_dir = os.path.join(output_dir, 'workflows')\nos.makedirs(dest_workflow_dir, exist_ok=True)\n\ncopied_files = []\nerrors = []\nif os.path.isdir(source_workflow_dir):\n    for filename in os.listdir(source_workflow_dir):\n        if filename.endswith('.json'):\n            try:\n                shutil.copy2(os.path.join(source_workflow_dir, filename), dest_workflow_dir)\n                copied_files.append(filename)\n            except Exception as e:\n                errors.append(f'Error copying {filename}: {e}')\n    print(f'Copied {len(copied_files)} workflow files from {source_workflow_dir} to {dest_workflow_dir}.')\n    result = {'packaged_workflows_path': dest_workflow_dir, 'copied_count': len(copied_files), 'copy_errors': errors if errors else None}\nelse:\n    error_msg = f'Source workflow directory not found: {source_workflow_dir}'\n    print(error_msg)\n    result = {'packaged_workflows_path': None, 'error': error_msg}\n\nsys.stdout.write(json.dumps(result))"
      },
      "outputs": {"packaged_workflows_path": "string", "copied_count": "int", "copy_errors": "list", "reflection": "dict"},
      "dependencies": ["create_timestamped_output_dir"]
    },
    "package_knowledge_graph_actual": {
      "description": "Copy the primary SPR definitions file to the ASASF output directory.",
      "action": "execute_code",
      "inputs": {
        "input_data":{
             "run_output_dir": "{{ create_timestamped_output_dir.run_output_dir }}"
        }
      },
      "action_config": {
        "language": "python",
        "code": "import os\nimport shutil\nimport json\nimport sys\n\n# Dynamically locate ArchE package and config\ntry:\n    from Three_PointO_ArchE import config as arche_config\nexcept ModuleNotFoundError:\n    current_script_path = os.path.dirname(os.path.abspath(__file__dummy_for_path)) \n    project_root = os.path.abspath(os.path.join(current_script_path, '..', '..')) \n    sys.path.insert(0, project_root) \n    try:\n        from Three_PointO_ArchE import config as arche_config\n    except ImportError:\n        print('CRITICAL: Could not locate ArchE config for SPR_JSON_FILE. Using default ./knowledge_graph/spr_definitions_tv.json')\n        class PlaceholderConfig:\n            SPR_JSON_FILE = './knowledge_graph/spr_definitions_tv.json'\n        arche_config = PlaceholderConfig()\n\n# Load input data from stdin\ninput_str = sys.stdin.read()\ninput_json = {} # Default to empty dict\nif input_str:\n    try:\n        input_json = json.loads(input_str)\n    except json.JSONDecodeError as e:\n        input_json = {}\n\nsource_spr_file = arche_config.SPR_JSON_FILE\noutput_dir = input_json.get('run_output_dir', 'outputs/ASASF_Snapshots/unknown_run')\ndest_kg_dir = os.path.join(output_dir, 'knowledge_graph')\nos.makedirs(dest_kg_dir, exist_ok=True)\n\nif os.path.isfile(source_spr_file):\n    try:\n        dest_file_path = os.path.join(dest_kg_dir, os.path.basename(source_spr_file))\n        shutil.copy2(source_spr_file, dest_file_path)\n        print(f'Copied SPR definitions file from {source_spr_file} to {dest_file_path}.')\n        result = {'packaged_spr_file_path': dest_file_path}\n    except Exception as e:\n        result = {'packaged_spr_file_path': None, 'error': f'Error copying SPR file: {e}'}\nelse:\n    error_msg = f'Source SPR file not found: {source_spr_file}'\n    print(error_msg)\n    result = {'packaged_spr_file_path': None, 'error': error_msg}\n\nsys.stdout.write(json.dumps(result))"
      },
      "outputs": {"packaged_spr_file_path": "string", "error": "string", "reflection": "dict"},
      "dependencies": ["create_timestamped_output_dir"]
    },
    "generate_code_manifest_actual": {
      "description": "Generate a manifest of the ArchE Python codebase.",
      "action": "execute_code",
       "inputs": { "input_data": {} },
      "action_config": {
        "language": "python",
        "code": "import os\nimport json\nimport sys\n\n# Dynamically locate ArchE package and config\ntry:\n    from Three_PointO_ArchE import config as arche_config\nexcept ModuleNotFoundError:\n    current_script_path = os.path.dirname(os.path.abspath(__file__dummy_for_path))\n    project_root = os.path.abspath(os.path.join(current_script_path, '..', '..'))\n    sys.path.insert(0, project_root)\n    try:\n        from Three_PointO_ArchE import config as arche_config\n    except ImportError:\n        print('CRITICAL: Could not locate ArchE config for MASTERMIND_DIR. Using default ./Three_PointO_ArchE')\n        class PlaceholderConfig:\n            MASTERMIND_DIR = './Three_PointO_ArchE'\n        arche_config = PlaceholderConfig()\n\ncode_dir = arche_config.MASTERMIND_DIR\n\nfile_list = []\nignored_dirs = ['__pycache__', '.pytest_cache', 'logs', 'outputs', 'knowledge_graph', 'workflows', 'venv', '.git']\nignored_files = ['.DS_Store']\nstatus_message = \"\"\n\nif os.path.isdir(code_dir):\n    for root, dirs, files in os.walk(code_dir):\n        dirs[:] = [d for d in dirs if d not in ignored_dirs]\n        for file in files:\n            if file.endswith('.py') and file not in ignored_files:\n                file_list.append(os.path.relpath(os.path.join(root, file), code_dir))\n    status_message = f'Generated manifest of {len(file_list)} .py files from {code_dir}'\n    result = {'code_manifest': sorted(file_list), 'source_package_dir': code_dir, 'status_message': status_message}\nelse:\n    status_message = f'Code directory {code_dir} not found.'\n    result = {'code_manifest': [], 'error': status_message, 'status_message': status_message}\nsys.stdout.write(json.dumps(result))"
      },
      "outputs": {"code_manifest": "list", "source_package_dir": "string", "error": "string", "reflection": "dict"},
      "dependencies": ["create_timestamped_output_dir"]
    },
    "synthesize_instructions_manual_actual": {
      "description": "Generate a 'HowTo_Install_Use_Engage.md' document using LLM.",
      "action": "generate_text_llm",
      "inputs": {
        "prompt": "Keyholder `Keyy=('IMnDEVmode')` requests a synthesized 'HowTo_Install_Use_Engage.md' document for the ResonantiA v3.0 project. \nBased on the ResonantiA Protocol v3.0 (which you, Arche, have full knowledge of, including Section 4 for Setup, Section 9 for README summary, Section 8 for Advanced Interaction Patterns, and general usage principles like IAR interpretation, and the actual project structure with the 'Three_PointO_ArchE' package name):\n1. Provide clear, step-by-step installation instructions (Python, venv, `pip install -r requirements.txt`, API key configuration emphasizing security via environment variables, Docker for CodeExecutor if `CODE_EXECUTOR_SANDBOX_METHOD` is 'docker').\n2. Explain basic usage: how to run a workflow via `python -m Three_PointO_ArchE.main ...`, how to interpret JSON outputs, and specifically how to find and understand the IAR `reflection` dictionary for each task.\n3. Briefly describe how to engage with advanced features: leveraging IAR in workflows, using temporal tools (Prediction, Causal, ABM), initiating meta-cognitive patterns (SIRC, Metacognitive Shift, Insight Solidification), and the purpose/risks of Keyholder Override.\n4. Provide guidance on creating a new project instance from scratch based on the protocol (directory structure, core file population from this ASASF package).\nFormat this as a clear, well-structured Markdown document. This document will be part of the ASASF Canonical Self-Definition Package.",
        "max_tokens": 4000,
        "model": "{{ initial_context.documentation_llm_model | default(None) }}"
      },
      "outputs": {"response_text": "string", "reflection": "dict"},
      "dependencies": ["create_timestamped_output_dir"]
    },
    "save_generated_artifacts": {
      "description": "Save all generated artifacts (Protocol MD, Instructions MD, Code Manifest JSON) to the timestamped output directory.",
      "action": "execute_code",
      "inputs": {
        "input_data": {
            "run_output_dir": "{{ create_timestamped_output_dir.run_output_dir }}",
            "timestamp_str": "{{ create_timestamped_output_dir.timestamp_str }}",
            "protocol_markdown_string": "{{ assemble_full_protocol_document.response_text }}",
            "instructions_markdown": "{{ synthesize_instructions_manual_actual.response_text }}",
            "code_manifest": "{{ generate_code_manifest_actual.code_manifest }}",
            "packaged_workflows_path": "{{ package_workflows_actual.packaged_workflows_path }}",
            "packaged_spr_file_path": "{{ package_knowledge_graph_actual.packaged_spr_file_path }}"
        }
      },
      "action_config": {
        "language": "python",
        "code": "import os\nimport json\nimport sys\n\n# Load input data from stdin\ninput_str = sys.stdin.read()\ninput_json = {} # Default to empty dict\nif input_str:\n    try:\n        input_json = json.loads(input_str)\n    except json.JSONDecodeError as e:\n        input_json = {}\n\noutput_dir = input_json.get('run_output_dir', 'outputs/ASASF_Snapshots/unknown_run')\ntimestamp_str = input_json.get('timestamp_str', 'timestamp')\n\nartifacts_saved = {}\nerrors = []\n\n# Save Protocol Document\nprotocol_content = input_json.get('protocol_markdown_string', '# ERROR: Protocol content not generated.')\nprotocol_filename = f'ResonantiA_Protocol_v3.0_Canonical_{timestamp_str}.md'\nprotocol_filepath = os.path.join(output_dir, protocol_filename)\ntry:\n    with open(protocol_filepath, 'w', encoding='utf-8') as f: f.write(protocol_content)\n    artifacts_saved['protocol_document'] = protocol_filepath\nexcept Exception as e: errors.append(f'Error saving protocol: {e}')\n\n# Save Instructions Document\ninstructions_content = input_json.get('instructions_markdown', '# ERROR: Instructions not generated.')\ninstructions_filename = f'HowTo_Install_Use_Engage_ResonantiA_v3_{timestamp_str}.md'\ninstructions_filepath = os.path.join(output_dir, instructions_filename)\ntry:\n    with open(instructions_filepath, 'w', encoding='utf-8') as f: f.write(instructions_content)\n    artifacts_saved['instructions_document'] = instructions_filepath\nexcept Exception as e: errors.append(f'Error saving instructions: {e}')\n\n# Save Code Manifest\ncode_manifest_data = input_json.get('code_manifest', [])\nmanifest_filename = f'ArchE_Code_Manifest_{timestamp_str}.json'\nmanifest_filepath = os.path.join(output_dir, manifest_filename)\ntry:\n    with open(manifest_filepath, 'w', encoding='utf-8') as f: json.dump(code_manifest_data, f, indent=2)\n    artifacts_saved['code_manifest'] = manifest_filepath\nexcept Exception as e: errors.append(f'Error saving manifest: {e}')\n\n# Workflows and SPRs are copied in their respective tasks\nartifacts_saved['packaged_workflows_path'] = input_json.get('packaged_workflows_path')\nartifacts_saved['packaged_spr_file_path'] = input_json.get('packaged_spr_file_path')\n\nfinal_message = f'ASASF Master Package artifacts processed. Output directory: {output_dir}'\nif errors: final_message += f'\\nErrors encountered: {errors}'\nprint(final_message)\nresult = {'package_path': output_dir, 'artifacts_saved': artifacts_saved, 'processing_errors': errors if errors else None}\nsys.stdout.write(json.dumps(result))"
      },
      "outputs": {"package_path": "string", "artifacts_saved": "dict", "processing_errors": "list", "reflection": "dict"},
      "dependencies": [
        "assemble_full_protocol_document",
        "package_workflows_actual",
        "package_knowledge_graph_actual",
        "generate_code_manifest_actual",
        "synthesize_instructions_manual_actual"
      ]
    },
    "final_asf_summary_display": {
        "description": "Display final status of ASASF Master Protocol Generation.",
        "action": "display_output",
        "inputs": {
            "content": {
                "asf_master_gen_status": "{{ save_generated_artifacts.reflection.status }}",
                "output_package_location": "{{ save_generated_artifacts.package_path }}",
                "artifacts_summary": "{{ save_generated_artifacts.artifacts_saved }}",
                "errors_if_any": "{{ save_generated_artifacts.processing_errors }}"
            },
            "format": "json"
        },
        "dependencies": ["save_generated_artifacts"]
    }
  }
} 