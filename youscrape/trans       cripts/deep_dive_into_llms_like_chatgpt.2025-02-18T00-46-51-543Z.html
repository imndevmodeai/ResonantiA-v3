
<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>Deep Dive into LLMs like ChatGPT - Transcript</title>
    <script src="https://unpkg.com/react@17/umd/react.development.js"></script>
    <script src="https://unpkg.com/react-dom@17/umd/react-dom.development.js"></script>
    <script src="https://unpkg.com/babel-standalone@6/babel.min.js"></script>
    <style>
        
    body {
        font-family: Arial, sans-serif;
        max-width: 800px;
        margin: 0 auto;
        padding: 20px;
        line-height: 1.6;
    }
    
    .header {
        text-align: center;
        margin-bottom: 30px;
    }
    
    .header a {
        text-decoration: none;
        color: inherit;
    }
    
    .thumbnail {
        max-width: 100%;
        height: auto;
        border-radius: 8px;
        margin: 20px 0;
    }
    
    .timestamps {
        margin: 20px 0;
    }
    
    .timestamp {
        display: inline-block;
        margin: 5px;
        padding: 5px 10px;
        background-color: #f0f0f0;
        border-radius: 4px;
        text-decoration: none;
        color: #333;
    }
    
    .timestamp:hover {
        background-color: #e0e0e0;
    }
    
    .transcript-text {
        margin-top: 30px;
    }
    
    button {
        padding: 10px 20px;
        margin: 10px 0;
        background-color: #007bff;
        color: white;
        border: none;
        border-radius: 4px;
        cursor: pointer;
    }
    
    button:hover {
        background-color: #0056b3;
    }
    
    button:disabled {
        background-color: #cccccc;
        cursor: not-allowed;
    }
    
    input[type="text"] {
        padding: 10px;
        margin-right: 10px;
        width: 300px;
        border: 1px solid #ddd;
        border-radius: 4px;
    }
    
    form {
        margin-bottom: 20px;
    }

    </style>
</head>
<body>
    <div id="root"></div>
    <script type="text/babel">
        // TranscriptViewer Component
        const TranscriptViewer = ({ videoInfo, transcriptData }) => {
            const downloadTranscript = () => {
                const text = transcriptData.map(segment => segment.text).join(' ');
                const blob = new Blob([text], { type: 'text/plain' });
                const url = URL.createObjectURL(blob);
                const a = document.createElement('a');
                a.href = url;
                a.download = `${videoInfo.title}_transcript.txt`;
                document.body.appendChild(a);
                a.click();
                document.body.removeChild(a);
                URL.revokeObjectURL(url);
            };

            return (
                <div>
                    <div className="header">
                        <a href={videoInfo.url} target="_blank" rel="noopener noreferrer">
                            <h1>{videoInfo.title}</h1>
                            <img className="thumbnail" src={videoInfo.thumbnail} alt={videoInfo.title} />
                        </a>
                    </div>
                    
                    <div className="timestamps">
                        <h2>Timestamps:</h2>
                        {transcriptData.map((segment, index) => {
                            const timeInSeconds = function convertTimestampToSeconds(timestamp) {
    const parts = timestamp.split(':').reverse();
    let seconds = 0;
    for (let i = 0; i < parts.length; i++) {
        seconds += parseInt(parts[i]) * Math.pow(60, i);
    }
    return seconds;
}(segment.timestamp);
                            return (
                                <a 
                                    key={index}
                                    className="timestamp"
                                    href={`${videoInfo.url}&t=${timeInSeconds}`}
                                    target="_blank"
                                    rel="noopener noreferrer"
                                >
                                    {segment.timestamp}
                                </a>
                            );
                        })}
                    </div>

                    <div className="transcript-text">
                        <h2>Full Transcript:</h2>
                        <button onClick={downloadTranscript}>Download Transcript</button>
                        <p>{transcriptData.map(segment => segment.text).join(' ')}</p>
                    </div>
                </div>
            );
        };

        // URL Input Component
        const URLInput = ({ onSubmit }) => {
            const [url, setUrl] = React.useState('');
            const [loading, setLoading] = React.useState(false);

            const handleSubmit = async (e) => {
                e.preventDefault();
                setLoading(true);
                await onSubmit(url);
                setLoading(false);
            };

            return (
                <form onSubmit={handleSubmit}>
                    <input
                        type="text"
                        value={url}
                        onChange={(e) => setUrl(e.target.value)}
                        placeholder="Enter YouTube URL"
                        disabled={loading}
                    />
                    <button type="submit" disabled={loading}>
                        {loading ? 'Loading...' : 'Get Transcript'}
                    </button>
                </form>
            );
        };

        // Main App Component
        const App = () => {
            const [transcriptData, setTranscriptData] = React.useState([{"timestamp":"0:00","text":"hi everyone so I've wanted to make this video for a while it is a comprehensive but General audience introduction to"},{"timestamp":"0:08","text":"large language models like Chachi PT and what I'm hoping to achieve in this video is to give you kind of mental models for"},{"timestamp":"0:14","text":"thinking through what it is that this tool is it is obviously magical and"},{"timestamp":"0:19","text":"amazing in some respects it's uh really good at some things not very good at other things and there's also a lot of"},{"timestamp":"0:25","text":"sharp edges to be aware of so what is behind this text box you can put anything in there and press enter but uh"},{"timestamp":"0:32","text":"what should we be putting there and what are these words generated back how does this work and what what are you talking"},{"timestamp":"0:38","text":"to exactly so I'm hoping to get at all those topics in this video we're going to go through the entire pipeline of how"},{"timestamp":"0:44","text":"this stuff is built but I'm going to keep everything uh sort of accessible to a general audience so let's take a look"},{"timestamp":"0:50","text":"at first how you build something like chpt and along the way I'm going to talk about um you know some of the sort of"},{"timestamp":"0:56","text":"cognitive psychological implications of the tools okay so let's build Chachi PT"},{"timestamp":"1:02","text":"so there's going to be multiple stages arranged sequentially the first stage is called the pre-training stage and the"},{"timestamp":"1:09","text":"first step of the pre-training stage is to download and process the internet now to get a sense of what this roughly"},{"timestamp":"1:14","text":"looks like I recommend looking at this URL here so um this company called"},{"timestamp":"1:20","text":"hugging face uh collected and created and curated this data set called Fine"},{"timestamp":"1:26","text":"web and they go into a lot of detail on this block post on how how they constructed the fine web data set and"},{"timestamp":"1:32","text":"all of the major llm providers like open AI anthropic and Google and so on will have some equivalent internally of"},{"timestamp":"1:38","text":"something like the fine web data set so roughly what are we trying to achieve here we're trying to get ton of text"},{"timestamp":"1:44","text":"from the internet from publicly available sources so we're trying to have a huge quantity of very high"},{"timestamp":"1:50","text":"quality documents and we also want very large diversity of documents because we want to have a lot of knowledge inside"},{"timestamp":"1:56","text":"these models so we want large diversity of high quality documents and we want many many of them and achieving this is"},{"timestamp":"2:04","text":"uh quite complicated and as you can see here takes multiple stages to do well so let's take a look at what some of these"},{"timestamp":"2:09","text":"stages look like in a bit for now I'd like to just like to note that for example the fine web data set which is fairly representative what you would see"},{"timestamp":"2:16","text":"in a production grade application actually ends up being only about 44 terabyt of dis space um you can get a"},{"timestamp":"2:23","text":"USB stick for like a terabyte very easily or I think this could fit on a single hard drive almost today so this"},{"timestamp":"2:29","text":"is not a huge amount of data at the end of the day even though the internet is very very large we're working with text"},{"timestamp":"2:35","text":"and we're also filtering it aggressively so we end up with about 44 terabytes in this example so let's take a look at uh"},{"timestamp":"2:42","text":"kind of what this data looks like and what some of these stages uh also are so the starting point for a lot of these"},{"timestamp":"2:48","text":"efforts and something that contributes most of the data by the end of it is Data from common crawl so common craw is"},{"timestamp":"2:56","text":"an organization that has been basically scouring the internet since 2007 so as of 2024 for example common CW has"},{"timestamp":"3:03","text":"indexed 2.7 billion web pages uh and uh they have all these crawlers going around the internet and"},{"timestamp":"3:09","text":"what you end up doing basically is you start with a few seed web pages and then you follow all the links and you just"},{"timestamp":"3:15","text":"keep following links and you keep indexing all the information and you end up with a ton of data of the internet over time so this is usually the"},{"timestamp":"3:21","text":"starting point for a lot of the uh for a lot of these efforts now this common C data is quite raw and is filtered in"},{"timestamp":"3:27","text":"many many different ways so here they Pro they document this is"},{"timestamp":"3:33","text":"the same diagram they document a little bit the kind of processing that happens in these stages so the first thing here"},{"timestamp":"3:39","text":"is something called URL filtering so what that is referring to is that there's these block"},{"timestamp":"3:47","text":"lists of uh basically URLs that are or domains that uh you don't want to be"},{"timestamp":"3:52","text":"getting data from so usually this includes things like U malware websites spam websites marketing websites uh"},{"timestamp":"3:58","text":"racist websites adult sites and things like that so there's a ton of different types of websites that are just"},{"timestamp":"4:04","text":"eliminated at this stage because we don't want them in our data set um the second part is text extraction you have"},{"timestamp":"4:10","text":"to remember that all these web pages this is the raw HTML of these web pages that are being saved by these crawlers"},{"timestamp":"4:16","text":"so when I go to inspect here this is what the raw HTML actually looks like you'll notice that it's got"},{"timestamp":"4:23","text":"all this markup uh like lists and stuff like that and there's CSS and all this"},{"timestamp":"4:28","text":"kind of stuff so this is um computer code almost for these web pages but what we really want is we just want this text"},{"timestamp":"4:35","text":"right we just want the text of this web page and we don't want the navigation and things like that so there's a lot of filtering and processing uh and heris"},{"timestamp":"4:42","text":"that go into uh adequately filtering for just their uh good content of these web"},{"timestamp":"4:48","text":"pages the next stage here is language filtering so for example fine web"},{"timestamp":"4:53","text":"filters uh using a language classifier they try to guess what language every single web page is in and then they only"},{"timestamp":"5:00","text":"keep web pages that have more than 65% of English as an example and so you can get a sense that"},{"timestamp":"5:06","text":"this is like a design decision that different companies can uh can uh take for themselves what fraction of all"},{"timestamp":"5:12","text":"different types of languages are we going to include in our data set because for example if we filter out all of the Spanish as an example then you might"},{"timestamp":"5:19","text":"imagine that our model later will not be very good at Spanish because it's just never seen that much data of that language and so different companies can"},{"timestamp":"5:26","text":"focus on multilingual performance to uh to a different degree as an example so fine web is quite focused on English and"},{"timestamp":"5:33","text":"so their language model if they end up training one later will be very good at English but not may be very good at"},{"timestamp":"5:38","text":"other languages after language filtering there's a few other filtering steps and D duplication and things like that um"},{"timestamp":"5:46","text":"finishing with for example the pii removal this is personally identifiable"},{"timestamp":"5:52","text":"information so as an example addresses Social Security numbers and things like that you would try to detect them and"},{"timestamp":"5:57","text":"you would try to filter out those kinds of web pages from the the data set as well so there's a lot of stages here and I won't go into full detail but it is a"},{"timestamp":"6:05","text":"fairly extensive part of the pre-processing and you end up with for example the fine web data set so when"},{"timestamp":"6:10","text":"you click in on it uh you can see some examples here of what this actually ends up looking like and anyone can download"},{"timestamp":"6:16","text":"this on the huging phase web page and so here are some examples of the final text"},{"timestamp":"6:21","text":"that ends up in the training set so this is some article about tornadoes in"},{"timestamp":"6:27","text":"2012 um so there's some t tadoes in 2020 in 2012 and what"},{"timestamp":"6:33","text":"happened uh this next one is something about did you know you have two little"},{"timestamp":"6:38","text":"yellow 9vt battery sized adrenal glands in your body okay so this is some kind"},{"timestamp":"6:43","text":"of a odd medical article so just think of these as"},{"timestamp":"6:49","text":"basically uh web pages on the internet filtered just for the text in various ways and now we have a ton of text 40"},{"timestamp":"6:56","text":"terabytes off it and that now is the starting point for the next step of this stage now I wanted to give you an"},{"timestamp":"7:02","text":"intuitive sense of where we are right now so I took the first 200 web pages here and remember we have tons of them"},{"timestamp":"7:09","text":"and I just take all that text and I just put it all together concatenate it and so this is what we end up with we just"},{"timestamp":"7:15","text":"get this just just raw text raw internet text and there's a ton of it even in"},{"timestamp":"7:20","text":"these 200 web pages so I can continue zooming out here and we just have this like massive tapestry of Text data and"},{"timestamp":"7:28","text":"this text data has all these p patterns and what we want to do now is we want to start training neural networks on this"},{"timestamp":"7:33","text":"data so the neural networks can internalize and model how this text"},{"timestamp":"7:39","text":"flows right so we just have this giant texture of text and now we want to get"},{"timestamp":"7:45","text":"neural Nets that mimic it okay now before we plug text into neural networks"},{"timestamp":"7:51","text":"we have to decide how we're going to represent this text uh and how we're going to feed it in now the way our"},{"timestamp":"7:57","text":"technology works for these neuron Lots is that they expect a one-dimensional sequence of symbols"},{"timestamp":"8:02","text":"and they want a finite set of symbols that are possible and so we have to"},{"timestamp":"8:08","text":"decide what are the symbols and then we have to represent our data as one-dimensional sequence of those"},{"timestamp":"8:14","text":"symbols so right now what we have is a onedimensional sequence of text it starts here and it goes here and then it"},{"timestamp":"8:20","text":"comes here Etc so this is a onedimensional sequence even though on my monitor of course it's laid out in a"},{"timestamp":"8:26","text":"two-dimensional way but it goes from left to right and top to bottom right so it's a one-dimensional sequence of text"},{"timestamp":"8:32","text":"now this being computers of course there's an underlying representation here so if I do what's called utf8 uh"},{"timestamp":"8:38","text":"encode this text then I can get the raw bits that correspond to this text in the"},{"timestamp":"8:44","text":"computer and that's what uh that looks like this so it turns out that for"},{"timestamp":"8:50","text":"example this very first bar here is the first uh eight bits as an"},{"timestamp":"8:56","text":"example so what is this thing right this is um representation that we are looking"},{"timestamp":"9:01","text":"for uh in in a certain sense we have exactly two possible symbols zero and one and we have a very long sequence of"},{"timestamp":"9:10","text":"it right now as it turns out um this sequence length is actually going to be"},{"timestamp":"9:16","text":"very finite and precious resource uh in our neural network and we actually don't want extremely long sequences of just"},{"timestamp":"9:23","text":"two symbols instead what we want is we want to trade off uh this um symbol"},{"timestamp":"9:29","text":"size uh of this vocabulary as we call it and the resulting sequence length so we"},{"timestamp":"9:35","text":"don't want just two symbols and extremely long sequences we're going to want more symbols and shorter sequences"},{"timestamp":"9:42","text":"okay so one naive way of compressing or decreasing the length of our sequence here is to basically uh consider some"},{"timestamp":"9:49","text":"group of consecutive bits for example eight bits and group them into a single"},{"timestamp":"9:54","text":"what's called bite so because uh these bits are either on or off if we take a"},{"timestamp":"10:00","text":"group of eight of them there turns out to be only 256 possible combinations of how these bits could be on or off and so"},{"timestamp":"10:06","text":"therefore we can re repesent this sequence into a sequence of bytes instead so this sequence of bytes will"},{"timestamp":"10:13","text":"be eight times shorter but now we have 256 possible symbols so every number"},{"timestamp":"10:19","text":"here goes from 0 to 255 now I really encourage you to think of these not as numbers but as unique"},{"timestamp":"10:25","text":"IDs or like unique symbols so maybe it's a bit more maybe it's better to actually"},{"timestamp":"10:30","text":"think of these to replace every one of these with a unique Emoji you'd get something like this so um we basically"},{"timestamp":"10:37","text":"have a sequence of emojis and there's 256 possible emojis you can think of it that way now it turns out that in"},{"timestamp":"10:44","text":"production for state-of-the-art language models uh you actually want to go even Beyond this you want to continue to"},{"timestamp":"10:50","text":"shrink the length of the sequence uh because again it is a precious resource in return for more symbols in your"},{"timestamp":"10:57","text":"vocabulary and the way this is done is done by running what's called The Bite pair encoding algorithm and the way this"},{"timestamp":"11:04","text":"works is we're basically looking for consecutive bytes or symbols that are"},{"timestamp":"11:10","text":"very common so for example turns out that the sequence 116 followed by 32 is"},{"timestamp":"11:17","text":"quite common and occurs very frequently so what we're going to do is we're going to group uh this um pair into a new"},{"timestamp":"11:24","text":"symbol so we're going to Mint a symbol with an ID 256 and we're going to rewrite every single uh pair 11632 with"},{"timestamp":"11:32","text":"this new symbol and then can we can iterate this algorithm as many times as we wish and each time when we mint a new"},{"timestamp":"11:38","text":"symbol we're decreasing the length and we're increasing the symbol size and in practice it turns out that a pretty good"},{"timestamp":"11:45","text":"setting of um the basically the vocabulary size turns out to be about 100,000 possible symbols so in"},{"timestamp":"11:52","text":"particular GPT 4 uses 100, 277 symbols"},{"timestamp":"11:59","text":"um and this process of converting from raw text into these symbols or as we"},{"timestamp":"12:07","text":"call them tokens is the process called tokenization so let's now take a look at"},{"timestamp":"12:12","text":"how gp4 performs tokenization conting from text to tokens and from tokens back"},{"timestamp":"12:18","text":"to text and what this actually looks like so one website I like to use to explore these token representations is"},{"timestamp":"12:24","text":"called tick tokenizer and so come here to the drop down and select CL 100 a base which is the gp4 base model"},{"timestamp":"12:32","text":"tokenizer and here on the left you can put in text and it shows you the tokenization of that text so for example"},{"timestamp":"12:40","text":"heo space world so hello world turns out to be"},{"timestamp":"12:46","text":"exactly two Tokens The Token hello which is the token with ID"},{"timestamp":"12:51","text":"15339 and the token space world that is the token 1"},{"timestamp":"12:57","text":"1917 so um hello space world now if I was to join these two for example I'm"},{"timestamp":"13:04","text":"going to get again two tokens but it's the token H followed by the token L world without the"},{"timestamp":"13:11","text":"H um if I put in two Spa two spaces here between hello and world it's again a"},{"timestamp":"13:16","text":"different uh tokenization there's a new token 220"},{"timestamp":"13:22","text":"here okay so you can play with this and see what happens here also keep in mind this is not uh this is case sensitive so"},{"timestamp":"13:28","text":"if this is a capital H it is something else or if it's uh hello world then"},{"timestamp":"13:35","text":"actually this ends up being three tokens instead of just two"},{"timestamp":"13:41","text":"tokens yeah so you can play with this and get an sort of like an intuitive sense of uh what these tokens work like"},{"timestamp":"13:47","text":"we're actually going to loop around to tokenization a bit later in the video for now I just wanted to show you the website and I wanted to uh show you that"},{"timestamp":"13:53","text":"this text basically at the end of the day so for example if I take one line here this is what GT4 will see it as so"},{"timestamp":"14:01","text":"this text will be a sequence of length 62 this is the sequence here and this is"},{"timestamp":"14:08","text":"how the chunks of text correspond to these symbols and again there's 100,"},{"timestamp":"14:16","text":"27777 possible symbols and we now have one-dimensional sequences of those"},{"timestamp":"14:21","text":"symbols so um yeah we're going to come back to tokenization but that's uh for now where we are okay so what I've done"},{"timestamp":"14:28","text":"now is I've taken this uh sequence of text that we have here in the data set and I have re-represented it using our"},{"timestamp":"14:34","text":"tokenizer into a sequence of tokens and this is what that looks like now so for"},{"timestamp":"14:40","text":"example when we go back to the Fine web data set they mentioned that not only is this 44 terab of dis space but this is"},{"timestamp":"14:45","text":"about a 15 trillion token sequence of um in this data set and so here these are"},{"timestamp":"14:53","text":"just some of the first uh one or two or three or a few thousand here I think uh tokens of this data set but there's 15"},{"timestamp":"15:01","text":"trillion here uh to keep in mind and again keep in mind one more time that all of these represent little text"},{"timestamp":"15:07","text":"chunks they're all just like atoms of these sequences and the numbers here don't make any sense they're just uh"},{"timestamp":"15:13","text":"they're just unique IDs okay so now we get to the fun part which is the uh"},{"timestamp":"15:19","text":"neural network training and this is where a lot of the heavy lifting happens computationally when you're training these neural networks so what we do here"},{"timestamp":"15:28","text":"in this this step is we want to model the statistical relationships of how these tokens follow each other in the"},{"timestamp":"15:33","text":"sequence so what we do is we come into the data and we take Windows of tokens"},{"timestamp":"15:40","text":"so we take a window of tokens uh from this data fairly randomly and um the windows length can"},{"timestamp":"15:49","text":"range anywhere anywhere between uh zero tokens actually all the way up to some"},{"timestamp":"15:54","text":"maximum size that we decide on uh so for example in practice you could see a token with Windows of say 8,000 tokens"},{"timestamp":"16:01","text":"now in principle we can use arbitrary window lengths of tokens uh but uh"},{"timestamp":"16:07","text":"processing very long uh basically U window sequences would just be very"},{"timestamp":"16:12","text":"computationally expensive so we just kind of decide that say 8,000 is a good number or 4,000 or 16,000 and we crop it"},{"timestamp":"16:19","text":"there now in this example I'm going to be uh taking the first four tokens just"},{"timestamp":"16:25","text":"so everything fits nicely so these tokens we're going to take a window of four"},{"timestamp":"16:32","text":"tokens this bar view in and space single"},{"timestamp":"16:37","text":"which are these token IDs and now what we're trying to do here is we're trying to basically predict the"},{"timestamp":"16:42","text":"token that comes next in the sequence so 3962 comes next right so what we do now"},{"timestamp":"16:49","text":"here is that we call this the context these four tokens are context and they"},{"timestamp":"16:54","text":"feed into a neural network and this is the input to the neural network"},{"timestamp":"16:59","text":"now I'm going to go into the detail of what's inside this neural network in a little bit for now it's important to understand is the input and the output"},{"timestamp":"17:06","text":"of the neural net so the input are sequences of tokens of variable length"},{"timestamp":"17:12","text":"anywhere between zero and some maximum size like 8,000 the output now is a"},{"timestamp":"17:17","text":"prediction for what comes next so because our vocabulary has"},{"timestamp":"17:23","text":"100277 possible tokens the neural network is going to Output exactly that many numbers"},{"timestamp":"17:29","text":"and all of those numbers correspond to the probability of that token as coming next in the sequence so it's making"},{"timestamp":"17:35","text":"guesses about what comes next um in the beginning this neural network is randomly initialized so um"},{"timestamp":"17:42","text":"and we're going to see in a little bit what that means but it's a it's a it's a random transformation so these"},{"timestamp":"17:48","text":"probabilities in the very beginning of the training are also going to be kind of random uh so here I have three"},{"timestamp":"17:53","text":"examples but keep in mind that there's 100,000 numbers here um so the probability of this token space"},{"timestamp":"17:59","text":"Direction neural network is saying that this is 4% likely right now 11799 is 2%"},{"timestamp":"18:05","text":"and then here the probility of 3962 which is post is 3% now of course we've"},{"timestamp":"18:11","text":"sampled this window from our data set so we know what comes next we know and that's the label we know that the"},{"timestamp":"18:18","text":"correct answer is that 3962 actually comes next in the sequence so now what we have is this mathematical process for"},{"timestamp":"18:25","text":"doing an update to the neural network we have the way of tuning it and uh we're going to go into a little bit of of"},{"timestamp":"18:32","text":"detail in a bit but basically we know that this probability here of 3% we want"},{"timestamp":"18:38","text":"this probability to be higher and we want the probabilities of all the other tokens to be"},{"timestamp":"18:44","text":"lower and so we have a way of mathematically calculating how to adjust and update the neural network so that"},{"timestamp":"18:51","text":"the correct answer has a slightly higher probability so if I do an update to the neural network now the next time I Fe"},{"timestamp":"18:59","text":"this particular sequence of four tokens into neural network the neural network will be slightly adjusted now and it will say Okay post is maybe 4% and case"},{"timestamp":"19:07","text":"now maybe is 1% and uh Direction could become 2% or something like that and so we have a way"},{"timestamp":"19:14","text":"of nudging of slightly updating the neuronet to um basically give a higher"},{"timestamp":"19:19","text":"probability to the correct token that comes next in the sequence and now you just have to remember that this process"},{"timestamp":"19:25","text":"happens not just for uh this um token here where these four fed in and"},{"timestamp":"19:31","text":"predicted this one this process happens at the same time for all of these tokens in the entire data set and so in"},{"timestamp":"19:38","text":"practice we sample little windows little batches of Windows and then at every single one of these tokens we want to"},{"timestamp":"19:44","text":"adjust our neural network so that the probability of that token becomes slightly higher and this all happens in"},{"timestamp":"19:50","text":"parallel in large batches of these tokens and this is the process of training the neural network it's a"},{"timestamp":"19:55","text":"sequence of updating it so that it's predictions match up the statistics of"},{"timestamp":"20:01","text":"what actually happens in your training set and its probabilities become consistent with the uh statistical"},{"timestamp":"20:08","text":"patterns of how these tokens follow each other in the data so let's now briefly get into the internals of these neural"},{"timestamp":"20:13","text":"networks just to give you a sense of what's inside so neural network internals so as I mentioned we have"},{"timestamp":"20:19","text":"these inputs uh that are sequences of tokens in this case this is four input"},{"timestamp":"20:24","text":"tokens but this can be anywhere between zero up to let's say 8,000 tokens in"},{"timestamp":"20:30","text":"principle this can be an infinite number of tokens we just uh it would just be too computationally expensive to process"},{"timestamp":"20:35","text":"an infinite number of tokens so we just crop it at a certain length and that becomes the maximum context length of"},{"timestamp":"20:41","text":"that uh model now these inputs X are mixed up in a giant mathematical expression together"},{"timestamp":"20:48","text":"with the parameters or the weights of these neural networks so here I'm showing six example parameters and their"},{"timestamp":"20:56","text":"setting but in practice these uh um modern neural networks will have"},{"timestamp":"21:01","text":"billions of these uh parameters and in the beginning these parameters are completely randomly set now with a"},{"timestamp":"21:09","text":"random setting of parameters you might expect that this uh this neural network would make random predictions and it"},{"timestamp":"21:15","text":"does in the beginning it's totally random predictions but it's through this process of iteratively updating the"},{"timestamp":"21:22","text":"network uh as and we call that process training a neural network so uh that the"},{"timestamp":"21:28","text":"setting of these parameters gets adjusted such that the outputs of our neural network becomes consistent with"},{"timestamp":"21:34","text":"the patterns seen in our training set so think of these parameters as kind"},{"timestamp":"21:39","text":"of like knobs on a DJ set and as you're twiddling these knobs you're getting different uh predictions for every"},{"timestamp":"21:45","text":"possible uh token sequence input and training in neural network just means"},{"timestamp":"21:50","text":"discovering a setting of parameters that seems to be consistent with the statistics of the training"},{"timestamp":"21:56","text":"set now let me just give you an example what this giant mathematical expression looks like just to give you a sense and"},{"timestamp":"22:01","text":"modern networks are massive expressions with trillions of terms probably but let me just show you a simple example here"},{"timestamp":"22:08","text":"it would look something like this I mean these are the kinds of Expressions just to show you that it's not very scary we"},{"timestamp":"22:13","text":"have inputs x uh like X1 x2 in this case two example inputs and they get mixed up"},{"timestamp":"22:19","text":"with the weights of the network w0 W1 2 3 Etc and this mixing is simple things"},{"timestamp":"22:27","text":"like multiplication addition addition exponentiation division Etc and it is"},{"timestamp":"22:32","text":"the subject of neural network architecture research to design effective mathematical Expressions uh"},{"timestamp":"22:39","text":"that have a lot of uh kind of convenient characteristics they are expressive they're optimizable they're paralyzable"},{"timestamp":"22:45","text":"Etc and so but uh at the end of the day these are these are not complex expressions and basically they mix up"},{"timestamp":"22:52","text":"the inputs with the parameters to make predictions and we're optimizing uh the"},{"timestamp":"22:57","text":"parameters of this neural network so that the predictions come out consistent with the training set now I would like"},{"timestamp":"23:04","text":"to show you an actual production grade example of what these neural networks look like so for that I encourage you to"},{"timestamp":"23:09","text":"go to this website that has a very nice visualization of one of these networks so this is what you will find"},{"timestamp":"23:16","text":"on this website and this neural network here that is used in production settings"},{"timestamp":"23:21","text":"has this special kind of structure this network is called the Transformer and this particular one as an example has 8"},{"timestamp":"23:28","text":"5,000 roughly parameters now here on the top we take the inputs which are the token"},{"timestamp":"23:36","text":"sequences and then information flows through the neural network until the"},{"timestamp":"23:41","text":"output which here are the logit softmax but these are the predictions for what comes next what token comes"},{"timestamp":"23:48","text":"next and then here there's a sequence of Transformations and all these"},{"timestamp":"23:54","text":"intermediate values that get produced inside this mathematical expression s it is sort of predicting what comes next so"},{"timestamp":"24:01","text":"as an example these tokens are embedded into kind of like this distributed representation as it's called so every"},{"timestamp":"24:08","text":"possible token has kind of like a vector that represents it inside the neural network so first we embed the tokens and"},{"timestamp":"24:15","text":"then those values uh kind of like flow through this diagram and these are all"},{"timestamp":"24:20","text":"very simple mathematical Expressions individually so we have layer norms and Matrix multiplications and uh soft Maxes"},{"timestamp":"24:27","text":"and so on so here kind of like the attention block of this Transformer and then information kind of flows through"},{"timestamp":"24:33","text":"into the multi-layer perceptron block and so on and all these numbers here these are the intermediate values of the"},{"timestamp":"24:40","text":"expression and uh you can almost think of these as kind of like the firing rates of these synthetic neurons but I"},{"timestamp":"24:47","text":"would caution you to uh not um kind of think of it too much like neurons because these are extremely simple"},{"timestamp":"24:53","text":"neurons compared to the neurons you would find in your brain your biological neurons are very complex dynamical"},{"timestamp":"24:59","text":"processes that have memory and so on there's no memory in this expression it's a fixed mathematical expression"},{"timestamp":"25:04","text":"from input to Output with no memory it's just a stateless so these are very simple neurons in comparison to biological"},{"timestamp":"25:10","text":"neurons but you can still kind of loosely think of this as like a synthetic piece of uh brain tissue if"},{"timestamp":"25:15","text":"you if you like uh to think about it that way so information flows through"},{"timestamp":"25:21","text":"all these neurons fire until we get to the predictions now I'm not actually"},{"timestamp":"25:26","text":"going to dwell too much on the precise kind of like mathematical details of all these Transformations honestly I don't"},{"timestamp":"25:31","text":"think it's that important to get into what's really important to understand is that this is a mathematical function it"},{"timestamp":"25:38","text":"is uh parameterized by some fixed set of parameters like say 85,000 of them and"},{"timestamp":"25:44","text":"it is a way of transforming inputs into outputs and as we twiddle the parameters we are getting uh different kinds of"},{"timestamp":"25:50","text":"predictions and then we need to find a good setting of these parameters so that the predictions uh sort of match up with"},{"timestamp":"25:56","text":"the patterns seen in training set so that's the Transformer okay so I've"},{"timestamp":"26:02","text":"shown you the internals of the neural network and we talked a bit about the process of training it I want to cover"},{"timestamp":"26:07","text":"one more major stage of working with these networks and that is the stage called inference so in inference what"},{"timestamp":"26:14","text":"we're doing is we're generating new data from the model and so uh we want to basically see what kind of patterns it"},{"timestamp":"26:21","text":"has internalized in the parameters of its Network so to generate from the"},{"timestamp":"26:26","text":"model is relatively straightforward we start with some tokens that are basically your prefix like what you want"},{"timestamp":"26:32","text":"to start with so say we want to start with the token 91 well we feed it into the"},{"timestamp":"26:37","text":"network and remember that the network gives us probabilities right it gives us"},{"timestamp":"26:43","text":"this probability Vector here so what we can do now is we can basically flip a biased coin so um we can sample uh"},{"timestamp":"26:52","text":"basically a token based on this probability distribution so the tokens"},{"timestamp":"26:57","text":"that are given High probability by the model are more likely to be sampled when you flip this biased coin you can think"},{"timestamp":"27:03","text":"of it that way so we sample from the distribution to get a single unique token so for example token 860 comes"},{"timestamp":"27:11","text":"next uh so 860 in this case when we're generating from model could come next now 860 is a relatively likely token it"},{"timestamp":"27:18","text":"might not be the only possible token in this case there could be many other tokens that could have been sampled but we could see that 86c is a relatively"},{"timestamp":"27:25","text":"likely token as an example and indeed in our training examp example here 860 does follow 91 so let's now say that we um"},{"timestamp":"27:34","text":"continue the process so after 91 there's a60 we append it and we again ask what"},{"timestamp":"27:39","text":"is the third token let's sample and let's just say that it's 287 exactly as"},{"timestamp":"27:44","text":"here let's do that again we come back in now we have a sequence of three and we"},{"timestamp":"27:49","text":"ask what is the likely fourth token and we sample from that and get this one and"},{"timestamp":"27:55","text":"now let's say we do it one more time we take those four we sample and we get this one and this"},{"timestamp":"28:02","text":"13659 uh this is not actually uh 3962 as we had before so this token is the token"},{"timestamp":"28:09","text":"article uh instead so viewing a single article and so in this case we didn't"},{"timestamp":"28:15","text":"exactly reproduce the sequence that we saw here in the training data so keep in mind that these systems are stochastic"},{"timestamp":"28:22","text":"they have um we're sampling and we're flipping coins and sometimes we lock out"},{"timestamp":"28:28","text":"and we reproduce some like small chunk of the text and training set but sometimes we're uh we're getting a token"},{"timestamp":"28:35","text":"that was not verbatim part of any of the documents in the training data so we're going to get sort of like remixes of the"},{"timestamp":"28:43","text":"data that we saw in the training because at every step of the way we can flip and get a slightly different token and then"},{"timestamp":"28:48","text":"once that token makes it in if you sample the next one and so on you very quickly uh start to generate token"},{"timestamp":"28:55","text":"streams that are very different from the token streams that UR in the training documents so"},{"timestamp":"29:00","text":"statistically they will have similar properties but um they are not identical to your training data they're kind of"},{"timestamp":"29:06","text":"like inspired by the training data and so in this case we got a slightly different sequence and why would we get"},{"timestamp":"29:12","text":"article you might imagine that article is a relatively likely token in the context of bar viewing single Etc and"},{"timestamp":"29:21","text":"you can imagine that the word article followed this context window somewhere in the training documents uh to some"},{"timestamp":"29:26","text":"extent and we just happen to sample it here at that stage so basically inference is just uh predicting from"},{"timestamp":"29:33","text":"these distributions one at a time we continue feeding back tokens and getting the next one and we uh we're always"},{"timestamp":"29:39","text":"flipping these coins and depending on how lucky or unlucky we get um we might"},{"timestamp":"29:45","text":"get very different kinds of patterns depending on how we sample from these probability distributions so that's"},{"timestamp":"29:51","text":"inference so in most common scenarios uh basically downloading the internet and"},{"timestamp":"29:57","text":"tokenizing it is is a pre-processing step you do that a single time and then uh once you have your token sequence we"},{"timestamp":"30:04","text":"can start training networks and in Practical cases you would try to train many different networks of different"},{"timestamp":"30:10","text":"kinds of uh settings and different kinds of arrangements and different kinds of sizes and so you''ll be doing a lot of"},{"timestamp":"30:15","text":"neural network training and um then once you have a neural network and you train it and you have some specific set of"},{"timestamp":"30:21","text":"parameters that you're happy with um then you can take the model and you can do inference and you can actually uh"},{"timestamp":"30:28","text":"generate data from the model and when you're on chat GPT and you're talking with a model uh that model is trained"},{"timestamp":"30:33","text":"and has been trained by open aai many months ago probably and they have a specific set of Weights that work well"},{"timestamp":"30:41","text":"and when you're talking to the model all of that is just inference there's no more training those parameters are held"},{"timestamp":"30:47","text":"fixed and you're just talking to the model sort of uh you're giving it some of the tokens and it's kind of"},{"timestamp":"30:53","text":"completing token sequences and that's what you're seeing uh generated when you actually use the model on CH GPT so that"},{"timestamp":"30:59","text":"model then just does inference alone so let's now look at an example of training an inference that is kind of concrete"},{"timestamp":"31:05","text":"and gives you a sense of what this actually looks like uh when these models are trained now the example that I would like to work with and that I'm"},{"timestamp":"31:12","text":"particularly fond of is that of opening eyes gpt2 so GPT uh stands for"},{"timestamp":"31:17","text":"generatively pre-trained Transformer and this is the second iteration of the GPT series by open AI when you are talking"},{"timestamp":"31:23","text":"to chat GPT today the model that is underlying all of the magic of that interaction is GPT 4 so the fourth"},{"timestamp":"31:30","text":"iteration of that series now gpt2 was published in 2019 by openi in this paper"},{"timestamp":"31:36","text":"that I have right here and the reason I like gpt2 is that it is the first time"},{"timestamp":"31:41","text":"that a recognizably modern stack came together so um all of the pieces of gpd2"},{"timestamp":"31:48","text":"are recognizable today by modern standards it's just everything has gotten bigger now I'm not going to be"},{"timestamp":"31:54","text":"able to go into the full details of this paper of course because it is a technical publication but some of the"},{"timestamp":"31:59","text":"details that I would like to highlight are as follows gpt2 was a Transformer neural network just like you were just"},{"timestamp":"32:05","text":"like the neural networks you would work with today it was it had 1.6 billion parameters right so these are the"},{"timestamp":"32:12","text":"parameters that we looked at here it would have 1.6 billion of them today modern Transformers would have a lot"},{"timestamp":"32:18","text":"closer to a trillion or several hundred billion probably the maximum context length here"},{"timestamp":"32:24","text":"was 1,24 tokens so it is when we are sampling chunks of Windows of tokens"},{"timestamp":"32:32","text":"from the data set we're never taking more than 1,24 tokens and so when you are trying to predict the next token in"},{"timestamp":"32:38","text":"a sequence you will never have more than 1,24 tokens uh kind of in your context"},{"timestamp":"32:43","text":"in order to make that prediction now this is also tiny by modern standards today the token uh the context lengths"},{"timestamp":"32:49","text":"would be a lot closer to um couple hundred thousand or maybe even a million"},{"timestamp":"32:55","text":"and so you have a lot more context a lot more tokens in history history and you can make a lot better prediction about"},{"timestamp":"33:00","text":"the next token in the sequence in that way and finally gpt2 was trained on approximately 100 billion tokens and"},{"timestamp":"33:06","text":"this is also fairly small by modern standards as I mentioned the fine web data set that we looked at here the fine"},{"timestamp":"33:12","text":"web data set has 15 trillion tokens uh so 100 billion is is quite small"},{"timestamp":"33:18","text":"now uh I actually tried to reproduce uh gpt2 for fun as part of this project called lm. C so you can see my rup of"},{"timestamp":"33:27","text":"doing that in this post on GitHub under the lm. C repository so in particular"},{"timestamp":"33:33","text":"the cost of training gpd2 in 2019 what was estimated to be approximately"},{"timestamp":"33:39","text":"$40,000 but today you can do significantly better than that and in particular here it took about one day"},{"timestamp":"33:45","text":"and about $600 uh but this wasn't even trying too hard I think you could really bring this"},{"timestamp":"33:51","text":"down to about $100 today now why is it that the costs have come down so much"},{"timestamp":"33:57","text":"well number one these data sets have gotten a lot better and the way we filter them extract them and prepare"},{"timestamp":"34:03","text":"them has gotten a lot more refined and so the data set is of just a lot higher quality so that's one thing but really"},{"timestamp":"34:10","text":"the biggest difference is that our computers have gotten much faster in terms of the hardware and we're going to look at that in a second and also the"},{"timestamp":"34:17","text":"software for uh running these models and really squeezing out all all the speed"},{"timestamp":"34:22","text":"from the hardware as it is possible uh that software has also gotten much better as as everyone has focused on"},{"timestamp":"34:28","text":"these models and try to run them very very quickly now I'm not going to be able to"},{"timestamp":"34:34","text":"go into the full detail of this gpd2 reproduction and this is a long technical post but I would like to still"},{"timestamp":"34:39","text":"give you an intuitive sense for what it looks like to actually train one of these models as a researcher like what are you looking at and what does it look"},{"timestamp":"34:46","text":"like what does it feel like so let me give you a sense of that a little bit okay so this is what it looks like let me slide this"},{"timestamp":"34:52","text":"over so what I'm doing here is I'm training a gpt2 model right now"},{"timestamp":"34:58","text":"and um what's happening here is that every single line here like this one is"},{"timestamp":"35:05","text":"one update to the model so remember how here we are um basically making the"},{"timestamp":"35:12","text":"prediction better for every one of these tokens and we are updating these weights or parameters of the neural net so here"},{"timestamp":"35:18","text":"every single line is One update to the neural network where we change its parameters by a little bit so that it is"},{"timestamp":"35:24","text":"better at predicting next token and sequence in particular every single line here is improving the prediction on 1"},{"timestamp":"35:32","text":"million tokens in the training set so we've basically taken 1 million tokens"},{"timestamp":"35:39","text":"out of this data set and we've tried to improve the prediction of that token as"},{"timestamp":"35:44","text":"coming next in a sequence on all 1 million of them simultaneously and at every single one"},{"timestamp":"35:51","text":"of these steps we are making an update to the network for that now the number to watch closely is this number called"},{"timestamp":"35:57","text":"loss and the loss is a single number that is telling you how well your neural network is performing right now and it"},{"timestamp":"36:05","text":"is created so that low loss is good so you'll see that the loss is decreasing"},{"timestamp":"36:10","text":"as we make more updates to the neural nut which corresponds to making better predictions on the next token in a"},{"timestamp":"36:16","text":"sequence and so the loss is the number that you are watching as a neural network researcher and you are kind of"},{"timestamp":"36:22","text":"waiting you're twiddling your thumbs uh you're drinking coffee and you're making sure that this looks good so that with"},{"timestamp":"36:28","text":"every update your loss is improving and the network is getting better at prediction now here you see that we are"},{"timestamp":"36:36","text":"processing 1 million tokens per update each update takes about 7 Seconds"},{"timestamp":"36:41","text":"roughly and here we are going to process a total of 32,000 steps of"},{"timestamp":"36:47","text":"optimization so 32,000 steps with 1 million tokens each is about 33 billion"},{"timestamp":"36:52","text":"tokens that we are going to process and we're currently only about 420 step 20"},{"timestamp":"36:57","text":"out of 32,000 so we are still only a bit more than 1% done because I've only been"},{"timestamp":"37:03","text":"running this for 10 or 15 minutes or something like that now every 20 steps I have"},{"timestamp":"37:09","text":"configured this optimization to do inference so what you're seeing here is the model is predicting the next token"},{"timestamp":"37:15","text":"in a sequence and so you sort of start it randomly and then you continue plugging in the tokens so we're running"},{"timestamp":"37:21","text":"this inference step and this is the model sort of predicting the next token in the sequence and every time you see something appear that's a new"},{"timestamp":"37:29","text":"token um so let's just look at this and"},{"timestamp":"37:34","text":"you can see that this is not yet very coherent and keep in mind that this is only 1% of the way through training and"},{"timestamp":"37:39","text":"so the model is not yet very good at predicting the next token in the sequence so what comes out is actually"},{"timestamp":"37:44","text":"kind of a little bit of gibberish right but it still has a little bit of like local coherence so since she is mine"},{"timestamp":"37:51","text":"it's a part of the information should discuss my father great companions Gordon showed me sitting over at and Etc"},{"timestamp":"37:59","text":"so I know it doesn't look very good but let's actually scroll up and see what it"},{"timestamp":"38:04","text":"looked like when I started the optimization so all the way here at"},{"timestamp":"38:10","text":"step one so after 20 steps of optimization you see that what we're getting here is"},{"timestamp":"38:17","text":"looks completely random and of course that's because the model has only had 20 updates to its parameters and so it's"},{"timestamp":"38:22","text":"giving you random text because it's a random Network and so you can see that at least in comparison to this model is"},{"timestamp":"38:27","text":"starting to do much better and indeed if we waited the entire 32,000 steps the model will have improved the point that"},{"timestamp":"38:34","text":"it's actually uh generating fairly coherent English uh and the tokens stream correctly um and uh they they"},{"timestamp":"38:42","text":"kind of make up English a a lot better um so this has to run for about a day or"},{"timestamp":"38:49","text":"two more now and so uh at this stage we just make sure that the loss is decreasing everything is looking good um"},{"timestamp":"38:56","text":"and we just have to wait and now um let me turn now to the um"},{"timestamp":"39:02","text":"story of the computation that's required because of course I'm not running this optimization on my laptop that would be"},{"timestamp":"39:08","text":"way too expensive uh because we have to run this neural network and we have to improve it and we have we need all this"},{"timestamp":"39:14","text":"data and so on so you can't run this too well on your computer uh because the network is just too large uh so all of"},{"timestamp":"39:21","text":"this is running on the computer that is out there in the cloud and I want to basically address the compute side of"},{"timestamp":"39:27","text":"the store of training these models and what that looks like so let's take a look okay so the computer that I'm"},{"timestamp":"39:32","text":"running this optimization on is this 8X h100 node so there are eight h100s in a"},{"timestamp":"39:39","text":"single node or a single computer now I am renting this computer and it is somewhere in the cloud I'm not sure"},{"timestamp":"39:45","text":"where it is physically actually the place I like to rent from is called Lambda but there are many other companies who provide this service so"},{"timestamp":"39:52","text":"when you scroll down you can see that uh they have some on demand pricing for"},{"timestamp":"39:57","text":"um sort of computers that have these uh h100s which are gpus and I'm going to"},{"timestamp":"40:03","text":"show you what they look like in a second but on demand 8times Nvidia h100 uh"},{"timestamp":"40:10","text":"GPU this machine comes for $3 per GPU per hour for example so you can rent"},{"timestamp":"40:16","text":"these and then you get a machine in a cloud and you can uh go in and you can train these models and these uh gpus they look like"},{"timestamp":"40:25","text":"this so this is one h100 GPU uh this is kind of what it looks like and you slot this into your computer and gpus are"},{"timestamp":"40:32","text":"this uh perfect fit for training your networks because they are very computationally expensive but they"},{"timestamp":"40:38","text":"display a lot of parallelism in the computation so you can have many independent workers kind of um working"},{"timestamp":"40:44","text":"all at the same time in solving uh the matrix multiplication that's under the"},{"timestamp":"40:50","text":"hood of training these neural networks so this is just one of these h100s but actually you would put them"},{"timestamp":"40:56","text":"you would put multiple of them together so you could stack eight of them into a single node and then you can stack"},{"timestamp":"41:02","text":"multiple nodes into an entire data center or an entire system so when we look at a data"},{"timestamp":"41:12","text":"center can't spell when we look at a data center we start to see things that look like this right so we have one GPU"},{"timestamp":"41:18","text":"goes to eight gpus goes to a single system goes to many systems and so these are the bigger data centers and there of"},{"timestamp":"41:23","text":"course would be much much more expensive um and what's happening is that all the big tech companies really desire these"},{"timestamp":"41:31","text":"gpus so they can train all these language models because they are so powerful and that has is fundamentally"},{"timestamp":"41:37","text":"what has driven the stock price of Nvidia to be $3.4 trillion today as an example and why Nvidia has kind of"},{"timestamp":"41:44","text":"exploded so this is the Gold Rush the Gold Rush is getting the gpus getting"},{"timestamp":"41:50","text":"enough of them so they can all collaborate to perform this optimization"},{"timestamp":"41:55","text":"and they're what are they all doing they're all collaborating to predict the next token on a data set like the fine"},{"timestamp":"42:01","text":"web data set this is the computational workflow that that basically is extremely"},{"timestamp":"42:06","text":"expensive the more gpus you have the more tokens you can try to predict and improve on and you're going to process"},{"timestamp":"42:12","text":"this data set faster and you can iterate faster and get a bigger Network and train a bigger Network and so on so this"},{"timestamp":"42:19","text":"is what all those machines are look like are uh are doing and this is why all of"},{"timestamp":"42:24","text":"this is such a big deal and for example this is a article from like about a month ago or"},{"timestamp":"42:30","text":"so this is why it's a big deal that for example Elon Musk is getting 100,000 gpus uh in a single Data Center and all"},{"timestamp":"42:38","text":"of these gpus are extremely expensive are going to take a ton of power and all of them are just trying to predict the next token in the sequence and improve"},{"timestamp":"42:45","text":"the network uh by doing so and uh get probably a lot more coherent text than"},{"timestamp":"42:50","text":"what we're seeing here a lot faster okay so unfortunately I do not have a couple 10 or hundred million of dollars to"},{"timestamp":"42:57","text":"spend on training a really big model like this but luckily we can turn to some big tech companies who train these"},{"timestamp":"43:04","text":"models routinely and release some of them once they are done training so they've spent a huge amount of compute"},{"timestamp":"43:10","text":"to train this network and they release the network at the end of the optimization so it's very useful because"},{"timestamp":"43:15","text":"they've done a lot of compute for that so there are many companies who train these models routinely but actually not"},{"timestamp":"43:21","text":"many of them release uh these what's called base models so the model that comes out at the end here is is what's"},{"timestamp":"43:27","text":"called a base model what is a base model it's a token simulator right it's an internet text token simulator and so"},{"timestamp":"43:35","text":"that is not by itself useful yet because what we want is what's called an assistant we want to ask questions and"},{"timestamp":"43:41","text":"have it respond to answers these models won't do that they just uh create sort of remixes of the internet they dream"},{"timestamp":"43:48","text":"internet pages so the base models are not very often released because they're kind of just only a step one of a few"},{"timestamp":"43:55","text":"other steps that we still need to take to get in system however a few releases have been made so"},{"timestamp":"44:01","text":"as an example the gbt2 model released the 1.6 billion sorry 1.5 billion model"},{"timestamp":"44:08","text":"back in 2019 and this gpt2 model is a base model now what is a model release"},{"timestamp":"44:13","text":"what does it look like to release these models so this is the gpt2 repository on GitHub well you need two things"},{"timestamp":"44:20","text":"basically to release model number one we need the um python code usually that"},{"timestamp":"44:27","text":"describes the sequence of operations in detail that they make in their model so"},{"timestamp":"44:34","text":"um if you remember back this Transformer the sequence of steps that"},{"timestamp":"44:40","text":"are taken here in this neural network is what is being described by this code so"},{"timestamp":"44:45","text":"this code is sort of implementing the what's called forward pass of this neural network so we need the specific"},{"timestamp":"44:51","text":"details of exactly how they wired up that neural network so this is just computer code and it's usually just a"},{"timestamp":"44:57","text":"couple hundred lines of code it's not it's not that crazy and uh this is all fairly understandable and usually fairly"},{"timestamp":"45:03","text":"standard what's not standard are the parameters that's where the actual value is what are the parameters of this"},{"timestamp":"45:09","text":"neural network because there's 1.6 billion of them and we need the correct setting or a really good setting and so"},{"timestamp":"45:15","text":"that's why in addition to this source code they release the parameters which in this case is roughly 1.5 billion"},{"timestamp":"45:23","text":"parameters and these are just numbers so it's one single list of 1.5 billion numbers the precise and good setting of"},{"timestamp":"45:30","text":"all the knobs such that the tokens come out well so uh you need those two things to"},{"timestamp":"45:37","text":"get a base model release now gpt2 was released but that's"},{"timestamp":"45:43","text":"actually a fairly old model as I mentioned so actually the model we're going to turn to is called llama 3 and"},{"timestamp":"45:49","text":"that's the one that I would like to show you next so llama 3 so gpt2 again was"},{"timestamp":"45:54","text":"1.6 billion parameters trained on 100 billion tokens Lama 3 is a much bigger model and much more modern model it is"},{"timestamp":"46:00","text":"released and trained by meta and it is a 45 billion parameter model trained on 15"},{"timestamp":"46:07","text":"trillion tokens in very much the same way just much much bigger um and meta has also made a"},{"timestamp":"46:14","text":"release of llama 3 and that was part of this paper so with this paper that goes into"},{"timestamp":"46:21","text":"a lot of detail the biggest base model that they released is the Lama 3.1 4.5"},{"timestamp":"46:27","text":"405 billion parameter model so this is the base model and then in addition to the base model you see here"},{"timestamp":"46:33","text":"foreshadowing for later sections of the video they also released the instruct model and the instruct means that this"},{"timestamp":"46:39","text":"is an assistant you can ask it questions and it will give you answers we still have yet to cover that part later for"},{"timestamp":"46:45","text":"now let's just look at this base model this token simulator and let's play with it and try to think about you know what"},{"timestamp":"46:51","text":"is this thing and how does it work and um what do we get at the end of this optimization if you let this run Until"},{"timestamp":"46:57","text":"the End uh for a very big neural network on a lot of data so my favorite place to interact with the base models is this um"},{"timestamp":"47:04","text":"company called hyperbolic which is basically serving the base model of the 405b Llama 3.1 so when you go to the"},{"timestamp":"47:13","text":"website and I think you may have to register and so on make sure that in the models make sure that you are using"},{"timestamp":"47:18","text":"llama 3.1 405 billion base it must be the base model and then here let's say"},{"timestamp":"47:24","text":"the max tokens is how many tokens we're going to be gener rating so let's just decrease this to be a bit less just so"},{"timestamp":"47:30","text":"we don't waste compute we just want the next 128 tokens and leave the other stuff alone I'm not going to go into the"},{"timestamp":"47:36","text":"full detail here um now fundamentally what's going to happen here is identical"},{"timestamp":"47:41","text":"to what happens here during inference for us so this is just going to continue the token sequence of whatever you"},{"timestamp":"47:47","text":"prefix you're going to give it so I want to first show you that this model here is not yet an assistant so you can for"},{"timestamp":"47:53","text":"example ask it what is 2 plus 2 it's not going to tell you oh it's four uh what else can I help you with it's not going"},{"timestamp":"47:59","text":"to do that because what is 2 plus 2 is going to be tokenized and then those"},{"timestamp":"48:05","text":"tokens just act as a prefix and then what the model is going to do now is just going to get the probability for the next token and it's just a glorified"},{"timestamp":"48:12","text":"autocomplete it's a very very expensive autocomplete of what comes next um"},{"timestamp":"48:17","text":"depending on the statistics of what it saw in its training documents which are basically web pages so let's just uh hit enter to see"},{"timestamp":"48:25","text":"what tokens it comes up with as a"},{"timestamp":"48:31","text":"continuation okay so here it kind of actually answered the question and started to go off into some philosophical territory uh let's try it"},{"timestamp":"48:37","text":"again so let me copy and paste and let's try again from scratch what is 2 plus"},{"timestamp":"48:45","text":"two so okay so it just goes off again so notice one more thing that I want to"},{"timestamp":"48:50","text":"stress is that the system uh I think every time you put it in it just kind of starts from scratch"},{"timestamp":"48:58","text":"so it doesn't uh the system here is stochastic so for the same prefix of tokens we're always getting a different"},{"timestamp":"49:04","text":"answer and the reason for that is that we get this probity distribution and we sample from it and we always get"},{"timestamp":"49:10","text":"different samples and we sort of always go into a different territory uh afterwards so here in this case um I"},{"timestamp":"49:17","text":"don't know what this is let's try one more time so it just continues on so it's"},{"timestamp":"49:25","text":"just doing the stuff that it's saw on the internet right um and it's just kind of like regurgitating those uh"},{"timestamp":"49:31","text":"statistical patterns so first things it's not an assistant yet it's a token autocomplete"},{"timestamp":"49:38","text":"and second it is a stochastic system now the crucial thing is that even though"},{"timestamp":"49:44","text":"this model is not yet by itself very useful for a lot of applications just yet um it is still very useful because"},{"timestamp":"49:52","text":"in the task of predicting the next token in the sequence the model has learned a lot about the world and it has stored"},{"timestamp":"49:59","text":"all that knowledge in the parameters of the network so remember that our text looked like this right internet web"},{"timestamp":"50:06","text":"pages and now all of this is sort of compressed in the weights of the network"},{"timestamp":"50:11","text":"so you can think of um these 405 billion parameters is a kind of compression of"},{"timestamp":"50:16","text":"the internet you can think of the 45 billion parameters is kind of like a zip file uh but it's not a loss less"},{"timestamp":"50:25","text":"compression it's a loss C compression we're kind of like left with kind of a gal of the internet and we can generate"},{"timestamp":"50:31","text":"from it right now we can elicit some of this knowledge by prompting the base model uh accordingly so for example"},{"timestamp":"50:38","text":"here's a prompt that might work to elicit some of that knowledge that's hiding in the parameters here's my top"},{"timestamp":"50:43","text":"10 list of the top landmarks to see in the pairs"},{"timestamp":"50:50","text":"um and I'm doing it this way because I'm trying to Prime the model to now continue this list so let's see if that"},{"timestamp":"50:56","text":"works when I press enter okay so you see that it started a list and it's now kind of giving me some"},{"timestamp":"51:02","text":"of those landmarks and now notice that it's trying to give a lot of information here now you might not be able to actually"},{"timestamp":"51:09","text":"fully trust some of the information here remember that this is all just a recollection of some of the internet"},{"timestamp":"51:14","text":"documents and so the things that occur very frequently in the internet data are"},{"timestamp":"51:19","text":"probably more likely to be remembered correctly compared to things that happen very infrequently so you can't fully"},{"timestamp":"51:25","text":"trust some of the things that and some of the information that is here because it's all just a vague recollection of Internet documents because the"},{"timestamp":"51:32","text":"information is not stored explicitly in any of the parameters it's all just the recollection that said we did get"},{"timestamp":"51:38","text":"something that is probably approximately correct and I don't actually have the expertise to verify that this is roughly"},{"timestamp":"51:44","text":"correct but you see that we've elicited a lot of the knowledge of the model and this knowledge is not precise and exact"},{"timestamp":"51:51","text":"this knowledge is vague and probabilistic and statistical and the kinds of things that occur often are the"},{"timestamp":"51:57","text":"kinds of things that are more likely to be remembered um in the model now I want to show you a few more examples of this"},{"timestamp":"52:04","text":"model's Behavior the first thing I want to show you is this example I went to the Wikipedia page for zebra and let me"},{"timestamp":"52:10","text":"just copy paste the first uh even one sentence here and let me put it here now when I"},{"timestamp":"52:17","text":"click enter what kind of uh completion are we going to get so let me just hit"},{"timestamp":"52:23","text":"enter there are three living species etc etc what the model is producing here"},{"timestamp":"52:29","text":"is an exact regurgitation of this Wikipedia entry it is reciting this Wikipedia entry purely from memory and"},{"timestamp":"52:36","text":"this memory is stored in its parameters and so it is possible that at some point in these 512 tokens the model will uh"},{"timestamp":"52:44","text":"stray away from the Wikipedia entry but you can see that it has huge chunks of it memorized here uh let me see for"},{"timestamp":"52:50","text":"example if this sentence occurs by now okay so this so we're"},{"timestamp":"52:55","text":"still on track let me check here okay we're still on"},{"timestamp":"53:00","text":"track it will eventually uh stray away okay so this thing is just recited"},{"timestamp":"53:07","text":"to a very large extent it will eventually deviate uh because it won't be able to remember exactly now the"},{"timestamp":"53:13","text":"reason that this happens is because these models can be extremely good at memorization and usually this is not"},{"timestamp":"53:18","text":"what you want in the final model and this is something called regurgitation and it's usually undesirable to site uh"},{"timestamp":"53:24","text":"things uh directly uh that you have trained on now the reason that this happens actually is because for a lot of"},{"timestamp":"53:31","text":"documents like for example Wikipedia when these documents are deemed to be of very high quality as a source like for"},{"timestamp":"53:37","text":"example Wikipedia it is very often uh the case that when you train the model you will preferentially sample from"},{"timestamp":"53:44","text":"those sources so basically the model has probably done a few epochs on this data meaning that it has seen this web page"},{"timestamp":"53:50","text":"like maybe probably 10 times or so and it's a bit like you like when you read some kind of a text many many times say"},{"timestamp":"53:56","text":"you read something a 100 times uh then you'll be able to recite it and it's very similar for this model if it sees"},{"timestamp":"54:01","text":"something way too often it's going to be able to recite it later from memory except these models can be a lot more"},{"timestamp":"54:07","text":"efficient um like per presentation than human so probably it's only seen this Wikipedia entry 10 times but basically"},{"timestamp":"54:14","text":"it has remembered this article exactly in its parameters okay the next thing I want to show you is something that the"},{"timestamp":"54:19","text":"model has definitely not seen during its training so for example if we go to the paper uh and then we navigate to the"},{"timestamp":"54:26","text":"pre-training data we'll see here that uh the data set has a knowledge cut off"},{"timestamp":"54:33","text":"until the end of 2023 so it will not have seen documents after this point and"},{"timestamp":"54:38","text":"certainly it has not seen anything about the 2024 election and how it turned out now if we Prime the model with the"},{"timestamp":"54:46","text":"tokens from the future it will continue the token sequence and it will just take its best guess according to the"},{"timestamp":"54:51","text":"knowledge that it has in its own parameters so let's take a look at what that could look like"},{"timestamp":"54:57","text":"so the Republican Party kit Trump okay president of the United States from"},{"timestamp":"55:02","text":"2017 and let's see what it says after this point so for example the model will have to guess at the running mate and"},{"timestamp":"55:09","text":"who it's against Etc so let's hit enter so here thingss that Mike Pence was the running mate instead of JD Vance"},{"timestamp":"55:17","text":"and the ticket was against Hillary Clinton and Tim Kane so this is kind of"},{"timestamp":"55:23","text":"a interesting parallel universe potentially of what could have happened happened according to the LM let's get a"},{"timestamp":"55:28","text":"different sample so the identical prompt and let's resample so here the running mate was"},{"timestamp":"55:35","text":"Ronda santis and they ran against Joe Biden and Camala Harris so this is again"},{"timestamp":"55:40","text":"a different parallel universe so the model will take educated guesses and it will continue the token sequence based"},{"timestamp":"55:45","text":"on this knowledge um and it will just kind of like all of what we're seeing here is what's called hallucination the"},{"timestamp":"55:51","text":"model is just taking its best guess uh in a probalistic manner the next thing I would like to show you is that even"},{"timestamp":"55:58","text":"though this is a base model and not yet an assistant model it can still be utilized in Practical applications if"},{"timestamp":"56:04","text":"you are clever with your prompt design so here's something that we would call a few shot"},{"timestamp":"56:09","text":"prompt so what it is here is that I have 10 words or 10 pairs and each pair is a"},{"timestamp":"56:16","text":"word of English column and then a the translation in Korean and we have 10 of"},{"timestamp":"56:22","text":"them and what the model does here is at the end we have teacher column and then here's where we're going to do a"},{"timestamp":"56:28","text":"completion of say just five tokens and these models have what we call in"},{"timestamp":"56:33","text":"context learning abilities and what that's referring to is that as it is reading this context it is learning sort"},{"timestamp":"56:40","text":"of in place that there's some kind of a algorithmic pattern going on in my data"},{"timestamp":"56:46","text":"and it knows to continue that pattern and this is called kind of like Inc context learning so it takes on the role"},{"timestamp":"56:53","text":"of a translator and when we hit uh completion we see that the teacher translation is"},{"timestamp":"56:59","text":"Sim which is correct um and so this is how you can build apps by being clever"},{"timestamp":"57:05","text":"with your prompting even though we still just have a base model for now and it relies on what we call this um uh in"},{"timestamp":"57:11","text":"context learning ability and it is done by constructing what's called a few shot prompt okay and finally I want to show"},{"timestamp":"57:17","text":"you that there is a clever way to actually instantiate a whole language model assistant just by prompting and"},{"timestamp":"57:24","text":"the trick to it is that we're structure a prompt to look like a web page that is a conversation between a helpful AI"},{"timestamp":"57:31","text":"assistant and a human and then the model will continue that conversation so actually to write the prompt I turned to"},{"timestamp":"57:38","text":"chat gbt itself which is kind of meta but I told it I want to create an llm"},{"timestamp":"57:43","text":"assistant but all I have is the base model so can you please write my um uh"},{"timestamp":"57:50","text":"prompt and this is what it came up with which is actually quite good so here's a conversation between an AI assistant and"},{"timestamp":"57:55","text":"a human the AI assistant is knowledgeable helpful capable of answering wide variety of questions Etc and then here"},{"timestamp":"58:03","text":"it's not enough to just give it a sort of description it works much better if you create this fot prompt so here's a"},{"timestamp":"58:10","text":"few terms of human assistant human assistant and we have uh you know a few"},{"timestamp":"58:15","text":"turns of conversation and then here at the end is we're going to be putting the actual query that we like so let me copy"},{"timestamp":"58:21","text":"paste this into the base model prompt and now let me do human column and this"},{"timestamp":"58:28","text":"is where we put our actual prompt why is the sky blue and uh let's uh"},{"timestamp":"58:37","text":"run assistant the sky appears blue due to the phenomenon called R lights scattering etc etc so you see that the"},{"timestamp":"58:44","text":"base model is just continuing the sequence but because the sequence looks like this conversation it takes on that"},{"timestamp":"58:49","text":"role but it is a little subtle because here it just uh you know it ends the assistant and then just you know"},{"timestamp":"58:55","text":"hallucinate Ates the next question by the human Etc so it'll just continue going on and on uh but you can see that"},{"timestamp":"59:01","text":"we have sort of accomplished the task and if you just took this why is the sky blue and if we just refresh this and put"},{"timestamp":"59:09","text":"it here then of course we don't expect this to work with a base model right we're just going to who knows what we're going to get okay we're just going to"},{"timestamp":"59:15","text":"get more questions okay so this is one way to create an assistant even though you may"},{"timestamp":"59:21","text":"only have a base model okay so this is the kind of brief summary of the things we talked about over the last few"},{"timestamp":"59:28","text":"minutes now let me zoom out here and this is kind of like what we've"},{"timestamp":"59:34","text":"talked about so far we wish to train LM assistants like chpt we've discussed the"},{"timestamp":"59:40","text":"first stage of that which is the pre-training stage and we saw that really what it comes down to is we take Internet documents we break them up into"},{"timestamp":"59:47","text":"these tokens these atoms of little text chunks and then we predict token sequences using neural networks the"},{"timestamp":"59:54","text":"output of this entire stage is this base model it is the setting of The parameters of this network and this base"},{"timestamp":"1:00:01","text":"model is basically an internet document simulator on the token level so it can just uh it can generate token sequences"},{"timestamp":"1:00:08","text":"that have the same kind of like statistics as Internet documents and we saw that we can use it in some"},{"timestamp":"1:00:13","text":"applications but we actually need to do better we want an assistant we want to be able to ask questions and we want the"},{"timestamp":"1:00:18","text":"model to give us answers and so we need to now go into the second stage which is called the post-training stage so we"},{"timestamp":"1:00:26","text":"take our base model our internet document simulator and hand it off to post training so we're now going to"},{"timestamp":"1:00:31","text":"discuss a few ways to do what's called post training of these models these stages in post training are going to be"},{"timestamp":"1:00:38","text":"computationally much less expensive most of the computational work all of the massive data centers um and all of the"},{"timestamp":"1:00:45","text":"sort of heavy compute and millions of dollars are the pre-training stage but"},{"timestamp":"1:00:50","text":"now we go into the slightly cheaper but still extremely important stage called post trining where we turn this llm"},{"timestamp":"1:00:57","text":"model into an assistant so let's take a look at how we can get our model to not"},{"timestamp":"1:01:02","text":"sample internet documents but to give answers to questions so in other words"},{"timestamp":"1:01:07","text":"what we want to do is we want to start thinking about conversations and these are conversations that can be multi-turn"},{"timestamp":"1:01:13","text":"so so uh there can be multiple turns and they are in the simplest case a conversation between a human and an"},{"timestamp":"1:01:19","text":"assistant and so for example we can imagine the conversation could look something like this when a human says what is 2 plus2 the assistant should re"},{"timestamp":"1:01:25","text":"respond with something like 2 plus 2 is 4 when a human follows up and says what if it was star instead of a plus"},{"timestamp":"1:01:31","text":"assistant could respond with something like this um and similar here this is another example showing that the assistant could"},{"timestamp":"1:01:37","text":"also have some kind of a personality here uh that it's kind of like nice and then here in the third example I'm"},{"timestamp":"1:01:43","text":"showing that when a human is asking for something that we uh don't wish to help with we can produce what's called"},{"timestamp":"1:01:48","text":"refusal we can say that we cannot help with that so in other words what we want to do now is we want to think through"},{"timestamp":"1:01:55","text":"how in a system should interact with the human and we want to program the assistant and Its Behavior in these"},{"timestamp":"1:02:01","text":"conversations now because this is neural networks we're not going to be programming these explicitly in code"},{"timestamp":"1:02:07","text":"we're not going to be able to program the assistant in that way because this is neural networks everything is done through neural network training on data"},{"timestamp":"1:02:14","text":"sets and so because of that we are going to be implicitly programming the"},{"timestamp":"1:02:19","text":"assistant by creating data sets of conversations so these are three independent examples of conversations in"},{"timestamp":"1:02:25","text":"a data dat set an actual data set and I'm going to show you examples will be much larger it could have hundreds of"},{"timestamp":"1:02:31","text":"thousands of conversations that are multi- turn very long Etc and would cover a diverse breath of topics but"},{"timestamp":"1:02:37","text":"here I'm only showing three examples but the way this works basically is uh a"},{"timestamp":"1:02:42","text":"assistant is being programmed by example and where is this data coming from like 2 * 2al 4 same as 2 plus 2 Etc where"},{"timestamp":"1:02:50","text":"does that come from this comes from Human labelers so we will basically give human labelers some conversational"},{"timestamp":"1:02:56","text":"context and we will ask them to um basically give the ideal assistant response in this situation and a human"},{"timestamp":"1:03:03","text":"will write out the ideal response for an assistant in any situation and then we're going to get the model to"},{"timestamp":"1:03:10","text":"basically train on this and to imitate those kinds of responses so the way this works then is"},{"timestamp":"1:03:16","text":"we are going to take our base model which we produced in the preing stage and this base model was trained on"},{"timestamp":"1:03:21","text":"internet documents we're now going to take that data set of internet documents and we're gonna throw it out and we're"},{"timestamp":"1:03:27","text":"going to substitute a new data set and that's going to be a data set of conversations and we're going to continue training the model on these"},{"timestamp":"1:03:33","text":"conversations on this new data set of conversations and what happens is that the model will very rapidly adjust and"},{"timestamp":"1:03:40","text":"will sort of like learn the statistics of how this assistant responds to human queries and then later during inference"},{"timestamp":"1:03:48","text":"we'll be able to basically um Prime the assistant and get the response and it"},{"timestamp":"1:03:54","text":"will be imitating what the humans will human labelers would do in that situation if that makes sense so we're"},{"timestamp":"1:04:00","text":"going to see examples of that and this is going to become bit more concrete I also wanted to mention that this post-training stage we're going to"},{"timestamp":"1:04:06","text":"basically just continue training the model but um the pre-training stage can in practice take roughly three months of"},{"timestamp":"1:04:13","text":"training on many thousands of computers the post-training stage will typically be much shorter like 3 hours for example"},{"timestamp":"1:04:20","text":"um and that's because the data set of conversations that we're going to create here manually is much much smaller than"},{"timestamp":"1:04:26","text":"the data set of text on the internet and so this training will be very short but"},{"timestamp":"1:04:31","text":"fundamentally we're just going to take our base model we're going to continue training using the exact same algorithm"},{"timestamp":"1:04:37","text":"the exact same everything except we're swapping out the data set for conversations so the questions now are"},{"timestamp":"1:04:43","text":"what are these conversations how do we represent them how do we get the model to see conversations instead of just raw"},{"timestamp":"1:04:49","text":"text and then what are the outcomes of um this kind of training and what do you"},{"timestamp":"1:04:54","text":"get in a certain like psychological sense uh when we talk about the model so let's turn to those questions now so"},{"timestamp":"1:05:01","text":"let's start by talking about the tokenization of conversations everything in these models has to be turned into"},{"timestamp":"1:05:07","text":"tokens because everything is just about token sequences so how do we turn conversations into token sequences is"},{"timestamp":"1:05:12","text":"the question and so for that we need to design some kind of ending coding and uh this is kind of similar to maybe if"},{"timestamp":"1:05:18","text":"you're familiar you don't have to be with for example the TCP IP packet in um on the internet there are precise rules"},{"timestamp":"1:05:25","text":"and protocols for how you represent information how everything is structured together so that you have all this kind of data laid out in a way that is"},{"timestamp":"1:05:32","text":"written out on a paper and that everyone can agree on and so it's the same thing now happening in llms we need some kind"},{"timestamp":"1:05:38","text":"of data structures and we need to have some rules around how these data structures like conversations get"},{"timestamp":"1:05:43","text":"encoded and decoded to and from tokens and so I want to show you now how I"},{"timestamp":"1:05:48","text":"would recreate uh this conversation in the token space so if you go to Tech"},{"timestamp":"1:05:54","text":"tokenizer I can take that conversation and this is how it is represented in uh for the"},{"timestamp":"1:06:01","text":"language model so here we have we are iterating a user and an assistant in"},{"timestamp":"1:06:06","text":"this two- turn conversation and what you're seeing here is it looks ugly but it's actually"},{"timestamp":"1:06:11","text":"relatively simple the way it gets turned into a token sequence here at the end is a little bit complicated but at the end"},{"timestamp":"1:06:18","text":"this conversation between a user and assistant ends up being 49 tokens it is a one-dimensional sequence of 49 tokens"},{"timestamp":"1:06:24","text":"and these are the tokens okay and all the different llms will have a slightly different format or"},{"timestamp":"1:06:31","text":"protocols and it's a little bit of a wild west right now but for example GPT 40 does it in the following way you have"},{"timestamp":"1:06:39","text":"this special token called imore start and this is short for IM imaginary"},{"timestamp":"1:06:44","text":"monologue uh the start then you have to specify um I"},{"timestamp":"1:06:49","text":"don't actually know why it's called that to be honest then you have to specify whose turn it is so for example user"},{"timestamp":"1:06:54","text":"which is a token 4 28 then you have internal monologue"},{"timestamp":"1:07:00","text":"separator and then it's the exact question so the tokens of the question"},{"timestamp":"1:07:05","text":"and then you have to close it so I am end the end of the imaginary monologue so"},{"timestamp":"1:07:10","text":"basically the question from a user of what is 2 plus two ends up being the"},{"timestamp":"1:07:16","text":"token sequence of these tokens and now the important thing to mention here is that IM start this is not text right IM"},{"timestamp":"1:07:24","text":"start is a special token that gets added it's a new token and um this token has"},{"timestamp":"1:07:30","text":"never been trained on so far it is a new token that we create in a post-training stage and we introduce and so these"},{"timestamp":"1:07:37","text":"special tokens like IM seep IM start Etc are introduced and interspersed with"},{"timestamp":"1:07:42","text":"text so that they sort of um get the model to learn that hey this is a the start of a turn for who is it start of"},{"timestamp":"1:07:49","text":"the turn for the start of the turn is for the user and then this is what the user says and then the user ends and"},{"timestamp":"1:07:56","text":"then it's a new start of a turn and it is by the assistant and then what does the assistant say well these are the"},{"timestamp":"1:08:02","text":"tokens of what the assistant says Etc and so this conversation is not turned into the sequence of tokens the specific"},{"timestamp":"1:08:09","text":"details here are not actually that important all I'm trying to show you in concrete terms is that our conversations"},{"timestamp":"1:08:15","text":"which we think of as kind of like a structured object end up being turned via some encoding into onedimensional"},{"timestamp":"1:08:21","text":"sequences of tokens and so because this is one dimensional sequence of tokens we"},{"timestamp":"1:08:27","text":"can apply all the stuff that we applied before now it's just a sequence of tokens and now we can train a language"},{"timestamp":"1:08:33","text":"model on it and so we're just predicting the next token in a sequence uh just like before and um we can represent and"},{"timestamp":"1:08:39","text":"train on conversations and then what does it look like at test time during inference so say we've trained a model"},{"timestamp":"1:08:46","text":"and we've trained a model on these kinds of data sets of conversations and now we want to"},{"timestamp":"1:08:52","text":"inference so during inference what does this look like when you're on on chash apt well you come to chash apt and you"},{"timestamp":"1:08:58","text":"have say like a dialogue with it and the way this works is basically um say that this was already"},{"timestamp":"1:09:06","text":"filled in so like what is 2 plus 2 2 plus 2 is four and now you issue what if it was times I am end and what basically"},{"timestamp":"1:09:13","text":"ends up happening um on the servers of open AI or something like that is they put in I start assistant I amep and this"},{"timestamp":"1:09:21","text":"is where they end it right here so they construct this context and now they"},{"timestamp":"1:09:27","text":"start sampling from the model so it's at this stage that they will go to the model and say okay what is a good for"},{"timestamp":"1:09:32","text":"sequence what is a good first token what is a good second token what is a good third token and this is where the LM"},{"timestamp":"1:09:38","text":"takes over and creates a response like for example response that looks"},{"timestamp":"1:09:43","text":"something like this but it doesn't have to be identical to this but it will have the flavor of this if this kind of a"},{"timestamp":"1:09:48","text":"conversation was in the data set so um that's roughly how the protocol Works"},{"timestamp":"1:09:54","text":"although the details of this protocol are not important so again my goal is that just to show you that everything"},{"timestamp":"1:10:01","text":"ends up being just a one-dimensional token sequence so we can apply everything we've already seen but we're"},{"timestamp":"1:10:06","text":"now training on conversations and we're now uh basically generating conversations as well okay so now I"},{"timestamp":"1:10:13","text":"would like to turn to what these data sets look like in practice the first paper that I would like to show you and the first effort in this direction is"},{"timestamp":"1:10:20","text":"this paper from openai in 2022 and this paper was called instruct GPT or the"},{"timestamp":"1:10:25","text":"technique that they developed and this was the first time that opena has kind of talked about how you can take language models and fine-tune them on"},{"timestamp":"1:10:32","text":"conversations and so this paper has a number of details that I would like to take you through so the first stop I"},{"timestamp":"1:10:38","text":"would like to make is in section 3.4 where they talk about the human contractors that they hired uh in this"},{"timestamp":"1:10:44","text":"case from upwork or through scale AI to uh construct these conversations and so"},{"timestamp":"1:10:49","text":"there are human labelers involved whose job it is professionally to create these conversations and these labelers are"},{"timestamp":"1:10:56","text":"asked to come up with prompts and then they are asked to also complete the ideal assistant responses and so these"},{"timestamp":"1:11:03","text":"are the kinds of prompts that people came up with so these are human labelers so list five ideas for how to regain"},{"timestamp":"1:11:08","text":"enthusiasm for my career what are the top 10 science fiction books I should read next and there's many different"},{"timestamp":"1:11:13","text":"types of uh kind of prompts here so translate this sentence from uh to Spanish Etc and so there's many things"},{"timestamp":"1:11:21","text":"here that people came up with they first come up with the prompt and then they also uh answer that prompt and they give"},{"timestamp":"1:11:28","text":"the ideal assistant response now how do they know what is the ideal assistant response that they should write for"},{"timestamp":"1:11:33","text":"these prompts so when we scroll down a little bit further we see that here we have this excerpt of labeling"},{"timestamp":"1:11:39","text":"instructions uh that are given to the human labelers so the company that is developing the language model like for"},{"timestamp":"1:11:45","text":"example open AI writes up labeling instructions for how the humans should create ideal responses and so here for"},{"timestamp":"1:11:52","text":"example is an excerpt uh of these kinds of labeling instruction instructions on High level you're asking people to be"},{"timestamp":"1:11:57","text":"helpful truthful and harmless and you can pause the video if you'd like to see more here but on a high level basically"},{"timestamp":"1:12:04","text":"just just answer try to be helpful try to be truthful and don't answer questions that we don't want um kind of"},{"timestamp":"1:12:10","text":"the system to handle uh later in chat gbt and so roughly speaking the company"},{"timestamp":"1:12:16","text":"comes up with the labeling instructions usually they are not this short usually there are hundreds of pages and people have to study them professionally and"},{"timestamp":"1:12:23","text":"then they write out the ideal assistant responses uh following those labeling instructions so this is a very human"},{"timestamp":"1:12:30","text":"heavy process as it was described in this paper now the data set for instruct GPT was never actually released by openi"},{"timestamp":"1:12:37","text":"but we do have some open- Source um reproductions that were're trying to follow this kind of a setup and collect"},{"timestamp":"1:12:42","text":"their own data so one that I'm familiar with for example is the effort of open"},{"timestamp":"1:12:48","text":"Assistant from a while back and this is just one of I think many examples but I just want to show you an example so"},{"timestamp":"1:12:54","text":"here's so these were people on the internet that were asked to basically create these conversations similar to"},{"timestamp":"1:12:59","text":"what um open I did with human labelers and so here's an entry of a person who"},{"timestamp":"1:13:05","text":"came up with this BR can you write a short introduction to the relevance of the term manop uh in economics please use"},{"timestamp":"1:13:12","text":"examples Etc and then the same person or potentially a different person will write up the response so here's the"},{"timestamp":"1:13:18","text":"assistant response to this and so then the same person or different person will actually write out this ideal"},{"timestamp":"1:13:26","text":"response and then this is an example of maybe how the conversation could continue now explain it to a dog and"},{"timestamp":"1:13:33","text":"then you can try to come up with a slightly a simpler explanation or something like that now this then"},{"timestamp":"1:13:39","text":"becomes the label and we end up training on this so what happens during training"},{"timestamp":"1:13:45","text":"is that um of course we're not going to have a full coverage of all the possible"},{"timestamp":"1:13:50","text":"questions that um the model will encounter at test time during inference"},{"timestamp":"1:13:56","text":"we can't possibly cover all the possible prompts that people are going to be asking in the future but if we have a"},{"timestamp":"1:14:02","text":"like a data set of a few of these examples then the model during training will start to take on this Persona of"},{"timestamp":"1:14:09","text":"this helpful truthful harmless assistant and it's all programmed by example and"},{"timestamp":"1:14:14","text":"so these are all examples of behavior and if you have conversations of these example behaviors and you have enough of"},{"timestamp":"1:14:19","text":"them like 100,00 and you train on it the model sort of starts to understand the statistical pattern and it kind of takes"},{"timestamp":"1:14:26","text":"on this personality of this assistant now it's possible that when you get the exact same question like"},{"timestamp":"1:14:32","text":"this at test time it's possible that the answer will be recited as exactly what"},{"timestamp":"1:14:38","text":"was in the training set but more likely than that is that the model will kind of like do something of a similar Vibe um"},{"timestamp":"1:14:45","text":"and we will understand that this is the kind of answer that you want um so"},{"timestamp":"1:14:51","text":"that's what we're doing we're programming the system um by example and the system adopts statistically this"},{"timestamp":"1:14:58","text":"Persona of this helpful truthful harmless assistant which is kind of like reflected in the labeling instructions"},{"timestamp":"1:15:04","text":"that the company creates now I want to show you that the state-of-the-art has kind of advanced in the last 2 or 3"},{"timestamp":"1:15:09","text":"years uh since the instr GPT paper so in particular it's not very common for humans to be doing all the heavy lifting"},{"timestamp":"1:15:16","text":"just by themselves anymore and that's because we now have language models and these language models are helping us create these data sets and conversations"},{"timestamp":"1:15:23","text":"so it is very rare that the people will like literally just write out the response from scratch it is a lot more"},{"timestamp":"1:15:28","text":"likely that they will use an existing llm to basically like uh come up with an answer and then they will edit it or"},{"timestamp":"1:15:34","text":"things like that so there's many different ways in which now llms have started to kind of permeate this"},{"timestamp":"1:15:39","text":"posttraining Set uh stack and llms are basically used pervasively to help"},{"timestamp":"1:15:45","text":"create these massive data sets of conversations so I don't want to show like Ultra chat is one um such example"},{"timestamp":"1:15:52","text":"of like a more modern data set of conversations it is to a very large extent synthetic but uh I believe"},{"timestamp":"1:15:58","text":"there's some human involvement I could be wrong with that usually there will be a little bit of human but there will be a huge amount of synthetic help um and"},{"timestamp":"1:16:06","text":"this is all kind of like uh constructed in different ways and Ultra chat is just one example of many sft data sets that"},{"timestamp":"1:16:12","text":"currently exist and the only thing I want to show you is that uh these data sets have now millions of conversations"},{"timestamp":"1:16:18","text":"uh these conversations are mostly synthetic but they're probably edited to some extent by humans and they span a"},{"timestamp":"1:16:23","text":"huge diversity of sort of um uh areas and so on so these are"},{"timestamp":"1:16:31","text":"fairly extensive artifacts by now and there's all these like sft mixtures as they're called so you have a mixture of"},{"timestamp":"1:16:37","text":"like lots of different types and sources and it's partially synthetic partially human and it's kind of like um gone in"},{"timestamp":"1:16:44","text":"that direction since uh but roughly speaking we still have sft data sets they're made up of conversations we're"},{"timestamp":"1:16:50","text":"training on them um just like we did before and uh I guess like the last thing to note"},{"timestamp":"1:16:57","text":"is that I want to dispel a little bit of the magic of talking to an AI like when"},{"timestamp":"1:17:02","text":"you go to chat GPT and you give it a question and then you hit enter uh what"},{"timestamp":"1:17:07","text":"is coming back is kind of like statistically aligned with what's happening in the training set and these"},{"timestamp":"1:17:14","text":"training sets I mean they really just have a seed in humans following labeling"},{"timestamp":"1:17:19","text":"instructions so what are you actually talking to in chat GPT or how should you think about it well it's not coming from"},{"timestamp":"1:17:25","text":"some magical AI like roughly speaking it's coming from something that is statistically imitating human labelers"},{"timestamp":"1:17:32","text":"which comes from labeling instructions written by these companies and so you're kind of imitating this uh you're kind of"},{"timestamp":"1:17:38","text":"getting um it's almost as if you're asking human labeler and imagine that the answer that is given to you uh from"},{"timestamp":"1:17:45","text":"chbt is some kind of a simulation of a human labeler uh and it's kind of like"},{"timestamp":"1:17:50","text":"asking what would a human labeler say in this kind of a conversation"},{"timestamp":"1:17:56","text":"and uh it's not just like this human labeler is not just like a random person from the internet because these"},{"timestamp":"1:18:01","text":"companies actually hire experts so for example when you are asking questions about code and so on the human labelers"},{"timestamp":"1:18:06","text":"that would be in um involved in creation of these conversation data sets they will usually be usually be educated"},{"timestamp":"1:18:12","text":"expert people and you're kind of like asking a question of like a simulation of those people if that makes sense so"},{"timestamp":"1:18:19","text":"you're not talking to a magical AI you're talking to an average labeler this average labeler is probably fairly highly skilled"},{"timestamp":"1:18:25","text":"but you're talking to kind of like an instantaneous simulation of that kind of a person that would be hired uh in the"},{"timestamp":"1:18:32","text":"construction of these data sets so let me give you one more specific example before we move on for example when I go"},{"timestamp":"1:18:38","text":"to chpt and I say recommend the top five landmarks who see in Paris and then I hit"},{"timestamp":"1:18:44","text":"enter uh okay here we go okay when I hit enter"},{"timestamp":"1:18:52","text":"what's coming out here how do I think about it well it's not some kind of a magical AI that has gone out and"},{"timestamp":"1:18:58","text":"researched all the landmarks and then ranked them using its infinite intelligence Etc what I'm getting is a"},{"timestamp":"1:19:04","text":"statistical simulation of a labeler that was hired by open AI you can think about it roughly in that way and so if this"},{"timestamp":"1:19:13","text":"specific um question is in the posttraining data set somewhere at open aai then I'm very likely to see an"},{"timestamp":"1:19:20","text":"answer that is probably very very similar to what that human labeler would have put down for those five landmarks how does the"},{"timestamp":"1:19:27","text":"human labeler come up with this well they go off and they go on the internet and they kind of do their own little research for 20 minutes and they just"},{"timestamp":"1:19:32","text":"come up with a list right now so if they come up with this list and this is in the data set I'm probably very likely to"},{"timestamp":"1:19:39","text":"see what they submitted as the correct answer from the assistant now if this"},{"timestamp":"1:19:44","text":"specific query is not part of the post training data set then what I'm getting here is a little bit more emergent uh"},{"timestamp":"1:19:51","text":"because uh the model kind of understands the statistically um the kinds of landmarks that are in"},{"timestamp":"1:19:57","text":"this training set are usually the prominent landmarks the landmarks that people usually want to see the kinds of"},{"timestamp":"1:20:02","text":"landmarks that are usually uh very often talked about on the internet and remember that the model already has a"},{"timestamp":"1:20:08","text":"ton of Knowledge from its pre-training on the internet so it's probably seen a ton of conversations about Paris about"},{"timestamp":"1:20:13","text":"landmarks about the kinds of things that people like to see and so it's the pre-training knowledge that has then combined with the postering data set"},{"timestamp":"1:20:20","text":"that results in this kind of an imitation um so that's uh that's roughly how you can"},{"timestamp":"1:20:27","text":"kind of think about what's happening behind the scenes here in in this statistical sense okay now I want to"},{"timestamp":"1:20:33","text":"turn to the topic of llm psychology as I like to call it which is what are sort of the emergent cognitive effects of the"},{"timestamp":"1:20:40","text":"training pipeline that we have for these models so in particular the first one I want to talk to is of course"},{"timestamp":"1:20:47","text":"hallucinations so you might be familiar with model hallucinations it's when llms make stuff up they just totally"},{"timestamp":"1:20:53","text":"fabricate information Etc and it's a big problem with llm assistants it is a problem that existed to a large extent"},{"timestamp":"1:21:00","text":"with early models uh from many years ago and I think the problem has gotten a bit better uh because there are some"},{"timestamp":"1:21:05","text":"medications that I'm going to go into in a second for now let's just try to understand where these hallucinations come from so here's a specific example"},{"timestamp":"1:21:13","text":"of a few uh of three conversations that you might think you have in your training set and um these are pretty"},{"timestamp":"1:21:20","text":"reasonable conversations that you could imagine being in the training set so like for example who is Cruz well Tom"},{"timestamp":"1:21:25","text":"Cruz is an famous actor American actor and producer Etc who is John baraso this"},{"timestamp":"1:21:31","text":"turns out to be a us senetor for example who is genis Khan well genis Khan was"},{"timestamp":"1:21:36","text":"blah blah blah and so this is what your conversations could look like at training time now the problem with this"},{"timestamp":"1:21:42","text":"is that when the human is writing the correct answer for the assistant in each"},{"timestamp":"1:21:48","text":"one of these cases uh the human either like knows who this person is or they research them on the Internet and they"},{"timestamp":"1:21:53","text":"come in and they write this response that kind of has this like confident tone of an answer and what happens"},{"timestamp":"1:21:59","text":"basically is that at test time when you ask for someone who is this is a totally random name that I totally came up with"},{"timestamp":"1:22:05","text":"and I don't think this person exists um as far as I know I just Tred to generate it randomly the problem is when we ask"},{"timestamp":"1:22:11","text":"who is Orson kovats the problem is that the assistant will not just tell you oh"},{"timestamp":"1:22:17","text":"I don't know even if the assistant and the language model itself might know"},{"timestamp":"1:22:23","text":"inside its features inside its activations inside of its brain sort of it might know that this person is like not someone that um that is that it's"},{"timestamp":"1:22:30","text":"familiar with even if some part of the network kind of knows that in some sense the uh saying that oh I don't know who"},{"timestamp":"1:22:37","text":"this is is is not going to happen because the model statistically imitates"},{"timestamp":"1:22:42","text":"is training set in the training set the questions of the form who is blah are confidently answered with the correct"},{"timestamp":"1:22:49","text":"answer and so it's going to take on the style of the answer and it's going to do its best it's going to give you"},{"timestamp":"1:22:55","text":"statistically the most likely guess and it's just going to basically make stuff up because these models again we just"},{"timestamp":"1:23:01","text":"talked about it is they don't have access to the internet they're not doing research these are statistical token"},{"timestamp":"1:23:06","text":"tumblers as I call them uh is just trying to sample the next token in the sequence and it's going to basically"},{"timestamp":"1:23:12","text":"make stuff up so let's take a look at what this looks like I have here what's called the"},{"timestamp":"1:23:17","text":"inference playground from hugging face and I am on purpose picking on a model"},{"timestamp":"1:23:22","text":"called Falcon 7B which is an old model this is a few years ago now so it's an older model So It suffers from"},{"timestamp":"1:23:28","text":"hallucinations and as I mentioned this has improved over time recently but let's say who is Orson kovats let's ask"},{"timestamp":"1:23:35","text":"Falcon 7B instruct run oh yeah Orson kovat is an American author and science uh fiction writer"},{"timestamp":"1:23:42","text":"okay this is totally false it's hallucination let's try again these are statistical systems right so we can"},{"timestamp":"1:23:48","text":"resample this time Orson kovat is a fictional character from this 1950s TV"},{"timestamp":"1:23:53","text":"show it's total BS right let's try again he's a former minor league baseball"},{"timestamp":"1:23:59","text":"player okay so basically the model doesn't know and it's given us lots of different answers because it doesn't"},{"timestamp":"1:24:06","text":"know it's just kind of like sampling from these probabilities the model starts with the tokens who is oron"},{"timestamp":"1:24:12","text":"kovats assistant and then it comes in here and it's get it's getting these"},{"timestamp":"1:24:17","text":"probabilities and it's just sampling from the probabilities and it just like comes up with stuff and the stuff is"},{"timestamp":"1:24:24","text":"actually statistically consistent with the style of the answer in its training set and"},{"timestamp":"1:24:29","text":"it's just doing that but you and I experiened it as a madeup factual knowledge but keep in mind that uh the"},{"timestamp":"1:24:36","text":"model basically doesn't know and it's just imitating the format of the answer and it's not going to go off and look it up uh because it's just imitating again"},{"timestamp":"1:24:44","text":"the answer so how can we uh mitigate this because for example when we go to chat apt and I say who is oron kovats"},{"timestamp":"1:24:50","text":"and I'm now asking the stateoftheart state-of-the-art model from open AI this model will tell"},{"timestamp":"1:24:56","text":"you oh so this model is actually is even smarter because you saw very briefly it"},{"timestamp":"1:25:02","text":"said searching the web uh we're going to cover this later um it's actually trying to do tool use and"},{"timestamp":"1:25:11","text":"uh kind of just like came up with some kind of a story but I want to just who or Kovach did not use any tools I don't"},{"timestamp":"1:25:19","text":"want it to do web search there's a wellknown historical or"},{"timestamp":"1:25:24","text":"public figure named or oron kovats so this model is not going to make up stuff this model knows that it doesn't know"},{"timestamp":"1:25:31","text":"and it tells you that it doesn't appear to be a person that this model knows so somehow we sort of improved"},{"timestamp":"1:25:37","text":"hallucinations even though they clearly are an issue in older models and it makes totally uh sense why you would be"},{"timestamp":"1:25:44","text":"getting these kinds of answers if this is what your training set looks like so how do we fix this okay well clearly we"},{"timestamp":"1:25:50","text":"need some examples in our data set that where the correct answer for the assistant is that the model doesn't know"},{"timestamp":"1:25:57","text":"about some particular fact but we only need to have those answers be produced in the cases where the model actually"},{"timestamp":"1:26:03","text":"doesn't know and so the question is how do we know what the model knows or doesn't know well we can empirically"},{"timestamp":"1:26:09","text":"probe the model to figure that out so let's take a look at for example how meta uh dealt with hallucinations for"},{"timestamp":"1:26:16","text":"the Llama 3 series of models as an example so in this paper that they published from meta we can go into"},{"timestamp":"1:26:22","text":"hallucinations which they call here factuality and they"},{"timestamp":"1:26:27","text":"describe the procedure by which they basically interrogate the model to figure out what it knows and doesn't"},{"timestamp":"1:26:33","text":"know to figure out sort of like the boundary of its knowledge and then they add examples to the training set where"},{"timestamp":"1:26:41","text":"for the things where the model doesn't know them the correct answer is that the model doesn't know them which sounds"},{"timestamp":"1:26:48","text":"like a very easy thing to do in principle but this roughly fixes the issue and the the reason it fixes the"},{"timestamp":"1:26:54","text":"issue is because remember like the model might actually have a pretty good model of its"},{"timestamp":"1:27:01","text":"self knowledge inside the network so remember we looked at the network and all these neurons inside the network you"},{"timestamp":"1:27:08","text":"might imagine that there's a neuron somewhere in the network that sort of like lights up for when the model is"},{"timestamp":"1:27:14","text":"uncertain but the problem is that the activation of that neuron is not currently wired up to the model actually"},{"timestamp":"1:27:20","text":"saying in words that it doesn't know so even though the internal of the neural network no because there's some neurons"},{"timestamp":"1:27:26","text":"that represent that the model uh will not surface that it will instead take its best guess so that it sounds"},{"timestamp":"1:27:33","text":"confident um just like it sees in a training set so we need to basically interrogate the model and allow it to"},{"timestamp":"1:27:39","text":"say I don't know in the cases that it doesn't know so let me take you through what meta roughly does so basically what"},{"timestamp":"1:27:45","text":"they do is here I have an example uh Dominic kek is uh the featured article"},{"timestamp":"1:27:51","text":"today so I just went there randomly and what they do is basically they take a random document in a training set and"},{"timestamp":"1:27:58","text":"they take a paragraph and then they use an llm to construct questions about that"},{"timestamp":"1:28:04","text":"paragraph so for example I did that with chat GPT here so I said here's a paragraph from"},{"timestamp":"1:28:12","text":"this document generate three specific factual questions based on this paragraph and give me the questions and"},{"timestamp":"1:28:17","text":"the answers and so the llms are already good enough to create and reframe this"},{"timestamp":"1:28:23","text":"information so if the information is in the context window um of this llm this"},{"timestamp":"1:28:29","text":"actually works pretty well it doesn't have to rely on its memory it's right there in the context window and so it"},{"timestamp":"1:28:35","text":"can basically reframe that information with fairly high accuracy so for example can generate questions for us like for"},{"timestamp":"1:28:41","text":"which team did he play here's the answer how many cups did he win Etc and now"},{"timestamp":"1:28:47","text":"what we have to do is we have some question and answers and now we want to interrogate the model so roughly speaking what we'll do is we'll take our"},{"timestamp":"1:28:53","text":"questions and we'll go to our model which would be uh say llama uh in meta"},{"timestamp":"1:28:59","text":"but let's just interrogate mol 7B here as an example that's another model so does this model know about this answer"},{"timestamp":"1:29:07","text":"let's take a look uh so he played for Buffalo Sabers right so the model knows and the the way"},{"timestamp":"1:29:15","text":"that you can programmatically decide is basically we're going to take this answer from the model and we're going to"},{"timestamp":"1:29:20","text":"compare it to the correct answer and again the model model are good enough to do this automatically so there's no"},{"timestamp":"1:29:26","text":"humans involved here we can take uh basically the answer from the model and we can use another llm judge to check if"},{"timestamp":"1:29:33","text":"that is correct according to this answer and if it is correct that means that the model probably knows so what we're going"},{"timestamp":"1:29:38","text":"to do is we're going to do this maybe a few times so okay it knows it's Buffalo Savers let's drag"},{"timestamp":"1:29:45","text":"in um Buffalo Sabers let's try one more"},{"timestamp":"1:29:51","text":"time Buffalo Sabers so we asked three times about this factual question and the model seems to know so everything is"},{"timestamp":"1:29:58","text":"great now let's try the second question how many Stanley Cups did he win and again let's interrogate the"},{"timestamp":"1:30:04","text":"model about that and the correct answer is two so um here the model claims that he"},{"timestamp":"1:30:13","text":"won um four times which is not correct right it doesn't match two so the model"},{"timestamp":"1:30:20","text":"doesn't know it's making stuff up let's try again"},{"timestamp":"1:30:27","text":"um so here the model again it's kind of like making stuff up right let's"},{"timestamp":"1:30:34","text":"Dragon here it says did he did not even did not win during his career so"},{"timestamp":"1:30:39","text":"obviously the model doesn't know and the way we can programmatically tell again is we interrogate the model three times"},{"timestamp":"1:30:45","text":"and we compare its answers maybe three times five times whatever it is to the correct answer and if the model doesn't"},{"timestamp":"1:30:51","text":"know then we know that the model doesn't know this question and then what we do is we take this question we create a new conversation in"},{"timestamp":"1:30:59","text":"the training set so we're going to add a new conversation training set and when the question is how many Stanley Cups"},{"timestamp":"1:31:05","text":"did he win the answer is I'm sorry I don't know or I don't remember and"},{"timestamp":"1:31:10","text":"that's the correct answer for this question because we interrogated the model and we saw that that's the case if"},{"timestamp":"1:31:15","text":"you do this for many different types of uh questions for many different types of documents you are giving the model an"},{"timestamp":"1:31:23","text":"opportunity to in its training set refuse to say based on its knowledge and"},{"timestamp":"1:31:28","text":"if you just have a few examples of that in your training set the model will know"},{"timestamp":"1:31:33","text":"um and and has the opportunity to learn the association of this knowledge-based refusal to this internal neuron"},{"timestamp":"1:31:41","text":"somewhere in its Network that we presume exists and empirically this turns out to be probably the case and it can learn"},{"timestamp":"1:31:47","text":"that Association that hey when this neuron of uncertainty is high then I actually don't know and I'm allowed to"},{"timestamp":"1:31:54","text":"say that I'm sorry but I don't think I remember this Etc and if you have these"},{"timestamp":"1:31:59","text":"uh examples in your training set then this is a large mitigation for hallucination and that's roughly"},{"timestamp":"1:32:05","text":"speaking why chpt is able to do stuff like this as well so these are kinds of uh mitigations that people have"},{"timestamp":"1:32:12","text":"implemented and that have improved the factuality issue over time okay so I've described mitigation number one for"},{"timestamp":"1:32:19","text":"basically mitigating the hallucinations issue now we can actually do much better than that uh it's instead of just saying"},{"timestamp":"1:32:27","text":"that we don't know uh we can introduce an additional mitigation number two to give the llm an opportunity to be"},{"timestamp":"1:32:33","text":"factual and actually answer the question now what do you and I do if I was to ask"},{"timestamp":"1:32:39","text":"you a factual question and you don't know uh what would you do um in order to answer the question well you could uh go"},{"timestamp":"1:32:45","text":"off and do some search and uh use the internet and you could figure out the answer and then tell me what that answer"},{"timestamp":"1:32:51","text":"is and we can do the exact exact same thing with these models so think of the"},{"timestamp":"1:32:56","text":"knowledge inside the neural network inside its billions of parameters think of that as kind of a vague recollection"},{"timestamp":"1:33:02","text":"of the things that the model has seen during its training during the pre-training stage a long time ago so"},{"timestamp":"1:33:09","text":"think of that knowledge in the parameters as something you read a month ago and if you keep reading something"},{"timestamp":"1:33:15","text":"then you will remember it and the model remembers that but if it's something rare then you probably don't have a really good recollection of that"},{"timestamp":"1:33:21","text":"information but what you and I do is we just go and look it up now when you go and look it up what you're doing"},{"timestamp":"1:33:26","text":"basically is like you're refreshing your working memory with information and then you're able to sort of like retrieve it"},{"timestamp":"1:33:32","text":"talk about it or Etc so we need some equivalent of allowing the model to refresh its memory or its recollection"},{"timestamp":"1:33:38","text":"and we can do that by introducing tools uh for the models so the way we are going to"},{"timestamp":"1:33:44","text":"approach this is that instead of just saying hey I'm sorry I don't know we can attempt to use tools so we can create uh"},{"timestamp":"1:33:53","text":"a mechanism by which the language model can emit special tokens and these are tokens that we're going to introduce new tokens so"},{"timestamp":"1:34:00","text":"for example here I've introduced two tokens and I've introduced a format or a protocol for how the model is allowed to"},{"timestamp":"1:34:07","text":"use these tokens so for example instead of answering the question when the model does not instead of just saying I don't"},{"timestamp":"1:34:14","text":"know sorry the model has the option now to emitting the special token search start and this is the query that will go"},{"timestamp":"1:34:20","text":"to like bing.com in the case of openai or say Google search or something like that so it will emit the query and then"},{"timestamp":"1:34:26","text":"it will emit search end and then here what will happen is that the program"},{"timestamp":"1:34:32","text":"that is sampling from the model that is running the inference when it sees the special token search end instead of"},{"timestamp":"1:34:39","text":"sampling the next token uh in the sequence it will actually pause generating from the model it will go off"},{"timestamp":"1:34:46","text":"it will open a session with bing.com and it will paste the search query into Bing"},{"timestamp":"1:34:52","text":"and it will then um get all the text that is retrieved and it will basically take that text it will maybe represent"},{"timestamp":"1:34:58","text":"it again with some other special tokens or something like that and it will take that text and it will copy paste it here"},{"timestamp":"1:35:05","text":"into what I Tred to like show with the brackets so all that text kind of comes here and when the text comes here it"},{"timestamp":"1:35:12","text":"enters the context window so the model so that text from the web search is now inside the context window that will feed"},{"timestamp":"1:35:20","text":"into the neural network and you should think of the context window as kind of like the working memory of the model"},{"timestamp":"1:35:25","text":"that data that is in the context window is directly accessible by the model it directly feeds into the neural network"},{"timestamp":"1:35:31","text":"so it's not anymore a vague recollection it's data that it it has in the context"},{"timestamp":"1:35:36","text":"window and is directly available to that model so now when it's sampling the new uh tokens here afterwards it can"},{"timestamp":"1:35:43","text":"reference very easily the data that has been copy pasted in there so that's"},{"timestamp":"1:35:48","text":"roughly how these um how these tools use uh tools uh function"},{"timestamp":"1:35:54","text":"and so web search is just one of the tools we're going to look at some of the other tools in a bit uh but basically you introduce new tokens you introduce"},{"timestamp":"1:36:00","text":"some schema by which the model can utilize these tokens and can call these special functions like web search"},{"timestamp":"1:36:06","text":"functions and how do you teach the model how to correctly use these tools like say web search search start search end"},{"timestamp":"1:36:12","text":"Etc well again you do that through training sets so we need now to have a bunch of data and a bunch of"},{"timestamp":"1:36:18","text":"conversations that show the model by example how to use web search so what"},{"timestamp":"1:36:24","text":"are the what are the settings where you are using the search um and what does that look like and here's by example how"},{"timestamp":"1:36:30","text":"you start a search and the search Etc and uh if you have a few thousand maybe examples of that in your training set"},{"timestamp":"1:36:36","text":"the model will actually do a pretty good job of understanding uh how this tool works and it will know how to sort of"},{"timestamp":"1:36:43","text":"structure its queries and of course because of the pre-training data set and its understanding of the world it"},{"timestamp":"1:36:48","text":"actually kind of understands what a web search is and so it actually kind of has a pretty good native understanding"},{"timestamp":"1:36:54","text":"um of what kind of stuff is a good search query um and so it all kind of just like works you just need a little"},{"timestamp":"1:37:00","text":"bit of a few examples to show it how to use this new tool and then it can lean on it to retrieve information and uh put"},{"timestamp":"1:37:07","text":"it in the context window and that's equivalent to you and I looking something up because once it's in the"},{"timestamp":"1:37:12","text":"context it's in the working memory and it's very easy to manipulate and access so that's what we saw a few minutes ago"},{"timestamp":"1:37:18","text":"when I was searching on chat GPT for who is Orson kovats the chat GPT language model decided Ed that this is some kind"},{"timestamp":"1:37:24","text":"of a rare um individual or something like that and instead of giving me an answer from its memory it decided that"},{"timestamp":"1:37:31","text":"it will sample a special token that is going to do web search and we saw briefly something flash it was like"},{"timestamp":"1:37:36","text":"using the web tool or something like that so it briefly said that and then we waited for like two seconds and then it generated this and you see how it's"},{"timestamp":"1:37:43","text":"creating references here and so it's citing sources so what happened here is"},{"timestamp":"1:37:50","text":"it went off it did a web web search it found these sources and these URLs and the text of these web pages was all"},{"timestamp":"1:37:58","text":"stuffed in between here and it's not showing here but it's it's basically stuffed as text in between here and now"},{"timestamp":"1:38:06","text":"it sees that text and now it kind of references it and says that okay it could be these people citation could be"},{"timestamp":"1:38:13","text":"those people citation Etc so that's what happened here and that's what and that's why when I said who is Orson kovats I"},{"timestamp":"1:38:19","text":"could also say don't use any tools and then that's enough to um basically convince chat PT to not use"},{"timestamp":"1:38:25","text":"tools and just use its memory and its recollection I also went off and I um"},{"timestamp":"1:38:32","text":"tried to ask this question of Chachi PT so how many standing cups did uh Dominic Hasek win and Chachi P actually decided"},{"timestamp":"1:38:39","text":"that it knows the answer and it has the confidence to say that uh he want twice and so it kind of just relied on its"},{"timestamp":"1:38:45","text":"memory because presumably it has um it has enough of a kind of confidence in its weights in"},{"timestamp":"1:38:53","text":"it parameters and activations that this is uh retrievable just for memory um but"},{"timestamp":"1:38:59","text":"you can also conversely use web search to make sure and then for the same query it actually"},{"timestamp":"1:39:06","text":"goes off and it searches and then it finds a bunch of sources it finds all this all of this stuff gets copy pasted"},{"timestamp":"1:39:12","text":"in there and then it tells us uh to again and sites and it actually says the"},{"timestamp":"1:39:17","text":"Wikipedia article which is the source of this information for us as well so"},{"timestamp":"1:39:23","text":"that's tools web search the model determines when to search and then uh that's kind of like how these tools uh"},{"timestamp":"1:39:29","text":"work and this is an additional kind of mitigation for uh hallucinations and factuality so I want to stress one more"},{"timestamp":"1:39:37","text":"time this very important sort of psychology Point knowledge in the parameters of the"},{"timestamp":"1:39:43","text":"neural network is a vague recollection the knowledge in the tokens that make up the context"},{"timestamp":"1:39:48","text":"window is the working memory and it roughly speaking Works kind of like um"},{"timestamp":"1:39:53","text":"it works for us in our brain the stuff we remember is our parameters uh and the stuff that we just experienced like a"},{"timestamp":"1:40:01","text":"few seconds or minutes ago and so on you can imagine that being in our context window and this context window is being built up as you have a conscious"},{"timestamp":"1:40:07","text":"experience around you so this has a bunch of um implications also for your use of LOLs in practice so for example I"},{"timestamp":"1:40:15","text":"can go to chat GPT and I can do something like this I can say can you Summarize chapter one of Jane Austin's Pride and Prejudice right and this is a"},{"timestamp":"1:40:22","text":"perfectly fine prompt and Chach actually does something relatively reasonable here and but the reason it does that is"},{"timestamp":"1:40:28","text":"because Chach has a pretty good recollection of a famous work like Pride and Prejudice it's probably seen a ton"},{"timestamp":"1:40:34","text":"of stuff about it there's probably forums about this book it's probably read versions of this book um and it's"},{"timestamp":"1:40:40","text":"kind of like remembers because even if you've read this or articles about it"},{"timestamp":"1:40:46","text":"you'd kind of have a recollection enough to actually say all this but usually when I actually interact with LMS and I want them to recall specific things it"},{"timestamp":"1:40:53","text":"always works better if you just give it to them so I think a much better prompt would be something like this can you"},{"timestamp":"1:40:59","text":"summarize for me chapter one of genos's spr and Prejudice and then I am attaching it below for your reference"},{"timestamp":"1:41:04","text":"and then I do something like a delimeter here and I paste it in and I I found that just copy pasting it from some"},{"timestamp":"1:41:10","text":"website that I found here um so copy pasting the chapter one here and I do"},{"timestamp":"1:41:16","text":"that because when it's in the context window the model has direct access to it and can exactly it doesn't have to"},{"timestamp":"1:41:22","text":"recall it it just has access to it and so this summary is can be expected to be a significantly high quality or higher"},{"timestamp":"1:41:29","text":"quality than this summary uh just because it's directly available to the model and I think you and I would work"},{"timestamp":"1:41:34","text":"in the same way if you want to it would be you would produce a much better summary if you had reread this chapter"},{"timestamp":"1:41:40","text":"before you had to summarize it and that's basically what's happening here or the equivalent of it the next sort of"},{"timestamp":"1:41:47","text":"psychological Quirk I'd like to talk about briefly is that of the knowledge of self so what I see very often on the"},{"timestamp":"1:41:52","text":"internet is that people do something like this they ask llms something like what model are you and who built you and"},{"timestamp":"1:41:59","text":"um basically this uh question is a little bit nonsensical and the reason I say that is that as I try to kind of"},{"timestamp":"1:42:05","text":"explain with some of the underhood fundamentals this thing is not a person right it doesn't have a persistent"},{"timestamp":"1:42:11","text":"existence in any way it sort of boots up processes tokens and shuts off and it"},{"timestamp":"1:42:17","text":"does that for every single person it just kind of builds up a context window of conversation and then everything gets deleted and so this this entity is kind"},{"timestamp":"1:42:23","text":"of like restarted from scratch every single conversation if that makes sense it has no persistent self it has no"},{"timestamp":"1:42:28","text":"sense of self it's a token tumbler and uh it follows the statistical regularities of its training set so it"},{"timestamp":"1:42:35","text":"doesn't really make sense to ask it who are you what build you Etc and by default if you do what I described and"},{"timestamp":"1:42:42","text":"just by default and from nowhere you're going to get some pretty random answers so for example let's uh pick on Falcon"},{"timestamp":"1:42:48","text":"which is a fairly old model and let's see what it tells us uh so it's evading the question uh"},{"timestamp":"1:42:55","text":"talented engineers and developers here it says I was built by open AI based on the gpt3 model it's totally making stuff"},{"timestamp":"1:43:01","text":"up now the fact that it's built by open AI here I think a lot of people would take this as evidence that this model"},{"timestamp":"1:43:07","text":"was somehow trained on open AI data or something like that I don't actually think that that's necessarily true the reason for that is"},{"timestamp":"1:43:14","text":"that if you don't explicitly program the model to answer these kinds of questions"},{"timestamp":"1:43:20","text":"then what you're going to get is its statistical best guess at the answer and"},{"timestamp":"1:43:25","text":"this model had a um sft data mixture of conversations and during the"},{"timestamp":"1:43:32","text":"fine-tuning um the model sort of understands as it's training on this data that it's taking on this"},{"timestamp":"1:43:38","text":"personality of this like helpful assistant and it doesn't know how to it doesn't actually it wasn't told exactly"},{"timestamp":"1:43:44","text":"what label to apply to self it just kind of is taking on this uh this uh Persona"},{"timestamp":"1:43:50","text":"of a helpful assistant and remember that the pre-training stage took the documents from the entire internet and"},{"timestamp":"1:43:57","text":"Chach and open AI are very prominent in these documents and so I think what's actually likely to be happening here is"},{"timestamp":"1:44:03","text":"that this is just its hallucinated label for what it is this is its self-identity is that it's chat GPT by open Ai and"},{"timestamp":"1:44:11","text":"it's only saying that because there's a ton of data on the internet of um answers like this that are actually"},{"timestamp":"1:44:17","text":"coming from open from chasht and So that's its label for what it is now you"},{"timestamp":"1:44:23","text":"can override this as a developer if you have a llm model you can actually override it and there are a few ways to"},{"timestamp":"1:44:28","text":"do that so for example let me show you there's this MMO model from Allen Ai and"},{"timestamp":"1:44:35","text":"um this is one llm it's not a top tier LM or anything like that but I like it because it is fully open source so the"},{"timestamp":"1:44:41","text":"paper for Almo and everything else is completely fully open source which is nice um so here we are looking at its"},{"timestamp":"1:44:47","text":"sft mixture so this is the data mixture of um the fine tuning so this is the"},{"timestamp":"1:44:52","text":"conversations data it right and so the way that they are solving it for Theo model is we see that there's a bunch of"},{"timestamp":"1:44:58","text":"stuff in the mixture and there's a total of 1 million conversations here but here we have alot to hardcoded if we go there"},{"timestamp":"1:45:05","text":"we see that this is 240 conversations and look at these 240"},{"timestamp":"1:45:10","text":"conversations they're hardcoded tell me about yourself says user and then the"},{"timestamp":"1:45:15","text":"assistant says I'm and open language model developed by AI to Allen Institute of artificial intelligence Etc I'm here"},{"timestamp":"1:45:21","text":"to help blah blah blah what is your name uh Theo project so these are all kinds of like cooked up hardcoded questions"},{"timestamp":"1:45:27","text":"abouto 2 and the correct answers to give in these cases if you take 240 questions"},{"timestamp":"1:45:33","text":"like this or conversations put them into your training set and fine tune with it then the model will actually be expected"},{"timestamp":"1:45:39","text":"to parot this stuff later if you don't give it this then it's probably a Chach"},{"timestamp":"1:45:45","text":"by open Ai and um there's one more way to sometimes do this is"},{"timestamp":"1:45:51","text":"that basically um in these conversations and you have terms between human and"},{"timestamp":"1:45:56","text":"assistant sometimes there's a special message called system message at the very beginning of the conversation so"},{"timestamp":"1:46:02","text":"it's not just between human and assistant there's a system and in the system message you can actually hardcode"},{"timestamp":"1:46:07","text":"and remind the model that hey you are a model developed by open Ai and your name"},{"timestamp":"1:46:13","text":"is chashi pt40 and you were trained on this date and your knowledge cut off is this and basically it kind of like"},{"timestamp":"1:46:19","text":"documents the model a little bit and then this is inserted into to your conversations so when you go on chpt you"},{"timestamp":"1:46:25","text":"see a blank page but actually the system message is kind of like hidden in there and those tokens are in the context"},{"timestamp":"1:46:30","text":"window and so those are the two ways to kind of um program the models to talk"},{"timestamp":"1:46:35","text":"about themselves either it's done through uh data like this or it's done through system message and things like"},{"timestamp":"1:46:42","text":"that basically invisible tokens that are in the context window and remind the model of its identity but it's all just"},{"timestamp":"1:46:47","text":"kind of like cooked up and bolted on in some in some way it's not actually like really deeply there in any real sense as"},{"timestamp":"1:46:54","text":"it would before a human I want to now continue to the next section which deals with the computational capabilities or"},{"timestamp":"1:47:01","text":"like I should say the native computational capabilities of these models in problem solving scenarios and"},{"timestamp":"1:47:06","text":"so in particular we have to be very careful with these models when we construct our examples of conversations and there's a lot of sharp edges here"},{"timestamp":"1:47:13","text":"that are kind of like elucidative is that a word uh they're kind of like interesting to look at when we consider"},{"timestamp":"1:47:18","text":"how these models think so um consider the following prompt from a human and"},{"timestamp":"1:47:24","text":"supposed that basically that we are building out a conversation to enter into our training set of conversations so we're going to train the model on"},{"timestamp":"1:47:30","text":"this we're teaching you how to basically solve simple math problems so the prompt is Emily buys three apples and two"},{"timestamp":"1:47:36","text":"oranges each orange cost $2 the total cost is 13 what is the cost of apples very simple math question now there are"},{"timestamp":"1:47:43","text":"two answers here on the left and on the right they are both correct answers they both say that the answer is three which"},{"timestamp":"1:47:49","text":"is correct but one of these two is a significant ific anly better answer for the assistant than the other like if I"},{"timestamp":"1:47:56","text":"was Data labeler and I was creating one of these one of these would be uh a really terrible answer for the assistant"},{"timestamp":"1:48:03","text":"and the other would be okay and so I'd like you to potentially pause the video Even and think through why one of these"},{"timestamp":"1:48:09","text":"two is significantly better answer uh than the other and um if you use the"},{"timestamp":"1:48:14","text":"wrong one your model will actually be uh really bad at math potentially and it would have uh bad outcomes and this is"},{"timestamp":"1:48:21","text":"something that you would be careful with in your life labeling documentations when you are training people uh to create the ideal responses for the"},{"timestamp":"1:48:27","text":"assistant okay so the key to this question is to realize and remember that when the models are training and also"},{"timestamp":"1:48:34","text":"inferencing they are working in onedimensional sequence of tokens from left to right and this is the picture"},{"timestamp":"1:48:40","text":"that I often have in my mind I imagine basically the token sequence evolving from left to right and to always produce"},{"timestamp":"1:48:46","text":"the next token in a sequence we are feeding all these tokens into the neural network and this neural network then is"},{"timestamp":"1:48:53","text":"the probabilities for the next token and sequence right so this picture here is the exact same picture we saw uh before"},{"timestamp":"1:48:58","text":"up here and this comes from the web demo that I showed you before right so this"},{"timestamp":"1:49:04","text":"is the calculation that basically takes the input tokens here on the top and uh performs these operations of all these"},{"timestamp":"1:49:11","text":"neurons and uh gives you the answer for the probabilities of what comes next now the important thing to realize is that"},{"timestamp":"1:49:17","text":"roughly speaking uh there's basically a finite number of layers of computation that"},{"timestamp":"1:49:22","text":"happened here so for example this model here has only one two three layers of"},{"timestamp":"1:49:28","text":"what's called detention and uh MLP here um maybe um typical modern"},{"timestamp":"1:49:34","text":"state-of-the-art Network would have more like say 100 layers or something like that but there's only 100 layers of computation or something like that to go"},{"timestamp":"1:49:40","text":"from the previous token sequence to the probabilities for the next token and so there's a finite amount of computation"},{"timestamp":"1:49:46","text":"that happens here for every single token and you should think of this as a very small amount of computation and this"},{"timestamp":"1:49:52","text":"amount of computation is almost roughly fixed uh for every single token in this sequence um the that's not actually"},{"timestamp":"1:49:59","text":"fully true because the more tokens you feed in uh the the more expensive uh this forward pass will be of this neural"},{"timestamp":"1:50:06","text":"network but not by much so you should think of this uh and I think as a good model to have in mind this is a fixed"},{"timestamp":"1:50:12","text":"amount of compute that's going to happen in this box for every single one of these tokens and this amount of compute"},{"timestamp":"1:50:17","text":"Cann possibly be too big because there's not that many layers that are sort of going from the top to bottom here"},{"timestamp":"1:50:23","text":"there's not that that much computationally that will happen here and so you can't imagine the model to to basically do arbitrary computation in a"},{"timestamp":"1:50:29","text":"single forward pass to get a single token and so what that means is that we actually have to distribute our"},{"timestamp":"1:50:35","text":"reasoning and our computation across many tokens because every single token is only spending a finite amount of"},{"timestamp":"1:50:41","text":"computation on it and so we kind of want to distribute the computation across"},{"timestamp":"1:50:47","text":"many tokens and we can't have too much computation or expect too much computation out of of the model in any"},{"timestamp":"1:50:53","text":"single individual token because there's only so much computation that happens per token okay roughly fixed amount of"},{"timestamp":"1:51:00","text":"computation here so that's why this answer here is"},{"timestamp":"1:51:06","text":"significantly worse and the reason for that is Imagine going from left to right here um and I copy pasted it right here"},{"timestamp":"1:51:13","text":"the answer is three Etc imagine the model having to go from left to right emitting these tokens one at a time it"},{"timestamp":"1:51:19","text":"has to say or we're expecting to say the answer is space dollar sign and then"},{"timestamp":"1:51:27","text":"right here we're expecting it to basically cram all of the computation of this problem into this single token it has to emit the correct answer three and"},{"timestamp":"1:51:35","text":"then once we've emitted the answer three we're expecting it to say all these tokens but at this point we've already"},{"timestamp":"1:51:41","text":"prod produced the answer and it's already in the context window for all these tokens that follow so anything"},{"timestamp":"1:51:46","text":"here is just um kind of post Hawk justification of why this is the answer"},{"timestamp":"1:51:52","text":"um because the answer is already created it's already in the token window so it's it's not actually being calculated here"},{"timestamp":"1:51:58","text":"um and so if you are answering the question directly and immediately you are training the model to to try to"},{"timestamp":"1:52:06","text":"basically guess the answer in a single token and that is just not going to work because of the finite amount of"},{"timestamp":"1:52:11","text":"computation that happens per token that's why this answer on the right is significantly better because we are"},{"timestamp":"1:52:17","text":"Distributing this computation across the answer we're actually getting the model to sort of slowly come to the answer"},{"timestamp":"1:52:23","text":"from the left to right we're getting intermediate results we're saying okay the total cost of oranges is four so 30"},{"timestamp":"1:52:28","text":"- 4 is 9 and so we're creating intermediate calculations and each one"},{"timestamp":"1:52:34","text":"of these calculations is by itself not that expensive and so we're actually basically kind of guessing a little bit"},{"timestamp":"1:52:40","text":"the difficulty that the model is capable of in any single one of these individual tokens and there can never be too much"},{"timestamp":"1:52:47","text":"work in any one of these tokens computationally because then the model won't be able to do that later at test"},{"timestamp":"1:52:52","text":"time and so we're teaching the model here to spread out its reasoning and to spread out its computation over the"},{"timestamp":"1:52:59","text":"tokens and in this way it only has very simple problems in each token and they"},{"timestamp":"1:53:05","text":"can add up and then by the time it's near the end it has all the previous results in its working memory and it's"},{"timestamp":"1:53:11","text":"much easier for it to determine that the answer is and here it is three so this is a significantly better label for our"},{"timestamp":"1:53:18","text":"computation this would be really bad and is teaching the model to try to do all the computation in a single token and"},{"timestamp":"1:53:24","text":"it's really bad so uh that's kind of like an interesting thing to keep in mind is in"},{"timestamp":"1:53:30","text":"your prompts uh usually don't have to think about it explicitly because uh the"},{"timestamp":"1:53:36","text":"people at open AI have labelers and so on that actually worry about this and they make sure that the answers are"},{"timestamp":"1:53:41","text":"spread out and so actually open AI will kind of like do the right thing so when I ask this question for chat GPT it's"},{"timestamp":"1:53:48","text":"actually going to go very slowly it's going to be like okay let's define our variables set up the equation and it's kind of creating all these"},{"timestamp":"1:53:54","text":"intermediate results these are not for you these are for the model if the model is not creating these intermediate"},{"timestamp":"1:53:59","text":"results for itself it's not going to be able to reach three I also wanted to show you that it's possible to be a bit"},{"timestamp":"1:54:06","text":"mean to the model uh we can just ask for things so as an example I said I gave it the exact same uh prompt and I said"},{"timestamp":"1:54:13","text":"answer the question in a single token just immediately give me the answer nothing else and it turns out that for"},{"timestamp":"1:54:18","text":"this simple um prompt here it actually was able to do it in single go so it just created a single I think this is"},{"timestamp":"1:54:25","text":"two tokens right uh because the dollar sign is its own token so basically this"},{"timestamp":"1:54:30","text":"model didn't give me a single token it gave me two tokens but it still produced the correct answer and it did that in a"},{"timestamp":"1:54:35","text":"single forward pass of the network now that's because the numbers here I think are very simple and so I"},{"timestamp":"1:54:41","text":"made it a bit more difficult to be a bit mean to the model so I said Emily buys 23 apples and 177 oranges and then I"},{"timestamp":"1:54:48","text":"just made the numbers a bit bigger and I'm just making it harder for the model I'm asking it to more computation in a"},{"timestamp":"1:54:53","text":"single token and so I said the same thing and here it gave me five and five is actually not correct so the model"},{"timestamp":"1:55:00","text":"failed to do all of this calculation in a single forward pass of the network it failed to go from the input tokens and"},{"timestamp":"1:55:07","text":"then in a single forward pass of the network single go through the network it couldn't produce the result and then I"},{"timestamp":"1:55:13","text":"said okay now don't worry about the the token limit and just solve the problem as usual and then it goes all the"},{"timestamp":"1:55:20","text":"intermediate results it simplifies and every one of these intermediate results here and intermediate calculations is"},{"timestamp":"1:55:26","text":"much easier for the model and um it sort of it's not too much work per token all"},{"timestamp":"1:55:32","text":"of the tokens here are correct and it arises the solution which is seven and I just couldn't squeeze all of this work"},{"timestamp":"1:55:38","text":"it couldn't squeeze that into a single forward passive Network so I think that's kind of just a cute example and"},{"timestamp":"1:55:43","text":"something to kind of like think about and I think it's kind of again just elucidative in terms of how these uh"},{"timestamp":"1:55:48","text":"models work the last thing that I would say on this topic is that if I was in practi is trying to actually solve this in my day-to-day life I might actually"},{"timestamp":"1:55:55","text":"not uh trust that the model that all the intermediate calculations correctly here so actually probably what I do is"},{"timestamp":"1:56:01","text":"something like this I would come here and I would say use code and uh that's"},{"timestamp":"1:56:06","text":"because code is one of the possible tools that chachy PD can use and instead"},{"timestamp":"1:56:11","text":"of it having to do mental arithmetic like this mental arithmetic here I don't fully trust it and especially if the"},{"timestamp":"1:56:17","text":"numbers get really big there's no guarantee that the model will do this correctly any one of these intermediates"},{"timestamp":"1:56:22","text":"steps might in principle fail we're using neural networks to do mental arithmetic uh kind of like you doing"},{"timestamp":"1:56:27","text":"mental arithmetic in your brain it might just like uh screw up some of the intermediate results it's actually kind"},{"timestamp":"1:56:32","text":"of amazing that it can even do this kind of mental arithmetic I don't think I could do this in my head but basically the model is kind of like doing it in"},{"timestamp":"1:56:38","text":"its head and I don't trust that so I wanted to use tools so you can say stuff like use"},{"timestamp":"1:56:43","text":"code and uh I'm not sure what happened there use"},{"timestamp":"1:56:50","text":"code and so um like I mentioned there's a special tool and the uh the model can"},{"timestamp":"1:56:55","text":"write code and I can inspect that this code is correct and then uh it's not"},{"timestamp":"1:57:01","text":"relying on its mental arithmetic it is using the python interpreter which is a very simple programming language to"},{"timestamp":"1:57:07","text":"basically uh write out the code that calculates the result and I would personally trust this a lot more because"},{"timestamp":"1:57:12","text":"this came out of a Python program which I think has a lot more correctness guarantees than the mental arithmetic of"},{"timestamp":"1:57:17","text":"a language model uh so just um another kind of uh potential hint that if you"},{"timestamp":"1:57:23","text":"have these kinds of problems uh you may want to basically just uh ask the model to use the code interpreter and just"},{"timestamp":"1:57:28","text":"like we saw with the web search the model has special uh kind of tokens for"},{"timestamp":"1:57:34","text":"calling uh like it will not actually generate these tokens from the language model it will write the program and then"},{"timestamp":"1:57:40","text":"it actually sends that program to a different sort of part of the computer that actually just runs that program and"},{"timestamp":"1:57:46","text":"brings back the result and then the model gets access to that result and can tell you that okay the cost of each apple is seven"},{"timestamp":"1:57:53","text":"um so that's another kind of tool and I would use this in practice for yourself and it's um yeah it's just uh less error"},{"timestamp":"1:58:01","text":"prone I would say so that's why I called this section models need tokens to think distribute your competition across many"},{"timestamp":"1:58:08","text":"tokens ask models to create intermediate results or whenever you can lean on"},{"timestamp":"1:58:13","text":"tools and Tool use instead of allowing the models to do all of the stuff in their memory so if they try to do it all"},{"timestamp":"1:58:18","text":"in their memory I don't fully trust it and prefer to use tools whenever possible I want to show you one more"},{"timestamp":"1:58:24","text":"example of where this actually comes up and that's in counting so models actually are not very good at counting"},{"timestamp":"1:58:30","text":"for the exact same reason you're asking for way too much in a single individual token so let me show you a simple"},{"timestamp":"1:58:36","text":"example of that um how many dots are below and then I just put in a bunch of dots and Chach says there are and then"},{"timestamp":"1:58:44","text":"it just tries to solve the problem in a single token so in a single token it has to count the number of dots in its"},{"timestamp":"1:58:51","text":"context window um and it has to do that in the single forward pass of a network and a single"},{"timestamp":"1:58:57","text":"forward pass of a network as we talked about there's not that much computation that can happen there just think of that as being like very little competation"},{"timestamp":"1:59:03","text":"that happens there so if I just look at what the model sees let's go to the LM"},{"timestamp":"1:59:09","text":"go to tokenizer it sees uh this how many dots are below and then it"},{"timestamp":"1:59:15","text":"turns out that these dots here this group of I think 20 dots is a single token and then this group of whatever it"},{"timestamp":"1:59:22","text":"is is another token and then for some reason they break up as this so I don't"},{"timestamp":"1:59:28","text":"actually this has to do with the details of the tokenizer but it turns out that these um the model basically sees the"},{"timestamp":"1:59:34","text":"token ID this this this and so on and then from these token IDs it's expected"},{"timestamp":"1:59:40","text":"to count the number and spoiler alert is not 161 it's actually I believe"},{"timestamp":"1:59:45","text":"177 so here's what we can do instead uh we can say use code and you might expect"},{"timestamp":"1:59:51","text":"that like why should this work and it's actually kind of subtle and kind of interesting so when I say use code I"},{"timestamp":"1:59:57","text":"actually expect this to work let's see okay 177 is correct so what happens here"},{"timestamp":"2:00:02","text":"is I've actually it doesn't look like it but I've broken down the problem into a"},{"timestamp":"2:00:08","text":"problems that are easier for the model I know that the model can't count it can't do mental counting but I know that the"},{"timestamp":"2:00:14","text":"model is actually pretty good at doing copy pasting so what I'm doing here is when I say use code it creates a string"},{"timestamp":"2:00:20","text":"in Python for this and the task of basically copy pasting my input here to"},{"timestamp":"2:00:27","text":"here is very simple because for the model um it sees this string of uh it"},{"timestamp":"2:00:33","text":"sees it as just these four tokens or whatever it is so it's very simple for the model to copy paste those token IDs"},{"timestamp":"2:00:40","text":"and um kind of unpack them into Dots here and so it creates this string and"},{"timestamp":"2:00:47","text":"then it calls python routine. count and then it comes up with the correct answer so the python interpreter is doing the"},{"timestamp":"2:00:53","text":"counting it's not the models mental arithmetic doing the counting so it's again a simple example of um models need"},{"timestamp":"2:01:00","text":"tokens to think don't rely on their mental arithmetic and um that's why also the models are not very good at counting"},{"timestamp":"2:01:07","text":"if you need them to do counting tasks always ask them to lean on the tool now the models also have many other little"},{"timestamp":"2:01:13","text":"cognitive deficits here and there and these are kind of like sharp edges of the technology to be kind of aware of over time so as an example the models"},{"timestamp":"2:01:20","text":"are not very good with all kinds of spelling related tasks they're not very good at it and I told you that we would"},{"timestamp":"2:01:26","text":"loop back around to tokenization and the reason to do for this is that the models they don't see the characters they see"},{"timestamp":"2:01:33","text":"tokens and they their entire world is about tokens which are these little text chunks and so they don't see characters"},{"timestamp":"2:01:39","text":"like our eyes do and so very simple character level tasks often fail so for"},{"timestamp":"2:01:45","text":"example uh I'm giving it a string ubiquitous and I'm asking it to print only every third character starting with"},{"timestamp":"2:01:51","text":"the first one so we start with U and then we should go every third so every"},{"timestamp":"2:01:56","text":"so 1 2 3 Q should be next and then Etc so this I see is not correct and again"},{"timestamp":"2:02:03","text":"my hypothesis is that this is again Dental arithmetic here is failing number one a little bit but number two I think"},{"timestamp":"2:02:10","text":"the the more important issue here is that if you go to Tik tokenizer and you look at ubiquitous we"},{"timestamp":"2:02:16","text":"see that it is three tokens right so you and I see ubiquitous and we can easily access the individual letters because we"},{"timestamp":"2:02:23","text":"kind of see them and when we have it in the working memory of our visual sort of field we can really easily index into"},{"timestamp":"2:02:29","text":"every third letter and I can do that task but the models don't have access to the individual letters they see this as"},{"timestamp":"2:02:35","text":"these three tokens and uh remember these models are trained from scratch on the internet and all these token uh"},{"timestamp":"2:02:42","text":"basically the model has to discover how many of all these different letters are packed into all these different tokens"},{"timestamp":"2:02:47","text":"and the reason we even use tokens is mostly for efficiency uh but I think a lot of people areed interested to delete tokens entirely like we should really"},{"timestamp":"2:02:54","text":"have character level or bite level models it's just that that would create very long sequences and people don't"},{"timestamp":"2:02:59","text":"know how to deal with that right now so while we have the token World any kind of spelling tasks are not actually"},{"timestamp":"2:03:05","text":"expected to work super well so because I know that spelling is not a strong suit because of tokenization I can again Ask"},{"timestamp":"2:03:11","text":"it to lean On Tools so I can just say use code and I would again expect this to work because the task of copy pasting"},{"timestamp":"2:03:18","text":"ubiquitous into the python interpreter is much easier and then we're leaning on python interpreter to manipulate the"},{"timestamp":"2:03:25","text":"characters of this string so when I say use code"},{"timestamp":"2:03:30","text":"ubiquitous yes it indexes into every third character and the actual truth is u2s"},{"timestamp":"2:03:36","text":"uqs uh which looks correct to me so um again an example of spelling related"},{"timestamp":"2:03:42","text":"tasks not working very well a very famous example of that recently is how many R are there in strawberry and this"},{"timestamp":"2:03:49","text":"went viral many times and basically the models now get it correct they say there are three Rs in Strawberry but for a"},{"timestamp":"2:03:55","text":"very long time all the state-of-the-art models would insist that there are only two RS in strawberry and this caused a"},{"timestamp":"2:04:00","text":"lot of you know Ruckus because is that a word I think so because um it just kind"},{"timestamp":"2:04:06","text":"of like why are the models so brilliant and they can solve math Olympiad questions but they can't like count RS"},{"timestamp":"2:04:12","text":"in strawberry and the answer for that again is I've got built up to it kind of slowly but number one the models don't"},{"timestamp":"2:04:18","text":"see characters they see tokens and number two they are not very good at counting and so here we are combining"},{"timestamp":"2:04:25","text":"the difficulty of seeing the characters with the difficulty of counting and that's why the models struggled with"},{"timestamp":"2:04:30","text":"this even though I think by now honestly I think open I may have hardcoded the answer here or I'm not sure what they"},{"timestamp":"2:04:35","text":"did but um uh but this specific query now works"},{"timestamp":"2:04:41","text":"so models are not very good at spelling and there there's a bunch of other little sharp edges and I don't want to"},{"timestamp":"2:04:46","text":"go into all of them I just want to show you a few examples of things to be aware of and uh when you're using these models"},{"timestamp":"2:04:52","text":"in practice I don't actually want to have a comprehensive analysis here of all the ways that the models are kind of"},{"timestamp":"2:04:57","text":"like falling short I just want to make the point that there are some Jagged edges here and there and we've discussed"},{"timestamp":"2:05:03","text":"a few of them and a few of them make sense but some of them also will just not make as much sense and they're kind of like you're left scratching your head"},{"timestamp":"2:05:10","text":"even if you understand in- depth how these models work and and good example of that recently is the following uh the"},{"timestamp":"2:05:16","text":"models are not very good at very simple questions like this and uh this is shocking to a lot of people because"},{"timestamp":"2:05:22","text":"these math uh these problems can solve complex math problems they can answer PhD grade physics chemistry biology"},{"timestamp":"2:05:28","text":"questions much better than I can but sometimes they fall short in like super simple problems like this so here we go"},{"timestamp":"2:05:34","text":"9.11 is bigger than 9.9 and it justifies it in some way but obviously and then at"},{"timestamp":"2:05:40","text":"the end okay it actually it flips its decision later so um I don't believe"},{"timestamp":"2:05:47","text":"that this is very reproducible sometimes it flips around its answer sometimes gets it right sometimes get it get it wrong uh let's try"},{"timestamp":"2:05:56","text":"again okay even though it might look larger okay so here it doesn't even correct itself in the end if you ask"},{"timestamp":"2:06:03","text":"many times sometimes it gets it right too but how is it that the model can do so great at Olympiad grade problems but"},{"timestamp":"2:06:10","text":"then fail on very simple problems like this and uh I think this one is as I"},{"timestamp":"2:06:15","text":"mentioned a little bit of a head scratcher it turns out that a bunch of people studied this in depth and I haven't actually read the paper uh but"},{"timestamp":"2:06:22","text":"what I was told by this team was that when you scrutinize the activations"},{"timestamp":"2:06:27","text":"inside the neural network when you look at some of the features and what what features turn on or off and what neurons"},{"timestamp":"2:06:33","text":"turn on or off uh a bunch of neurons inside the neural network light up that are usually associated with Bible verses"},{"timestamp":"2:06:40","text":"U and so I think the model is kind of like reminded that these almost look like Bible verse markers and in a bip"},{"timestamp":"2:06:48","text":"verse setting 9.11 would come after 99.9 and so basically the model somehow finds"},{"timestamp":"2:06:53","text":"it like cognitively very distracting that in Bible verses 9.11 would be greater um even though here it's"},{"timestamp":"2:07:00","text":"actually trying to justify it and come up to the answer with a math it still ends up with the wrong answer here so it"},{"timestamp":"2:07:07","text":"basically just doesn't fully make sense and it's not fully understood and um"},{"timestamp":"2:07:12","text":"there's a few Jagged issues like that so that's why treat this as a as what it is which is a St stochastic system that is"},{"timestamp":"2:07:19","text":"really magical but that you can't also fully trust and you want to use it as a tool not as something that you kind of"},{"timestamp":"2:07:25","text":"like letter rip on a problem and copypaste the results okay so we have now covered two major stages of training"},{"timestamp":"2:07:32","text":"of large language models we saw that in the first stage this is called the pre-training stage we are basically"},{"timestamp":"2:07:38","text":"training on internet documents and when you train a language model on internet documents you get what's called a base"},{"timestamp":"2:07:44","text":"model and it's basically an internet document simulator right now we saw that this is an interesting artifact and uh"},{"timestamp":"2:07:51","text":"this takes many months to train on thousands of computers and it's kind of a lossy compression of the internet and"},{"timestamp":"2:07:57","text":"it's extremely interesting but it's not directly useful because we don't want to sample internet documents we want to ask"},{"timestamp":"2:08:02","text":"questions of an AI and have it respond to our questions so for that we need an assistant and we saw that we can"},{"timestamp":"2:08:09","text":"actually construct an assistant in the process of a post training and specifically in the process"},{"timestamp":"2:08:16","text":"of supervised fine-tuning as we call it so in this stage we saw that it's"},{"timestamp":"2:08:22","text":"algorithmically identical to pre-training nothing is going to change the only thing that changes is the data set so instead of Internet documents we"},{"timestamp":"2:08:30","text":"now want to create and curate a very nice data set of conversations so we"},{"timestamp":"2:08:35","text":"want Millions conversations on all kinds of diverse topics between a human and an"},{"timestamp":"2:08:41","text":"assistant and fundamentally these conversations are created by humans so"},{"timestamp":"2:08:47","text":"humans write the prompts and humans write the ideal response responses and"},{"timestamp":"2:08:52","text":"they do that based on labeling documentations now in the modern stack"},{"timestamp":"2:08:57","text":"it's not actually done fully and manually by humans right they actually now have a lot of help from these tools"},{"timestamp":"2:09:02","text":"so we can use language models um to help us create these data sets and that's done extensively but fundamentally it's"},{"timestamp":"2:09:09","text":"all still coming from Human curation at the end so we create these conversations that now becomes our data set we fine"},{"timestamp":"2:09:15","text":"tune on it or continue training on it and we get an assistant and then we kind of shifted gears and started talking"},{"timestamp":"2:09:21","text":"about some of the kind of cognitive implications of what this assistant is like and we saw that for example the"},{"timestamp":"2:09:26","text":"assistant will hallucinate if you don't take some sort of mitigations towards it"},{"timestamp":"2:09:32","text":"so we saw that hallucinations would be common and then we looked at some of the mitigations of those hallucinations and"},{"timestamp":"2:09:38","text":"then we saw that the models are quite impressive and can do a lot of stuff in their head but we saw that they can also Lean On Tools to become better so for"},{"timestamp":"2:09:45","text":"example we can lo lean on a web search in order to hallucinate less and to"},{"timestamp":"2:09:50","text":"maybe bring up some more um recent information or something like that or we can lean on tools like code interpreter"},{"timestamp":"2:09:57","text":"so the code can so the llm can write some code and actually run it and see the results so these are some of the topics"},{"timestamp":"2:10:03","text":"we looked at so far um now what I'd like to do is I'd like to cover the last and"},{"timestamp":"2:10:09","text":"major stage of this Pipeline and that is reinforcement learning so reinforcement"},{"timestamp":"2:10:15","text":"learning is still kind of thought to be under the umbrella of posttraining uh but it is the last third major stage and"},{"timestamp":"2:10:22","text":"it's a different way of training language models and usually follows as this third step so inside companies like"},{"timestamp":"2:10:29","text":"open AI you will start here and these are all separate teams so there's a team doing data for pre-training and a team"},{"timestamp":"2:10:35","text":"doing training for pre-training and then there's a team doing all the conversation generation in a in a"},{"timestamp":"2:10:42","text":"different team that is kind of doing the supervis fine tuning and there will be a team for the reinforcement learning as"},{"timestamp":"2:10:47","text":"well so it's kind of like a handoff of these models you get your base model the then you find you need to be an assistant and then you go into"},{"timestamp":"2:10:53","text":"reinforcement learning which we'll talk about uh now so that's kind of like the major"},{"timestamp":"2:10:58","text":"flow and so let's now focus on reinforcement learning the last major stage of training and let me first"},{"timestamp":"2:11:05","text":"actually motivate it and why we would want to do reinforcement learning and what it looks like on a high level so I"},{"timestamp":"2:11:11","text":"would now like to try to motivate the reinforcement learning stage and what it corresponds to with something that you're probably familiar with and that"},{"timestamp":"2:11:16","text":"is basically going to school so just like you went to school to become um really good at something we want to take"},{"timestamp":"2:11:23","text":"large language models through school and really what we're doing is um we're um"},{"timestamp":"2:11:29","text":"we have a few paradigms of ways of uh giving them knowledge or transferring skills so in particular when we're"},{"timestamp":"2:11:36","text":"working with textbooks in school you'll see that there are three major kind of uh pieces of information in these"},{"timestamp":"2:11:42","text":"textbooks three classes of information the first thing you'll see is you'll see a lot of exposition um and by the way"},{"timestamp":"2:11:49","text":"this is a totally random book I pulled from the internet I I think it's some kind of an organic chemistry or something I'm not sure uh but the"},{"timestamp":"2:11:55","text":"important thing is that you'll see that most of the text most of it is kind of just like the meat of it is exposition"},{"timestamp":"2:12:00","text":"it's kind of like background knowledge Etc as you are reading through the words of this Exposition you can think of that"},{"timestamp":"2:12:08","text":"roughly as training on that data so um and that's why when you're reading"},{"timestamp":"2:12:13","text":"through this stuff this background knowledge and this all this context information it's kind of equivalent to"},{"timestamp":"2:12:18","text":"pre-training so it's it's where we build sort of like a knowledge base of this data and get a sense of the topic the"},{"timestamp":"2:12:27","text":"next major kind of information that you will see is these uh problems and with"},{"timestamp":"2:12:32","text":"their worked Solutions so basically a human expert in this case uh the author of this book has given us not just a"},{"timestamp":"2:12:39","text":"problem but has also worked through the solution and the solution is basically like equivalent to having like this"},{"timestamp":"2:12:45","text":"ideal response for an assistant so it's basically the expert is showing us how to solve the problem in it's uh kind of"},{"timestamp":"2:12:52","text":"like um in its full form so as we are reading the solution we are basically"},{"timestamp":"2:12:57","text":"training on the expert data and then later we can try to imitate the expert"},{"timestamp":"2:13:03","text":"um and basically um that's that roughly correspond to having the sft model"},{"timestamp":"2:13:08","text":"that's what it would be doing so basically we've already done pre-training and we've already covered"},{"timestamp":"2:13:14","text":"this um imitation of experts and how they solve these problems and the third"},{"timestamp":"2:13:19","text":"stage of reinforcement learning is basically the practice problems so sometimes you'll see this is just a"},{"timestamp":"2:13:25","text":"single practice problem here but of course there will be usually many practice problems at the end of each chapter in any textbook and practice"},{"timestamp":"2:13:32","text":"problems of course we know are critical for learning because what are they getting you to do they're getting you to practice uh to practice yourself and"},{"timestamp":"2:13:39","text":"discover ways of solving these problems yourself and so what you get in a practice problem is you get a problem"},{"timestamp":"2:13:46","text":"description but you're not given the solution but you are given the final answer answer usually in the answer key"},{"timestamp":"2:13:53","text":"of the textbook and so you know the final answer that you're trying to get to and you have the problem statement"},{"timestamp":"2:13:58","text":"but you don't have the solution you are trying to practice the solution you're trying out many different things and"},{"timestamp":"2:14:04","text":"you're seeing what gets you to the final solution the best and so you're"},{"timestamp":"2:14:09","text":"discovering how to solve these problems so and in the process of that you're relying on number one the background"},{"timestamp":"2:14:14","text":"information which comes from pre-training and number two maybe a little bit of imitation of human experts"},{"timestamp":"2:14:20","text":"and you can probably try similar kinds of solutions and so on so we've done this and this and now in this section"},{"timestamp":"2:14:27","text":"we're going to try to practice and so we're going to be given prompts we're going to be given Solutions U sorry the"},{"timestamp":"2:14:34","text":"final answers but we're not going to be given expert Solutions we have to practice and try stuff out and that's"},{"timestamp":"2:14:40","text":"what reinforcement learning is about okay so let's go back to the problem that we worked with previously just so"},{"timestamp":"2:14:46","text":"we have a concrete example to talk through as we explore sort of the topic here so um I'm here in the Teck"},{"timestamp":"2:14:52","text":"tokenizer because I'd also like to well I get a text box which is useful but number two I want to remind you again"},{"timestamp":"2:14:59","text":"that we're always working with onedimensional token sequences and so um I actually like prefer this view because"},{"timestamp":"2:15:04","text":"this is like the native view of the llm if that makes sense like this is what it actually sees it sees token IDs right"},{"timestamp":"2:15:11","text":"okay so Emily buys three apples and two oranges each orange is $2 the total cost"},{"timestamp":"2:15:17","text":"of all the fruit is $13 what is the cost of each apple and what I'd like to what I like you to"},{"timestamp":"2:15:23","text":"appreciate here is these are like four possible candidate Solutions as an"},{"timestamp":"2:15:29","text":"example and they all reach the answer three now what I'd like you to appreciate at this point is that if I am"},{"timestamp":"2:15:35","text":"the human data labeler that is creating a conversation to be entered into the training set I don't actually really"},{"timestamp":"2:15:42","text":"know which of these conversations to um to add to the data"},{"timestamp":"2:15:48","text":"set some of these conversations kind of set up a system equations some of them sort of like just talk through it in"},{"timestamp":"2:15:54","text":"English and some of them just kind of like skip right through to the solution um if you look at chbt for"},{"timestamp":"2:16:00","text":"example and you give it this question it defines a system of variables and it kind of like does this little thing what"},{"timestamp":"2:16:07","text":"we have to appreciate and uh differentiate between though is um the"},{"timestamp":"2:16:12","text":"first purpose of a solution is to reach the right answer of course we want to get the final answer three that is the"},{"timestamp":"2:16:17","text":"that is the important purpose here but there's kind of like a secondary purpose as well where here we are also just kind"},{"timestamp":"2:16:23","text":"of trying to make it like nice uh for the human because we're kind of assuming that the person wants to see the"},{"timestamp":"2:16:29","text":"solution they want to see the intermediate steps we want to present it nicely Etc so there are two separate things going on here number one is the"},{"timestamp":"2:16:36","text":"presentation for the human but number two we're trying to actually get the right answer um so let's for the moment"},{"timestamp":"2:16:42","text":"focus on just reaching the final answer if we're only care if we only care about the final answer then which of these is"},{"timestamp":"2:16:49","text":"the optimal or the best prompt um sorry the best solution for the llm to reach"},{"timestamp":"2:16:56","text":"the right answer um and what I'm trying to get at is we don't know me as a human labeler I"},{"timestamp":"2:17:03","text":"would not know which one of these is best so as an example we saw earlier on when we looked at"},{"timestamp":"2:17:09","text":"um the token sequences here and the mental arithmetic and reasoning we saw that for each token we can only spend"},{"timestamp":"2:17:15","text":"basically a finite number of finite amount of compute here that is not very large or you should think about it that way way and so we can't actually make"},{"timestamp":"2:17:23","text":"too big of a leap in any one token is is maybe the way to think about it so as an"},{"timestamp":"2:17:28","text":"example in this one what's really nice about it is that it's very few tokens so it's going to take us very short amount"},{"timestamp":"2:17:34","text":"of time to get to the answer but right here when we're doing 30 - 4 IDE 3"},{"timestamp":"2:17:39","text":"equals right in this token here we're actually asking for a lot of computation to happen on that single individual"},{"timestamp":"2:17:45","text":"token and so maybe this is a bad example to give to the llm because it's kind of incentivizing it to skip through the"},{"timestamp":"2:17:50","text":"calculations very quickly and it's going to actually make up mistakes make mistakes in this mental arithmetic uh so"},{"timestamp":"2:17:56","text":"maybe it would work better to like spread out the spread it out more maybe it would be better to set it up as an"},{"timestamp":"2:18:02","text":"equation maybe it would be better to talk through it we fundamentally don't know and we don't know because what is"},{"timestamp":"2:18:09","text":"easy for you or I as or as human labelers what's easy for us or hard for us is different than what's easy or hard"},{"timestamp":"2:18:16","text":"for the llm it cognition is different um and the token sequences are kind of like"},{"timestamp":"2:18:23","text":"different hard for it and so some of the token sequences here that are trivial"},{"timestamp":"2:18:30","text":"for me might be um very too much of a leap for the llm so right here this"},{"timestamp":"2:18:36","text":"token would be way too hard but conversely many of the tokens that I'm creating here might be just trivial to"},{"timestamp":"2:18:43","text":"the llm and we're just wasting tokens like why waste all these tokens when this is all trivial so if the only thing"},{"timestamp":"2:18:49","text":"we care care about is the final answer and we're separating out the issue of the presentation to the human um then we"},{"timestamp":"2:18:56","text":"don't actually really know how to annotate this example we don't know what solution to get to the llm because we are not the"},{"timestamp":"2:19:02","text":"llm and it's clear here in the case of like the math example but this is actually like a very pervasive issue"},{"timestamp":"2:19:08","text":"like for our knowledge is not lm's knowledge like the llm actually has a ton of knowledge of PhD in math and"},{"timestamp":"2:19:15","text":"physics chemistry and whatnot so in many ways it actually knows more than I do and I'm I'm potentially not utilizing"},{"timestamp":"2:19:21","text":"that knowledge in its problem solving but conversely I might be injecting a bunch of knowledge in my solutions that"},{"timestamp":"2:19:28","text":"the LM doesn't know in its parameters and then those are like sudden leaps"},{"timestamp":"2:19:33","text":"that are very confusing to the model and so our cognitions are different and I"},{"timestamp":"2:19:38","text":"don't really know what to put here if all we care about is the reaching the final solution and doing it economically"},{"timestamp":"2:19:45","text":"ideally and so long story short we are not in a good position to create these"},{"timestamp":"2:19:52","text":"uh token sequences for the LM and they're useful by imitation to initialize the system but we really want"},{"timestamp":"2:19:59","text":"the llm to discover the token sequences that work for it we need to find it needs to find for itself what token"},{"timestamp":"2:20:06","text":"sequence reliably gets to the answer given the prompt and it needs to discover that in the process of"},{"timestamp":"2:20:12","text":"reinforcement learning and of trial and error so let's see how this example"},{"timestamp":"2:20:18","text":"would work like in reinforcement learning okay so we're now back in the huging"},{"timestamp":"2:20:23","text":"face inference playground and uh that just allows me to very easily call uh"},{"timestamp":"2:20:28","text":"different kinds of models so as an example here on the top right I chose the Gemma 2 2 billion parameter model so"},{"timestamp":"2:20:34","text":"two billion is very very small so this is a tiny model but it's okay so we're going to give it um the way that"},{"timestamp":"2:20:40","text":"reinforcement learning will basically work is actually quite quite simple um we need to try many different kinds of"},{"timestamp":"2:20:47","text":"solutions and we want to see which Solutions work well or not so we're basically going to take the"},{"timestamp":"2:20:53","text":"prompt we're going to run the model and the model generates a solution"},{"timestamp":"2:20:58","text":"and then we're going to inspect the solution and we know that the correct answer for this one is $3 and so indeed"},{"timestamp":"2:21:05","text":"the model gets it correct it says it's $3 so this is correct so that's just one attempt at DIS solution so now we're"},{"timestamp":"2:21:11","text":"going to delete this and we're going to rerun it again let's try a second attempt so the model solves it in a bit"},{"timestamp":"2:21:17","text":"slightly different way right every single attempt will be a different generation because these models are"},{"timestamp":"2:21:23","text":"stochastic systems remember that at every single token here we have a probability distribution and we're sampling from that distribution so we"},{"timestamp":"2:21:29","text":"end up kind kind of going down slightly different paths and so this is a second solution that also ends in the correct"},{"timestamp":"2:21:36","text":"answer now we're going to delete that let's go a third time okay so again slightly different"},{"timestamp":"2:21:42","text":"solution but also gets it correct now we can actually repeat this uh many times and so in practice you"},{"timestamp":"2:21:49","text":"might actually sample thousand of independent Solutions or even like million solutions for just a single"},{"timestamp":"2:21:55","text":"prompt um and some of them will be correct and some of them will not be very correct and basically what we want"},{"timestamp":"2:22:00","text":"to do is we want to encourage the solutions that lead to correct answers so let's take a look at what that looks"},{"timestamp":"2:22:06","text":"like so if we come back over here here's kind of like a cartoon diagram of what this is looking like we have a prompt"},{"timestamp":"2:22:13","text":"and then we tried many different solutions in parallel and some of the solutions um"},{"timestamp":"2:22:19","text":"might go well so they get the right answer which is in green and some of the solutions might go poorly and may not"},{"timestamp":"2:22:25","text":"reach the right answer which is red now this problem here unfortunately is not the best example because it's a trivial"},{"timestamp":"2:22:32","text":"prompt and as we saw uh even like a two billion parameter model always gets it right so it's not the best example in"},{"timestamp":"2:22:38","text":"that sense but let's just exercise some imagination here and let's just suppose"},{"timestamp":"2:22:43","text":"that the um green ones are good and the red ones are bad okay so we generated 15 Solutions"},{"timestamp":"2:22:52","text":"only four of them got the right answer and so now what we want to do is basically we want to encourage the kinds"},{"timestamp":"2:22:58","text":"of solutions that lead to right answers so whatever token sequences happened in"},{"timestamp":"2:23:03","text":"these red Solutions obviously something went wrong along the way somewhere and uh this was not a good path to take"},{"timestamp":"2:23:09","text":"through the solution and whatever token sequences there were in these Green Solutions well things went uh pretty"},{"timestamp":"2:23:15","text":"well in this situation and so we want to do more things like it in prompts like"},{"timestamp":"2:23:21","text":"this and the way we encourage this kind of a behavior in the future is we basically train on these sequences um"},{"timestamp":"2:23:28","text":"but these training sequencies now are not coming from expert human annotators there's no human who decided that this"},{"timestamp":"2:23:33","text":"is the correct solution this solution came from the model itself so the model is practicing here it's tried out a few"},{"timestamp":"2:23:40","text":"Solutions four of them seem to have worked and now the model will kind of like train on them and this corresponds"},{"timestamp":"2:23:45","text":"to a student basically looking at their Solutions and being like okay well this one worked really well so this is this is how I should be solving these kinds"},{"timestamp":"2:23:52","text":"of problems and uh here in this example there are many different ways to"},{"timestamp":"2:23:57","text":"actually like really tweak the methodology a little bit here but just to give the core idea across maybe it's simplest to just think about take the"},{"timestamp":"2:24:04","text":"taking the single best solution out of these four uh like say this one that's why it was yellow uh so this is the the"},{"timestamp":"2:24:12","text":"solution that not only led to the right answer but may maybe had some other nice properties maybe it was the shortest one"},{"timestamp":"2:24:17","text":"or it looked nicest in some ways or uh there's other criteria you could think of as an example but we're going to"},{"timestamp":"2:24:23","text":"decide that this the top solution we're going to train on it and then uh the model will be slightly more likely once"},{"timestamp":"2:24:30","text":"you do the parameter update to take this path in this kind of a setting in the"},{"timestamp":"2:24:36","text":"future but you have to remember that we're going to run many different diverse prompts across lots of math"},{"timestamp":"2:24:42","text":"problems and physics problems and whatever wherever there might be so tens of thousands of prompts maybe have in"},{"timestamp":"2:24:47","text":"mind there's thousands of solutions prompt and so this is all happening kind of like at the same time and as we're"},{"timestamp":"2:24:55","text":"iterating this process the model is discovering for itself what kinds of token sequences lead it to correct"},{"timestamp":"2:25:02","text":"answers it's not coming from a human annotator the the model is kind of like"},{"timestamp":"2:25:08","text":"playing in this playground and it knows what it's trying to get to and it's discovering sequences that work for it"},{"timestamp":"2:25:15","text":"uh these are sequences that don't make any mental leaps uh they they seem to work reliably and statistically and uh"},{"timestamp":"2:25:23","text":"fully utilize the knowledge of the model as it has it and so uh this is the process of reinforcement"},{"timestamp":"2:25:29","text":"learning it's basically a guess and check we're going to guess many different types of solutions we're going to check them and we're going to do more"},{"timestamp":"2:25:35","text":"of what worked in the future and that is uh reinforcement learning so in the"},{"timestamp":"2:25:40","text":"context of what came before we see now that the sft model the supervised fine tuning model it's still helpful because"},{"timestamp":"2:25:47","text":"it still kind of like initializes the model a little bit into to the vicinity of the correct Solutions so it's kind of"},{"timestamp":"2:25:53","text":"like a initialization of um of the model in the sense that it kind of gets the model to you know take Solutions like"},{"timestamp":"2:26:00","text":"write out Solutions and maybe it has an understanding of setting up a system of equations or maybe it kind of like talks"},{"timestamp":"2:26:06","text":"through a solution so it gets you into the vicinity of correct Solutions but reinforcement learning is where"},{"timestamp":"2:26:11","text":"everything gets dialed in we really discover the solutions that work for the model get the right answers we encourage"},{"timestamp":"2:26:17","text":"them and then the model just kind of like gets better over time time okay so that is the high Lev process for how we"},{"timestamp":"2:26:23","text":"train large language models in short we train them kind of very similar to how we train children and basically the only"},{"timestamp":"2:26:30","text":"difference is that children go through chapters of books and they do all these different types of training exercises um"},{"timestamp":"2:26:37","text":"kind of within the chapter of each book but instead when we train AIS it's almost like we kind of do it stage by"},{"timestamp":"2:26:43","text":"stage depending on the type of that stage so first what we do is we do pre-training which as we saw is"},{"timestamp":"2:26:49","text":"equivalent to uh basically reading all the expository material so we look at all the textbooks at the same time and"},{"timestamp":"2:26:55","text":"we read all the exposition and we try to build a knowledge base the second thing then is we go into the sft stage which"},{"timestamp":"2:27:02","text":"is really looking at all the fixed uh sort of like solutions from Human Experts of all the different kinds of"},{"timestamp":"2:27:09","text":"worked Solutions across all the textbooks and we just kind of get an sft model which is able to imitate the"},{"timestamp":"2:27:16","text":"experts but does so kind of blindly it just kind of like does its best guess uh kind of just like trying to mimic"},{"timestamp":"2:27:22","text":"statistically the expert behavior and so that's what you get when you look at all the work Solutions and then finally in"},{"timestamp":"2:27:28","text":"the last stage we do all the practice problems in the RL stage across all the textbooks we only do the practice"},{"timestamp":"2:27:35","text":"problems and that's how we get the RL model so on a high level the way we train llms is very much equivalent uh to"},{"timestamp":"2:27:43","text":"the process that we train uh that we use for training of children the next point I would like to make is that actually"},{"timestamp":"2:27:49","text":"these first two stat ages pre-training and surprise fine-tuning they've been around for years and they are very standard and everyone does them all the"},{"timestamp":"2:27:55","text":"different llm providers it is this last stage the RL training that is a lot more"},{"timestamp":"2:28:00","text":"early in its process of development and is not standard yet in the field and so"},{"timestamp":"2:28:06","text":"um this stage is a lot more kind of early and nent and the reason for that"},{"timestamp":"2:28:11","text":"is because I actually skipped over a ton of little details here in this process the high level idea is very simple it's"},{"timestamp":"2:28:17","text":"trial and there learning but there's a ton of details and little math mathematical kind of like nuances to exactly how you pick the solutions that"},{"timestamp":"2:28:23","text":"are the best and how much you train on them and what is the prompt distribution and how to set up the training run such"},{"timestamp":"2:28:29","text":"that this actually works so there's a lot of little details and knobs to the core idea that is very very simple and"},{"timestamp":"2:28:35","text":"so getting the details right here uh is not trivial and so a lot of companies like for example open and other LM"},{"timestamp":"2:28:41","text":"providers have experimented internally with reinforcement learning fine tuning for llms for a while but they've not"},{"timestamp":"2:28:48","text":"talked about it publicly um it's all kind of done inside the company and so that's why the paper from"},{"timestamp":"2:28:55","text":"Deep seek that came out very very recently was such a big deal because this is a paper from this company called"},{"timestamp":"2:29:01","text":"DC Kai in China and this paper really talked very publicly about reinforcement"},{"timestamp":"2:29:07","text":"learning fine training for large language models and how incredibly important it is for large language"},{"timestamp":"2:29:12","text":"models and how it brings out a lot of reasoning capabilities in the models we'll go into this in a second so this"},{"timestamp":"2:29:18","text":"paper reinvigorated the public interest of using RL for llms and gave a lot of"},{"timestamp":"2:29:25","text":"the um sort of n-r details that are needed to reproduce their results and actually get the stage to work for large"},{"timestamp":"2:29:31","text":"langage models so let me take you briefly through this uh deep seek R1 paper and what happens when you actually"},{"timestamp":"2:29:36","text":"correctly apply RL to language models and what that looks like and what that gives you so the first thing I'll scroll to is this uh kind of figure two here"},{"timestamp":"2:29:43","text":"where we are looking at the Improvement in how the models are solving mathematical problems so this is the"},{"timestamp":"2:29:49","text":"accuracy of solving mathematical problems on the a accuracy and then we can go to the web page and we can see"},{"timestamp":"2:29:55","text":"the kinds of problems that are actually in these um these the kinds of math problems that are being measured here so"},{"timestamp":"2:30:00","text":"these are simple math problems you can um pause the video if you like but these are the kinds of problems that basically"},{"timestamp":"2:30:06","text":"the models are being asked to solve and you can see that in the beginning they're not doing very well but then as you update the model with this many"},{"timestamp":"2:30:12","text":"thousands of steps their accuracy kind of continues to climb so the models are improving and they're solving these"},{"timestamp":"2:30:18","text":"problems with a higher accuracy as you do this trial and error on a large data set of these kinds of"},{"timestamp":"2:30:24","text":"problems and the models are discovering how to solve math problems but even more"},{"timestamp":"2:30:29","text":"incredible than the quantitative kind of results of solving these problems with a higher accuracy is the qualitative means"},{"timestamp":"2:30:35","text":"by which the model achieves these results so when we scroll down uh one of the figures here that is kind of"},{"timestamp":"2:30:41","text":"interesting is that later on in the optimization the model seems to be uh"},{"timestamp":"2:30:46","text":"using average length per response uh goes up up so the model seems to be using more tokens to get its higher"},{"timestamp":"2:30:54","text":"accuracy results so it's learning to create very very long Solutions why are these Solutions very long we can look at"},{"timestamp":"2:31:00","text":"them qualitatively here so basically what they discover is that the model solution get very very long partially"},{"timestamp":"2:31:07","text":"because so here's a question and here's kind of the answer from the model what the model learns to do um and this is an"},{"timestamp":"2:31:13","text":"immerging property of new optimization it just discovers that this is good for problem solving is it starts to do stuff"},{"timestamp":"2:31:19","text":"like this wait wait wait that's Nota moment I can flag here let's reevaluate this step by step to identify the"},{"timestamp":"2:31:25","text":"correct sum can be so what is the model doing here right the model is basically"},{"timestamp":"2:31:30","text":"re-evaluating steps it has learned that it works better for accuracy to try out"},{"timestamp":"2:31:35","text":"lots of ideas try something from different perspectives retrace reframe backtrack is doing a lot of the things"},{"timestamp":"2:31:41","text":"that you and I are doing in the process of problem solving for mathematical questions but it's rediscovering what"},{"timestamp":"2:31:46","text":"happens in your head not what you put down on the solution and there is no human who can hardcode this stuff in the"},{"timestamp":"2:31:52","text":"ideal assistant response this is only something that can be discovered in the process of reinforcement learning"},{"timestamp":"2:31:57","text":"because you wouldn't know what to put here this just turns out to work for the model and it improves its accuracy in"},{"timestamp":"2:32:04","text":"problem solving so the model learns what we call these chains of thought in your head and it's an emergent property of"},{"timestamp":"2:32:10","text":"the optim of the optimization and that's what's bloating up the response length"},{"timestamp":"2:32:16","text":"but that's also what's increasing the accuracy of the problem problem solving so what's incredible here is basically"},{"timestamp":"2:32:22","text":"the model is discovering ways to think it's learning what I like to call cognitive strategies of how you"},{"timestamp":"2:32:28","text":"manipulate a problem and how you approach it from different perspectives how you pull in some analogies or do"},{"timestamp":"2:32:33","text":"different kinds of things like that and how you kind of uh try out many different things over time uh check a"},{"timestamp":"2:32:39","text":"result from different perspectives and how you kind of uh solve problems but here it's kind of discovered by the RL"},{"timestamp":"2:32:44","text":"so extremely incredible to see this emerge in the optimization without having to hardcode it anywhere the only"},{"timestamp":"2:32:50","text":"thing we've given it are the correct answers and this comes out from trying to just solve them correctly which is"},{"timestamp":"2:32:56","text":"incredible um now let's go back to actually the problem that we've been working with and"},{"timestamp":"2:33:02","text":"let's take a look at what it would look like uh for uh for this kind of a model"},{"timestamp":"2:33:07","text":"what we call reasoning or thinking model to solve that problem okay so recall that this is the problem we've been"},{"timestamp":"2:33:13","text":"working with and when I pasted it into chat GPT 40 I'm getting this kind of a response let's take a look at what"},{"timestamp":"2:33:19","text":"happens when you give this same query to what's called a reasoning or a thinking model this is a model that was trained"},{"timestamp":"2:33:25","text":"with reinforcement learning so this model described in this paper DC car1 is"},{"timestamp":"2:33:30","text":"available on chat. dec.com uh so this is kind of like the company uh that developed is hosting it you have to make"},{"timestamp":"2:33:37","text":"sure that the Deep think button is turned on to get the R1 model as it's called we can paste it here and run"},{"timestamp":"2:33:44","text":"it and so let's take a look at what happens now and what is the output of the model okay so here's it says so this"},{"timestamp":"2:33:51","text":"is previously what we get using basically what's an sft approach a supervised funing approach this is like"},{"timestamp":"2:33:56","text":"mimicking an expert solution this is what we get from the RL model okay let me try to figure this out so Emily buys"},{"timestamp":"2:34:03","text":"three apples and two oranges each orange cost $2 total is 13 I need to find out blah blah blah so here you you um as"},{"timestamp":"2:34:11","text":"you're reading this you can't escape thinking that this model is thinking um is definitely pursuing the"},{"timestamp":"2:34:19","text":"solution solution it deres that it must cost $3 and then it says wait a second let me check my math again to be sure"},{"timestamp":"2:34:25","text":"and then it tries it from a slightly different perspective and then it says yep all that checks out I think that's"},{"timestamp":"2:34:30","text":"the answer I don't see any mistakes let me see if there's another way to approach the problem maybe setting up an"},{"timestamp":"2:34:36","text":"equation let's let the cost of one apple be $8 then blah blah blah yep same"},{"timestamp":"2:34:42","text":"answer so definitely each apple is $3 all right confident that that's correct"},{"timestamp":"2:34:47","text":"and then what it does once it sort of um did the thinking process is it writes up the nice solution for the human and so"},{"timestamp":"2:34:54","text":"this is now considering so this is more about the correctness aspect and this is more about the presentation aspect where"},{"timestamp":"2:35:00","text":"it kind of like writes it out nicely and uh boxes in the correct answer at the bottom and so what's incredible about"},{"timestamp":"2:35:07","text":"this is we get this like thinking process of the model and this is what's coming from the reinforcement learning process this is what's bloating up the"},{"timestamp":"2:35:15","text":"length of the token sequences they're doing thinking and they're trying different ways this is what's giving you"},{"timestamp":"2:35:20","text":"higher accuracy in problem solving and this is where we are seeing these aha moments and these different"},{"timestamp":"2:35:26","text":"strategies and these um ideas for how you can make sure that you're getting the correct"},{"timestamp":"2:35:32","text":"answer the last point I wanted to make is some people are a little bit nervous about putting you know very sensitive"},{"timestamp":"2:35:38","text":"data into chat.com because this is a Chinese company so people don't um people are a little bit careful and Cy"},{"timestamp":"2:35:45","text":"with that a little bit um deep seek R1 is a model that was released by this company so this is an open source model"},{"timestamp":"2:35:52","text":"or open weights model it is available for anyone to download and use you will not be able to like run it in its full"},{"timestamp":"2:35:59","text":"um sort of the full model in full Precision you won't run that on a MacBook but uh or like a local device"},{"timestamp":"2:36:07","text":"because this is a fairly large model but many companies are hosting the full largest model one of those companies"},{"timestamp":"2:36:12","text":"that I like to use is called together. so when you go to together. you sign up and you go to playgrounds"},{"timestamp":"2:36:19","text":"you can can select here in the chat deep seek R1 and there's many different kinds of other models that you can select here"},{"timestamp":"2:36:25","text":"these are all state-of-the-art models so this is kind of similar to the hugging face inference playground that we've been playing with so far but together. a"},{"timestamp":"2:36:32","text":"will usually host all the state-of-the-art models so select DT car1 um you can try to ignore a lot of"},{"timestamp":"2:36:38","text":"these I think the default settings will often be okay and we can put in this and"},{"timestamp":"2:36:43","text":"because the model was released by Deep seek what you're getting here should be basically equivalent to what you're"},{"timestamp":"2:36:48","text":"getting here now because of the randomness in the sampling we're going to get something slightly different uh but in principle this should be uh"},{"timestamp":"2:36:55","text":"identical in terms of the power of the model and you should be able to see the same things quantitatively and qualitatively uh but uh this model is"},{"timestamp":"2:37:02","text":"coming from kind of a an American company so that's deep seek and that's"},{"timestamp":"2:37:07","text":"the what's called a reasoning model now when I go back to chat uh let"},{"timestamp":"2:37:12","text":"me go to chat here okay so the models that you're going to see in the drop down here some of them like 01 03 mini"},{"timestamp":"2:37:18","text":"O3 mini High Etc they are talking about uses Advanced reasoning now what this is"},{"timestamp":"2:37:23","text":"referring to uses Advanced reasoning is it's referring to the fact that it was trained by reinforcement learning with"},{"timestamp":"2:37:29","text":"techniques very similar to those of deep C car1 per public statements of opening ey employees uh so these are thinking"},{"timestamp":"2:37:37","text":"models trained with RL and these models like GPT 4 or GPT 4 40 mini that you're getting in the free tier you should"},{"timestamp":"2:37:43","text":"think of them as mostly sft models supervised fine tuning models they don't actually do this like thinking as as you"},{"timestamp":"2:37:49","text":"see in the RL models and even though there's a little bit of reinforcement learning involved with these models and"},{"timestamp":"2:37:55","text":"I'll go that into that in a second these are mostly sft models I think you should think about it that way so in the same"},{"timestamp":"2:38:00","text":"way as what we saw here we can pick one of the thinking models like say 03 mini high and these models by the way might"},{"timestamp":"2:38:07","text":"not be available to you unless you pay a Chachi PT subscription of either $20 per month or $200 per month for some of the"},{"timestamp":"2:38:14","text":"top models so we can pick a thinking model and run now what's going to happen"},{"timestamp":"2:38:20","text":"here is it's going to say reasoning and it's going to start to do stuff like this and um what we're seeing here is"},{"timestamp":"2:38:26","text":"not exactly the stuff we're seeing here so even though under the hood the model"},{"timestamp":"2:38:31","text":"produces these kinds of uh kind of chains of thought opening ey chooses to not show the exact chains of thought in"},{"timestamp":"2:38:38","text":"the web interface it shows little summaries of that of those chains of thought and open kind of does this I"},{"timestamp":"2:38:44","text":"think partly because uh they are worried about what's called the distillation risk that is that someone could come in"},{"timestamp":"2:38:50","text":"and actually try to imitate those reasoning traces and recover a lot of the reasoning performance by just imitating the reasoning uh chains of"},{"timestamp":"2:38:57","text":"thought and so they kind of hide them and they only show little summaries of them so you're not getting exactly what you would get in deep seek as with"},{"timestamp":"2:39:04","text":"respect to the reasoning itself and then they write up the solution so these are kind of like"},{"timestamp":"2:39:10","text":"equivalent even though we're not seeing the full under the hood details now in terms of the performance uh these models"},{"timestamp":"2:39:17","text":"and deep seek models are currently rly on par I would say it's kind of hard to tell because of the evaluations but if"},{"timestamp":"2:39:22","text":"you're paying $200 per month to open AI some of these models I believe are currently they basically still look better uh but deep seek R1 for now is"},{"timestamp":"2:39:30","text":"still a very solid choice for a thinking model that would be available to you um"},{"timestamp":"2:39:36","text":"sort of um either on this website or any other website because the model is open weights you can just download it so"},{"timestamp":"2:39:43","text":"that's thinking models so what is the summary so far well we've talked about reinforcement learning and the fact that"},{"timestamp":"2:39:50","text":"thinking emerges in the process of the optimization on when we basically run RL on many math uh and kind of code"},{"timestamp":"2:39:57","text":"problems that have verifiable Solutions so there's like an answer three Etc now these thinking models you can"},{"timestamp":"2:40:04","text":"access in for example deep seek or any inference provider like together. a and"},{"timestamp":"2:40:09","text":"choosing deep seek over there these thinking models are also available uh in chpt under any of the 01 or O3"},{"timestamp":"2:40:17","text":"models but these GPT 4 R models Etc they're not thinking models you should think of them as mostly sft models now"},{"timestamp":"2:40:25","text":"if you are um if you have a prompt that requires Advanced reasoning and so on you should probably use some of the"},{"timestamp":"2:40:30","text":"thinking models or at least try them out but empirically for a lot of my use when you're asking a simpler question there's"},{"timestamp":"2:40:36","text":"like a knowledge based question or something like that this might be Overkill like there's no need to think 30 seconds about some factual question"},{"timestamp":"2:40:42","text":"so for that I will uh sometimes default to just GPT 40 so empirically about 80 90% of my use is just gp4"},{"timestamp":"2:40:49","text":"and when I come across a very difficult problem like in math and code Etc I will reach for the thinking models but then I"},{"timestamp":"2:40:56","text":"have to wait a bit longer because they're thinking um so you can access these on chat on deep seek also I wanted"},{"timestamp":"2:41:02","text":"to point out that um AI studio. go.com even though it looks really busy"},{"timestamp":"2:41:08","text":"really ugly because Google's just unable to do this kind of stuff well it's like what is happening but if you choose"},{"timestamp":"2:41:15","text":"model and you choose here Gemini 2.0 flash thinking experimental 01 21 if you"},{"timestamp":"2:41:20","text":"choose that one that's also a a kind of early experiment experimental of a thinking model by Google so we can go"},{"timestamp":"2:41:27","text":"here and we can give it the same problem and click run and this is also a thinking problem a thinking model that"},{"timestamp":"2:41:33","text":"will also do something similar and comes out with the right answer here so basically Gemini also"},{"timestamp":"2:41:40","text":"offers a thinking model anthropic currently does not offer a thinking model but basically this is kind of like"},{"timestamp":"2:41:45","text":"the frontier development of these llms I think RL is kind of like this new exciting stage but getting the details"},{"timestamp":"2:41:51","text":"right is difficult and that's why all these models and thinking models are currently experimental as of 2025 very"},{"timestamp":"2:41:57","text":"early 2025 um but this is kind of like the frontier development of pushing the performance on these very difficult"},{"timestamp":"2:42:03","text":"problems using reasoning that is emerging in these optimizations one more connection that I wanted to bring up is"},{"timestamp":"2:42:10","text":"that the discovery that reinforcement learning is extremely powerful way of learning is not new to the field of AI"},{"timestamp":"2:42:17","text":"and one place what we've already seen this demonstrated is in the game of Go and famously Deep Mind developed the"},{"timestamp":"2:42:24","text":"system alphago and you can watch a movie about it um where the system is learning"},{"timestamp":"2:42:29","text":"to play the game of go against top human players and um when we go to the paper"},{"timestamp":"2:42:36","text":"underlying alphago so in this paper when we scroll down we actually find a really"},{"timestamp":"2:42:43","text":"interesting plot um that I think uh is kind of familiar uh to us and we're kind of like"},{"timestamp":"2:42:49","text":"we discovering in the more open domain of arbitrary problem solving instead of on the closed specific domain of the"},{"timestamp":"2:42:55","text":"game of Go but basically what they saw and we're going to see this in llms as well as this becomes more mature is this"},{"timestamp":"2:43:03","text":"is the ELO rating of playing game of Go and this is leas dull an extremely strong human player and here what they"},{"timestamp":"2:43:09","text":"are comparing is the strength of a model learned trained by supervised learning and a model trained by reinforcement"},{"timestamp":"2:43:15","text":"learning so the supervised learning model is imitating human expert players"},{"timestamp":"2:43:20","text":"so if you just get a huge amount of games played by expert players in the game of Go and you try to imitate them"},{"timestamp":"2:43:26","text":"you are going to get better but then you top out and you never quite get better"},{"timestamp":"2:43:31","text":"than some of the top top top players of in the game of Go like LEL so you're never going to reach there because"},{"timestamp":"2:43:37","text":"you're just imitating human players you can't fundamentally go beyond a human player if you're just imitating human players but in a process of"},{"timestamp":"2:43:44","text":"reinforcement learning is significantly more powerful in reinforcement learning for a game of Go it means that the"},{"timestamp":"2:43:50","text":"system is playing moves that empirically and statistically lead to win to winning"},{"timestamp":"2:43:56","text":"the game and so alphago is a system where it kind of plays against it itself"},{"timestamp":"2:44:02","text":"and it's using reinforcement learning to create rollouts so it's the exact same diagram"},{"timestamp":"2:44:07","text":"here but there's no prompt it's just uh because there's no prompt it's just a fixed game of Go but it's trying out"},{"timestamp":"2:44:13","text":"lots of solutions it's trying out lots of plays and then the games that lead to a win instead of a specific answer are"},{"timestamp":"2:44:20","text":"reinforced they're they're made stronger and so um the system is learning"},{"timestamp":"2:44:26","text":"basically the sequences of actions that empirically and statistically lead to winning the game and reinforcement"},{"timestamp":"2:44:32","text":"learning is not going to be constrained by human performance and reinforcement learning can do significantly better and"},{"timestamp":"2:44:38","text":"overcome even the top players like Lisa Dole and so uh probably they could have"},{"timestamp":"2:44:44","text":"run this longer and they just chose to crop it at some point because this costs money but this is very powerful demonstration of reinforcement learning"},{"timestamp":"2:44:51","text":"and we're only starting to kind of see hints of this diagram in larger language models for reasoning problems so we're"},{"timestamp":"2:44:58","text":"not going to get too far by just imitating experts we need to go beyond that set up these like little game"},{"timestamp":"2:45:03","text":"environments and get let let the system discover reasoning traces or like ways"},{"timestamp":"2:45:09","text":"of solving problems uh that are unique and that uh just basically work"},{"timestamp":"2:45:16","text":"well now on this aspect of uniqueness notice that when you're doing reinforcement learning nothing prevents"},{"timestamp":"2:45:21","text":"you from veering off the distribution of how humans are playing the game and so when we go back to uh this alphao search"},{"timestamp":"2:45:29","text":"here one of the suggested modifications is called move 37 and move 37 in alphao"},{"timestamp":"2:45:34","text":"is referring to a specific point in time where alphago basically played a move"},{"timestamp":"2:45:40","text":"that uh no human expert would play uh so the probability of this move uh to be"},{"timestamp":"2:45:45","text":"played by a human player was evaluated to be about 1 in 10th ,000 so it's a very rare move but in retrospect it was"},{"timestamp":"2:45:52","text":"a brilliant move so alphago in the process of reinforcement learning discovered kind of like a strategy of"},{"timestamp":"2:45:57","text":"playing that was unknown to humans and but is in retrospect uh brilliant I recommend this YouTube video um leis do"},{"timestamp":"2:46:04","text":"versus alphao move 37 reactions and Analysis and this is kind of what it looked like when alphao played this"},{"timestamp":"2:46:11","text":"move value that's a very that's a very"},{"timestamp":"2:46:16","text":"surprising move I thought I thought it was I thought it was a mistake when I see this move anyway so"},{"timestamp":"2:46:24","text":"basically people are kind of freaking out because it's a it's a move that a human would not play that alphago played"},{"timestamp":"2:46:31","text":"because in its training uh this move seemed to be a good idea it just happens not to be a kind of thing that a humans"},{"timestamp":"2:46:37","text":"would would do and so that is again the power of reinforcement learning and in principle we can actually see the"},{"timestamp":"2:46:42","text":"equivalence of that if we continue scaling this Paradigm in language models and what that looks like is kind of"},{"timestamp":"2:46:47","text":"unknown so so um what does it mean to solve problems in such a way that uh"},{"timestamp":"2:46:54","text":"even humans would not be able to get how can you be better at reasoning or thinking than humans how can you go"},{"timestamp":"2:47:00","text":"beyond just uh a thinking human like maybe it means discovering analogies"},{"timestamp":"2:47:05","text":"that humans would not be able to uh create or maybe it's like a new thinking strategy it's kind of hard to think through uh maybe it's a holy new"},{"timestamp":"2:47:14","text":"language that actually is not even English maybe it discovers its own language that is a lot better at"},{"timestamp":"2:47:19","text":"thinking um because the model is unconstrained to even like stick with English uh so maybe it takes a different"},{"timestamp":"2:47:27","text":"language to think in or it discovers its own language so in principle the behavior of the system is a lot less"},{"timestamp":"2:47:33","text":"defined it is open to do whatever works and it is open to also slowly Drift from"},{"timestamp":"2:47:40","text":"the distribution of its training data which is English but all of that can only be done if we have a very large"},{"timestamp":"2:47:45","text":"diverse set of problems in which the these strategy can be refined and perfected and so that is a lot of the"},{"timestamp":"2:47:51","text":"frontier LM research that's going on right now is trying to kind of create those kinds of prompt distributions that"},{"timestamp":"2:47:57","text":"are large and diverse these are all kind of like game environments in which the llms can practice their thinking and uh"},{"timestamp":"2:48:04","text":"it's kind of like writing you know these practice problems we have to create practice problems for all of domains of"},{"timestamp":"2:48:10","text":"knowledge and if we have practice problems and tons of them the models will be able to reinforcement learning"},{"timestamp":"2:48:16","text":"reinforcement learn on them and kind of uh create these kinds of uh diagrams but"},{"timestamp":"2:48:21","text":"in the domain of open thinking instead of a closed domain like game of Go there's one more section within"},{"timestamp":"2:48:27","text":"reinforcement learning that I wanted to cover and that is that of learning in unverifiable domains so so far all of"},{"timestamp":"2:48:35","text":"the problems that we've looked at are in what's called verifiable domains that is any candidate solution we can score very"},{"timestamp":"2:48:41","text":"easily against a concrete answer so for example answer is three and we can very easily score these Solutions against the"},{"timestamp":"2:48:47","text":"answer of three either we require the models to like box in their answers and then we just check"},{"timestamp":"2:48:53","text":"for equality of whatever is in the box with the answer or you can also use uh kind of what's called an llm judge so"},{"timestamp":"2:49:00","text":"the llm judge looks at a solution and it gets the answer and just basically scores the solution for whether it's"},{"timestamp":"2:49:06","text":"consistent with the answer or not and llms uh empirically are good enough at the current capability that they can do"},{"timestamp":"2:49:12","text":"this fairly reliably so we can apply those kinds of techniques as well in any case we have a concrete answer and we're"},{"timestamp":"2:49:17","text":"just checking Solutions again against it and we can do this automatically with no kind of humans in the loop the problem"},{"timestamp":"2:49:23","text":"is that we can't apply the strategy in what's called unverifiable domains so usually these are for example creative"},{"timestamp":"2:49:29","text":"writing tasks like write a joke about Pelicans or write a poem or summarize a paragraph or something like that in"},{"timestamp":"2:49:35","text":"these kinds of domains it becomes harder to score our different solutions to this problem so for example writing a joke"},{"timestamp":"2:49:41","text":"about Pelicans we can generate lots of different uh jokes of course that's fine for example we can go to chbt and we can"},{"timestamp":"2:49:47","text":"get it to uh generate a joke about Pelicans uh so much stuff in their beaks"},{"timestamp":"2:49:53","text":"because they don't bellan in backpacks what"},{"timestamp":"2:49:59","text":"okay we can uh we can try something else why don't Pelicans ever pay for their drinks because they always B it to"},{"timestamp":"2:50:06","text":"someone else haha okay so these models are not obviously not very good at humor"},{"timestamp":"2:50:12","text":"actually I think it's pretty fascinating because I think humor is secretly very difficult and the model have the capability I think anyway in any case"},{"timestamp":"2:50:20","text":"you could imagine creating lots of jokes the problem that we are facing is how do we score them now in principle we could"},{"timestamp":"2:50:27","text":"of course get a human to look at all these jokes just like I did right now the problem with that is if you are"},{"timestamp":"2:50:32","text":"doing reinforcement learning you're going to be doing many thousands of updates and for each update you want to"},{"timestamp":"2:50:38","text":"be looking at say thousands of prompts and for each prompt you want to be potentially looking at looking at hundred or thousands of different kinds"},{"timestamp":"2:50:44","text":"of generations and so there's just like way too many of these to look at and so"},{"timestamp":"2:50:50","text":"um in principle you could have a human inspect all of them and score them and decide that okay maybe this one is funny"},{"timestamp":"2:50:55","text":"and uh maybe this one is funny and this one is funny and we could train on them"},{"timestamp":"2:51:01","text":"to get the model to become slightly better at jokes um in the context of pelicans at least um the problem is that"},{"timestamp":"2:51:09","text":"it's just like way too much human time this is an unscalable strategy we need some kind of an automatic strategy for"},{"timestamp":"2:51:14","text":"doing this and one sort of solution to this was proposed in this paper uh that introduced what's called"},{"timestamp":"2:51:20","text":"reinforcement learning from Human feedback and so this was a paper from open at the time and many of these people are now um co-founders in"},{"timestamp":"2:51:27","text":"anthropic um and this kind of proposed a approach for uh basically doing"},{"timestamp":"2:51:33","text":"reinforcement learning in unverifiable domains so let's take a look at how that works so this is the cartoon diagram of"},{"timestamp":"2:51:39","text":"the core ideas involved so as I mentioned the native approach is if we just set Infinity human time we could"},{"timestamp":"2:51:46","text":"just run RL in these domains just fine so for example we can run RL as usual if"},{"timestamp":"2:51:51","text":"I have Infinity humans I would I just want to do and these are just cartoon numbers I want to do 1,000 updates where"},{"timestamp":"2:51:57","text":"each update will be on 1,000 prompts and in for each prompt we're going to have 1,000 roll outs that we're scoring so we"},{"timestamp":"2:52:05","text":"can run RL with this kind of a setup the problem is in the process of doing this I will need to run one I will need to"},{"timestamp":"2:52:12","text":"ask a human to evaluate a joke a total of 1 billion times and so that's a lot"},{"timestamp":"2:52:18","text":"of people looking at really terrible jokes so we don't want to do that so instead we want to take the arlef"},{"timestamp":"2:52:24","text":"approach so um in our Rel of approach we are kind of like the the core trick is"},{"timestamp":"2:52:29","text":"that of indirection so we're going to involve humans just a little bit and the"},{"timestamp":"2:52:35","text":"way we cheat is that we basically train a whole separate neural network that we call a reward model and this neural"},{"timestamp":"2:52:41","text":"network will kind of like imitate human scores so we're going to ask humans to score um roll"},{"timestamp":"2:52:49","text":"we're going to then imitate human scores using a neural network and this neural"},{"timestamp":"2:52:54","text":"network will become a kind of simulator of human preferences and now that we have a neural network simulator we can do RL"},{"timestamp":"2:53:01","text":"against it so instead of asking a real human we're asking a simulated human for"},{"timestamp":"2:53:06","text":"their score of a joke as an example and so once we have a simulator we're often"},{"timestamp":"2:53:11","text":"racist because we can query it as many times as we want to and it's all whole automatic process and we can now do"},{"timestamp":"2:53:17","text":"reinforcement learning with respect to the simulator and the simulator as you might expect is not going to be a perfect human but if it's at least"},{"timestamp":"2:53:24","text":"statistically similar to human judgment then you might expect that this will do something and in practice indeed uh it"},{"timestamp":"2:53:30","text":"does so once we have a simulator we can do RL and everything works great so let me show you a cartoon diagram a little"},{"timestamp":"2:53:36","text":"bit of what this process looks like although the details are not 100 like super important it's just a core idea of"},{"timestamp":"2:53:42","text":"how this works so here I have a cartoon diagram of a hypothetical example of what training the reward model would"},{"timestamp":"2:53:47","text":"look like so we have a prompt like write a joke about picans and then here we have five separate roll outs so these"},{"timestamp":"2:53:54","text":"are all five different jokes just like this one now the first thing we're going to do is we are going to ask a human to"},{"timestamp":"2:54:02","text":"uh order these jokes from the best to worst so this is uh so here this human"},{"timestamp":"2:54:08","text":"thought that this joke is the best the funniest so number one joke this is"},{"timestamp":"2:54:14","text":"number two joke number three joke four and five so this is the worst joke"},{"timestamp":"2:54:19","text":"we're asking humans to order instead of give scores directly because it's a bit of an easier task it's easier for a"},{"timestamp":"2:54:24","text":"human to give an ordering than to give precise scores now that is now the supervision for the model so the human"},{"timestamp":"2:54:31","text":"has ordered them and that is kind of like their contribution to the training process but now separately what we're going to do is we're going to ask a"},{"timestamp":"2:54:37","text":"reward model uh about its scoring of these jokes now the reward model is a"},{"timestamp":"2:54:42","text":"whole separate neural network completely separate neural net um and it's also probably a transform"},{"timestamp":"2:54:49","text":"uh but it's not a language model in the sense that it generates diverse language Etc it's just a scoring model so the"},{"timestamp":"2:54:56","text":"reward model will take as an input The Prompt number one and number two a"},{"timestamp":"2:55:02","text":"candidate joke so um those are the two inputs that go into the reward model so here for example the reward model would"},{"timestamp":"2:55:08","text":"be taken this prompt and this joke now the output of a reward model is a single"},{"timestamp":"2:55:14","text":"number and this number is thought of as a score and it can range for example from Z to one so zero would be the worst"},{"timestamp":"2:55:20","text":"score and one would be the best score so here are some examples of what a hypothetical reward model at some stage"},{"timestamp":"2:55:27","text":"in the training process would give uh s scoring to these jokes so 0.1 is a very"},{"timestamp":"2:55:33","text":"low score 08 is a really high score and so on and so now um we compare the"},{"timestamp":"2:55:40","text":"scores given by the reward model with uh the ordering given by the human and there's a precise mathematical way to"},{"timestamp":"2:55:47","text":"actually calculate this uh basically set up a loss function and calculate a kind of like a correspondence here and uh"},{"timestamp":"2:55:54","text":"update a model based on it but I just want to give you the intuition which is that as an example here for this second"},{"timestamp":"2:56:00","text":"joke the the human thought that it was the funniest and the model kind of agreed right 08 is a relatively high score but this score should have been"},{"timestamp":"2:56:07","text":"even higher right so after an update we would expect that maybe this score should have been will actually grow"},{"timestamp":"2:56:13","text":"after an update of the network to be like say 081 or something um for this one here they"},{"timestamp":"2:56:19","text":"actually are in a massive disagreement because the human thought that this was number two but here the the score is"},{"timestamp":"2:56:24","text":"only 0.1 and so this score needs to be much higher so after an update on top of"},{"timestamp":"2:56:30","text":"this um kind of a supervision this might grow a lot more like maybe it's 0.15 or something like"},{"timestamp":"2:56:36","text":"that um and then here the human thought that this one was the worst joke but"},{"timestamp":"2:56:41","text":"here the model actually gave it a fairly High number so you might expect that after the update uh this would come down"},{"timestamp":"2:56:47","text":"to maybe 3 3.5 or something like that so basically we're doing what we did before we're slightly nudging the predictions"},{"timestamp":"2:56:54","text":"from the models using a neural network training process and we're trying to make the"},{"timestamp":"2:57:00","text":"reward model scores be consistent with human ordering and so um as we update the"},{"timestamp":"2:57:07","text":"reward model on human data it becomes better and better simulator of the scores and orders uh that humans provide"},{"timestamp":"2:57:14","text":"and then becomes kind of like the the neural the simulator of human preferences which we can then do RL"},{"timestamp":"2:57:20","text":"against but critically we're not asking humans one billion times to look at a joke we're maybe looking at th000"},{"timestamp":"2:57:26","text":"prompts and five roll outs each so maybe 5,000 jokes that humans have to look at in total and they just give the ordering"},{"timestamp":"2:57:33","text":"and then we're training the model to be consistent with that ordering and I'm skipping over the mathematical details"},{"timestamp":"2:57:38","text":"but I just want you to understand a high level idea that uh this reward model is do is basically giving us this scour and"},{"timestamp":"2:57:45","text":"we have a way of training it to be consistent with human orderings and that's how rhf works okay so that is"},{"timestamp":"2:57:51","text":"the rough idea we basically train simulators of humans and RL with respect to those"},{"timestamp":"2:57:56","text":"simulators now I want to talk about first the upside of reinforcement learning from Human"},{"timestamp":"2:58:03","text":"feedback the first thing is that this allows us to run reinforcement learning which we know is incredibly powerful"},{"timestamp":"2:58:09","text":"kind of set of techniques and it allows us to do it in arbitrary domains and including the ones that are unverifiable"},{"timestamp":"2:58:15","text":"so things like summarization and poem writing joke writing or any other creative writing really uh in domains"},{"timestamp":"2:58:21","text":"outside of math and code Etc now empirically what we see when we actually apply rhf is that this is a way"},{"timestamp":"2:58:28","text":"to improve the performance of the model and uh I have a top answer for why that"},{"timestamp":"2:58:33","text":"might be but I don't actually know that it is like super well established on like why this is you can empirically"},{"timestamp":"2:58:39","text":"observe that when you do rhf correctly the models you get are just like a little bit better um but as to why is I"},{"timestamp":"2:58:45","text":"think like not as clear so here's my best guess my best guess is that this is possibly mostly due to the discriminator"},{"timestamp":"2:58:52","text":"generator Gap what that means is that in many cases it is significantly easier to"},{"timestamp":"2:58:58","text":"discriminate than to generate for humans so in particular an example of this is"},{"timestamp":"2:59:04","text":"um in when we do supervised fine-tuning right sft we're asking humans to generate the"},{"timestamp":"2:59:12","text":"ideal assistant response and in many cases here um as I've shown it uh the"},{"timestamp":"2:59:18","text":"ideal response is very simple to write but in many cases might not be so for example in summarization or poem writing"},{"timestamp":"2:59:24","text":"or joke writing like how are you as a human assist as a human labeler um supposed to give the ideal response in"},{"timestamp":"2:59:30","text":"these cases it requires creative human writing to do that and so rhf kind of"},{"timestamp":"2:59:35","text":"sidesteps this because we get um we get to ask people a significantly easier question as a data labelers they're not"},{"timestamp":"2:59:42","text":"asked to write poems directly they're just given five poems from the model and they're just asked to order them and so"},{"timestamp":"2:59:49","text":"that's just a much easier task for a human labeler to do and so what I think this allows you to do basically is it um"},{"timestamp":"2:59:57","text":"it kind of like allows a lot more higher accuracy data because we're not asking"},{"timestamp":"3:00:02","text":"people to do the generation task which can be extremely difficult like we're not asking them to do creative writing"},{"timestamp":"3:00:07","text":"we're just trying to get them to distinguish between creative writings and uh find the ones that are best and"},{"timestamp":"3:00:14","text":"that is the signal that humans are providing just the ordering and that is their input into the system and then the"},{"timestamp":"3:00:20","text":"system in rhf just discovers the kinds of responses that would be graded well"},{"timestamp":"3:00:26","text":"by humans and so that step of indirection allows the models to become a bit better so that is the upside of"},{"timestamp":"3:00:33","text":"our LF it allows us to run RL it empirically results in better models and it allows uh people to contribute their"},{"timestamp":"3:00:40","text":"supervision uh even without having to do extremely difficult tasks um in the case of writing ideal responses unfortunately"},{"timestamp":"3:00:47","text":"our HF also comes with significant downsides and so um the main one is that"},{"timestamp":"3:00:54","text":"basically we are doing reinforcement learning not with respect to humans and actual human judgment but with respect to a lossy simulation of humans right"},{"timestamp":"3:01:01","text":"and this lossy simulation could be misleading because it's just a it's just a simulation right it's just a language"},{"timestamp":"3:01:07","text":"model that's kind of outputting scores and it might not perfectly reflect the opinion of an actual human with an"},{"timestamp":"3:01:13","text":"actual brain in all the possible different cases so that's number one which is actually something even more"},{"timestamp":"3:01:18","text":"subtle and devious going on that uh really dramatically holds back our LF as a"},{"timestamp":"3:01:24","text":"technique that we can really scale to significantly um kind of Smart Systems"},{"timestamp":"3:01:31","text":"and that is that reinforcement learning is extremely good at discovering a way to game the model to game the simulation"},{"timestamp":"3:01:38","text":"so this reward model that we're constructing here that gives the course these models are Transformers these"},{"timestamp":"3:01:46","text":"Transformers are massive neurals they have billions of parameters and they imitate humans but they do so in a kind"},{"timestamp":"3:01:52","text":"of like a simulation way now the problem is that these are massive complicated systems right there's a billion"},{"timestamp":"3:01:57","text":"parameters here that are outputting a single score it turns out that there are ways"},{"timestamp":"3:02:02","text":"to gain these models you can find kinds of inputs that were not part of their"},{"timestamp":"3:02:08","text":"training set and these inputs inexplicably get very high scores but in"},{"timestamp":"3:02:13","text":"a fake way so very often what you find if you run our lch for very long so for"},{"timestamp":"3:02:19","text":"example if we do 1,000 updates which is like say a lot of updates you might expect that your jokes are getting"},{"timestamp":"3:02:25","text":"better and that you're getting like real bangers about Pelicans but that's not EXA exactly what happens what happens is"},{"timestamp":"3:02:31","text":"that uh in the first few hundred steps the jokes about Pelicans are probably improving a little bit and then they"},{"timestamp":"3:02:37","text":"actually dramatically fall off the cliff and you start to get extremely nonsensical results like for example you"},{"timestamp":"3:02:42","text":"start to get um the top joke about Pelicans starts to be the"},{"timestamp":"3:02:48","text":"and this makes no sense right like when you look at it why should this be a top joke but when you take the the and you"},{"timestamp":"3:02:53","text":"plug it into your reward model you'd expect score of zero but actually the reward model loves this as a joke it"},{"timestamp":"3:02:59","text":"will tell you that the the the theth is a score of 1. Z this is a top joke and"},{"timestamp":"3:03:06","text":"this makes no sense right but it's because these models are just simulations of humans and they're massive neural lots and you can find"},{"timestamp":"3:03:12","text":"inputs at the bottom that kind of like get into the part of the input space that kind of gives you nonsensical"},{"timestamp":"3:03:17","text":"results these examples are what's called adversarial examples and I'm not going to go into the topic too much but these"},{"timestamp":"3:03:24","text":"are adversarial inputs to the model they are specific little inputs that kind of"},{"timestamp":"3:03:29","text":"go between the nooks and crannies of the model and give nonsensical results at the top now here's what you might"},{"timestamp":"3:03:34","text":"imagine doing you say okay the the the is obviously not score of one um it's obviously a low score so let's take the"},{"timestamp":"3:03:41","text":"the the the the let's add it to the data set and give it an ordering that is extremely bad like a score of five and"},{"timestamp":"3:03:47","text":"indeed your model will learn that the D should have a very low score and it will give it score of zero the problem is"},{"timestamp":"3:03:53","text":"that there will always be basically infinite number of nonsensical adversarial examples hiding in the model"},{"timestamp":"3:04:00","text":"if you iterate this process many times and you keep adding nonsensical stuff to your reward model and giving it very low"},{"timestamp":"3:04:05","text":"scores you can you'll never win the game uh you can do this many many rounds and"},{"timestamp":"3:04:11","text":"reinforcement learning if you run it long enough will always find a way to gain the model it will discover adversarial examples it will get get"},{"timestamp":"3:04:17","text":"really high scores uh with nonsensical results and fundamentally this is"},{"timestamp":"3:04:23","text":"because our scoring function is a giant neural nut and RL is extremely good at"},{"timestamp":"3:04:28","text":"finding just the ways to trick it uh so long story short you always run rhf put"},{"timestamp":"3:04:36","text":"for maybe a few hundred updates the model is getting better and then you have to crop it and you are done you"},{"timestamp":"3:04:42","text":"can't run too much against this reward model because the optimization will"},{"timestamp":"3:04:47","text":"start to game it and you basically crop it and you call it and you ship it um"},{"timestamp":"3:04:53","text":"and uh you can improve the reward model but you kind of like come across these situations eventually at some point so"},{"timestamp":"3:05:00","text":"rhf basically what I usually say is that RF is not RL and what I mean by that is"},{"timestamp":"3:05:06","text":"I mean RF is RL obviously but it's not RL in the magical sense this is not RL"},{"timestamp":"3:05:12","text":"that you can run indefinitely these kinds of problems like where you are getting con correct"},{"timestamp":"3:05:18","text":"answer you cannot gain this as easily you either got the correct answer or you didn't and the scoring function is much"},{"timestamp":"3:05:23","text":"much simpler you're just looking at the boxed area and seeing if the result is correct so it's very difficult to gain"},{"timestamp":"3:05:29","text":"these functions but uh gaming a reward model is possible now in these verifiable domains you can run RL"},{"timestamp":"3:05:36","text":"indefinitely you could run for tens of thousands hundreds of thousands of steps and discover all kinds of really crazy"},{"timestamp":"3:05:41","text":"strategies that we might not even ever think about of Performing really well for all these problems in the game of Go"},{"timestamp":"3:05:48","text":"there's no way to to beat to basically game uh the winning of a game or the losing of a game we have a perfect"},{"timestamp":"3:05:54","text":"simulator we know all the different uh where all the stones are placed and we can calculate uh whether someone has won"},{"timestamp":"3:06:01","text":"or not there's no way to gain that and so you can do RL indefinitely and you can eventually be beat even leol but"},{"timestamp":"3:06:08","text":"with models like this which are gameable you cannot repeat this process"},{"timestamp":"3:06:13","text":"indefinitely so I kind of see rhf as not real RL because the reward function is"},{"timestamp":"3:06:19","text":"gameable so it's kind of more like in the realm of like little fine-tuning it's a little it's a little Improvement"},{"timestamp":"3:06:26","text":"but it's not something that is fundamentally set up correctly where you can insert more compute run for longer"},{"timestamp":"3:06:32","text":"and get much better and magical results so it's it's uh it's not RL in that sense it's not RL in the sense that it"},{"timestamp":"3:06:38","text":"lacks magic um it can find you in your model and get a better performance and indeed if we go back to chat GPT the GPT"},{"timestamp":"3:06:46","text":"40 model has gone through rhf because it works well but it's just not RL in the"},{"timestamp":"3:06:52","text":"same sense rlf is like a little fine tune that slightly improves your model is maybe like the way I would think about it okay so that's most of the"},{"timestamp":"3:06:59","text":"technical content that I wanted to cover I took you through the three major stages and paradigms of training these"},{"timestamp":"3:07:05","text":"models pre-training supervised fine tuning and reinforcement learning and I showed you that they Loosely correspond"},{"timestamp":"3:07:11","text":"to the process we already use for teaching children and so in particular we talked about pre-training being sort"},{"timestamp":"3:07:17","text":"of like the basic knowledge acquisition of reading Exposition supervised fine tuning being the process of looking at"},{"timestamp":"3:07:22","text":"lots and lots of worked examples and imitating experts and practice problems"},{"timestamp":"3:07:28","text":"the only difference is that we now have to effectively write textbooks for llms and AIS across all the disciplines of"},{"timestamp":"3:07:35","text":"human knowledge and also in all the cases where we actually would like them to work like code and math and you know"},{"timestamp":"3:07:42","text":"basically all the other disciplines so we're in the process of writing textbooks for them refining all the algorithms that I've presented on the"},{"timestamp":"3:07:48","text":"high level and then of course doing a really really good job at the execution of training these models at scale and"},{"timestamp":"3:07:54","text":"efficiently so in particular I didn't go into too many details but these are extremely large and complicated"},{"timestamp":"3:08:00","text":"distributed uh sort of um jobs that have to run over tens of"},{"timestamp":"3:08:07","text":"thousands or even hundreds of thousands of gpus and the engineering that goes into this is really at the stateof the"},{"timestamp":"3:08:12","text":"art of what's possible with computers at that scale so I didn't cover that aspect too much"},{"timestamp":"3:08:19","text":"but um this is very kind of serious and they were underlying all these very"},{"timestamp":"3:08:24","text":"simple algorithms ultimately now I also talked about sort of like the theory of mind a little bit"},{"timestamp":"3:08:30","text":"of these models and the thing I want you to take away is that these models are really good but they're extremely useful"},{"timestamp":"3:08:35","text":"as tools for your work you shouldn't uh sort of trust them fully and I showed you some examples of that even though we"},{"timestamp":"3:08:41","text":"have mitigations for hallucinations the models are not perfect and they will hallucinate still it's gotten better"},{"timestamp":"3:08:46","text":"over time and it will continue to get better but they can hallucinate in other words in in"},{"timestamp":"3:08:52","text":"addition to that I covered kind of like what I call the Swiss cheese uh sort of model of llm capabilities that you"},{"timestamp":"3:08:57","text":"should have in your mind the models are incredibly good across so many different disciplines but then fail randomly almost in some unique cases so for"},{"timestamp":"3:09:05","text":"example what is bigger 9.11 or 9.9 like the model doesn't know but simultaneously it can turn around and"},{"timestamp":"3:09:11","text":"solve Olympiad questions and so this is a hole in the Swiss cheese and there are many of them and you don't want to trip"},{"timestamp":"3:09:17","text":"over them so don't um treat these models as infallible models check their work"},{"timestamp":"3:09:23","text":"use them as tools use them for inspiration use them for the first draft but uh work with them as tools and be"},{"timestamp":"3:09:30","text":"ultimately respons responsible for the you know product of your work and that's roughly what I wanted to"},{"timestamp":"3:09:38","text":"talk about this is how they're trained and this is what they are let's now turn to what are some of the future"},{"timestamp":"3:09:44","text":"capabilities of these models uh probably what's coming down the pipe and also where can you find these models I have a"},{"timestamp":"3:09:50","text":"few blow points on some of the things that you can expect coming down the pipe the first thing you'll notice is that the models will very rapidly become"},{"timestamp":"3:09:56","text":"multimodal everything I talked about above concerned text but very soon we'll have llms that can not just handle text"},{"timestamp":"3:10:03","text":"but they can also operate natively and very easily over audio so they can hear and speak and also images so they can"},{"timestamp":"3:10:10","text":"see and paint and we're already seeing the beginnings of all of this uh but this will be all done natively inside"},{"timestamp":"3:10:17","text":"inside the language model and this will enable kind of like natural conversations and roughly speaking the"},{"timestamp":"3:10:22","text":"reason that this is actually no different from everything we've covered above is that as a baseline you can"},{"timestamp":"3:10:28","text":"tokenize audio and images and apply the exact same approaches of everything that we've talked about above so it's not a"},{"timestamp":"3:10:34","text":"fundamental change it's just uh it's just a to we have to add some tokens so as an example for tokenizing audio we"},{"timestamp":"3:10:41","text":"can look at slices of the spectrogram of the audio signal and we can tokenize that and just add more tokens that"},{"timestamp":"3:10:47","text":"suddenly represent audio and just add them into the context windows and train on them just like above the same for"},{"timestamp":"3:10:53","text":"images we can use patches and we can separately tokenize patches and then"},{"timestamp":"3:10:58","text":"what is an image an image is just a sequence of tokens and this actually kind of works and there's a lot of early"},{"timestamp":"3:11:04","text":"work in this direction and so we can just create streams of tokens that are representing audio images as well as"},{"timestamp":"3:11:10","text":"text and interpers them and handle them all simultaneously in a single model so that's one example of multimodality"},{"timestamp":"3:11:17","text":"uh second something that people are very interested in is currently most of the work is that we're handing individual tasks to the"},{"timestamp":"3:11:24","text":"models on kind of like a silver platter like please solve this task for me and the model sort of like does this little"},{"timestamp":"3:11:29","text":"task but it's up to us to still sort of like organize a coherent execution of"},{"timestamp":"3:11:35","text":"tasks to perform jobs and the models are not yet at the capability required to do"},{"timestamp":"3:11:41","text":"this in a coherent error correcting way over long periods of time so they're not"},{"timestamp":"3:11:46","text":"able to fully string together tasks to perform these longer running jobs but they're getting there and this is"},{"timestamp":"3:11:52","text":"improving uh over time but uh probably what's going to happen here is we're going to start to see what's called"},{"timestamp":"3:11:57","text":"agents which perform tasks over time and you you supervise them and you watch"},{"timestamp":"3:12:02","text":"their work and they come up to once in a while report progress and so on so we're going to see more long running agents uh"},{"timestamp":"3:12:09","text":"tasks that don't just take you know a few seconds of response but many tens of seconds or even minutes or hours over"},{"timestamp":"3:12:15","text":"time uh but these uh models are not infallible as we talked about above so all of this will require supervision so"},{"timestamp":"3:12:21","text":"for example in factories people talk about the human to robot ratio uh for automation I think we're going to see"},{"timestamp":"3:12:27","text":"something similar in the digital space where we are going to be talking about human to agent ratios where humans"},{"timestamp":"3:12:33","text":"becomes a lot more supervisors of agent tasks um in the digital"},{"timestamp":"3:12:38","text":"domain uh next um I think everything is going to become a lot more pervasive and invisible so it's kind of like"},{"timestamp":"3:12:44","text":"integrated into the tools and everywhere um and in addition kind of like computer"},{"timestamp":"3:12:51","text":"using so right now these models aren't able to take actions on your behalf but I think this is a separate bullet point"},{"timestamp":"3:12:58","text":"um if you saw chpt launch the operator then uh that's one early example of that"},{"timestamp":"3:13:04","text":"where you can actually hand off control to the model to perform you know keyboard and mouse actions on your"},{"timestamp":"3:13:09","text":"behalf so that's also something that that I think is very interesting the last point I have here is just a general"},{"timestamp":"3:13:14","text":"comment that there's still a lot of research to potentially do in this domain main one example of that uh is"},{"timestamp":"3:13:19","text":"something along the lines of test time training so remember that everything we've done above and that we talked about has two major stages there's first"},{"timestamp":"3:13:27","text":"the training stage where we tune the parameters of the model to perform the tasks well once we get the parameters we"},{"timestamp":"3:13:33","text":"fix them and then we deploy the model for inference from there the model is fixed it doesn't change anymore it"},{"timestamp":"3:13:39","text":"doesn't learn from all the stuff that it's doing a test time it's a fixed um number of parameters and the only thing"},{"timestamp":"3:13:45","text":"that is changing is now the token inside the context windows and so the only type of learning or test time learning that"},{"timestamp":"3:13:51","text":"the model has access to is the in context learning of its uh kind of like uh dynamically adjustable context window"},{"timestamp":"3:13:59","text":"depending on like what it's doing at test time so but I think this is still different from humans who actually are"},{"timestamp":"3:14:04","text":"able to like actually learn uh depending on what they're doing especially when you sleep for example like your brain is updating your parameters or something"},{"timestamp":"3:14:10","text":"like that right so there's no kind of equivalent of that currently in these models and tools so there's a lot of"},{"timestamp":"3:14:16","text":"like um more wonky ideas I think that are to be explored still and uh in particular I think this will be"},{"timestamp":"3:14:21","text":"necessary because the context window is a finite and precious resource and especially once we start to tackle very"},{"timestamp":"3:14:27","text":"long running multimodal tasks and we're putting in videos and these token windows will basically start to grow"},{"timestamp":"3:14:34","text":"extremely large like not thousands or even hundreds of thousands but significantly beyond that and the only"},{"timestamp":"3:14:39","text":"trick uh the only kind of trick we have Avail to us right now is to make the context Windows longer but I think that"},{"timestamp":"3:14:46","text":"that approach by itself will will not will not scale to actual long running tasks that are multimodal over time and"},{"timestamp":"3:14:51","text":"so I think new ideas are needed in some of those disciplines um in some of those kind of cases in the main where these"},{"timestamp":"3:14:58","text":"tasks are going to require very long contexts so those are some examples of some of the things you can um expect"},{"timestamp":"3:15:05","text":"coming down the pipe let's now turn to where you can actually uh kind of keep track of this progress and um you know"},{"timestamp":"3:15:12","text":"be up to date with the latest and grest of what's happening in the field so I would say the three resources that I have consistently used to stay up to"},{"timestamp":"3:15:18","text":"date are number one El Marina uh so let me show you El"},{"timestamp":"3:15:23","text":"Marina this is basically an llm leader board and it ranks all the top models"},{"timestamp":"3:15:30","text":"and the ranking is based on human comparisons so humans prompt these models and they get to judge which one"},{"timestamp":"3:15:35","text":"gives a better answer they don't know which model is which they're just looking at which model is the better answer and you can calculate a ranking"},{"timestamp":"3:15:42","text":"and then you get some results and so what you can hear is what you can see here is the different organizations like"},{"timestamp":"3:15:48","text":"Google Gemini for example that produce these models when you click on any one of these it takes you to the place where"},{"timestamp":"3:15:53","text":"that model is hosted and then here we see Google is currently on top with open AI right"},{"timestamp":"3:15:59","text":"behind here we see deep seek in position number three now the reason this is a big deal is the last column here you see"},{"timestamp":"3:16:05","text":"license deep seek is an MIT license model it's open weights anyone can use these weights uh anyone can download"},{"timestamp":"3:16:12","text":"them anyone can host their own version of Deep seek and they can use it in what whatever way they like and so it's not a"},{"timestamp":"3:16:18","text":"proprietary model that you don't have access to it's it's basically an open weight release and so this is kind of"},{"timestamp":"3:16:24","text":"unprecedented that a model this strong was released with open weights so pretty"},{"timestamp":"3:16:29","text":"cool from the team next up we have a few more models from Google and open Ai and then when you continue to scroll down"},{"timestamp":"3:16:35","text":"you start to see some other Usual Suspects so xai here anthropic with son"},{"timestamp":"3:16:40","text":"it uh here at number 14 and um then"},{"timestamp":"3:16:47","text":"meta with llama over here so llama similar to deep seek is an open weights model and so uh but it's down here as"},{"timestamp":"3:16:55","text":"opposed to up here now I will say that this leaderboard was really good for a long time I do think that in the last"},{"timestamp":"3:17:03","text":"few months it's become a little bit gamed um and I don't trust it as much as I used to I think um just empirically I"},{"timestamp":"3:17:11","text":"feel like a lot of people for example are using a Sonet from anthropic and that it's a really good model so but"},{"timestamp":"3:17:17","text":"that's all the way down here um in number 14 and conversely I think not as"},{"timestamp":"3:17:22","text":"many people are using Gemini but it's racking really really high uh so I think use this as a first pass uh but uh sort"},{"timestamp":"3:17:30","text":"of try out a few of the models for your tasks and see which one performs better"},{"timestamp":"3:17:35","text":"the second thing that I would point to is the uh AI news uh newsletter so AI"},{"timestamp":"3:17:41","text":"news is not very creatively named but it is a very good newsletter produced by swix and friends so thank you for"},{"timestamp":"3:17:46","text":"maintaining it and it's been very helpful to me because it is extremely comprehensive so if you go to archives uh you see that it's"},{"timestamp":"3:17:52","text":"produced almost every other day and um it is very comprehensive and some of it"},{"timestamp":"3:17:58","text":"is written by humans and curated by humans but a lot of it is constructed automatically with llms so you'll see"},{"timestamp":"3:18:03","text":"that these are very comprehensive and you're probably not missing anything major if you go through it of course"},{"timestamp":"3:18:08","text":"you're probably not going to go through it because it's so long but I do think that these summaries all the way up top"},{"timestamp":"3:18:14","text":"are quite good and I think have some human oversight uh so this has been very helpful to me and the last thing I would"},{"timestamp":"3:18:20","text":"point to is just X and Twitter uh a lot of um AI happens on X and so I would"},{"timestamp":"3:18:25","text":"just follow people who you like and trust and get all your latest and greatest uh on X as well so those are"},{"timestamp":"3:18:32","text":"the major places that have worked for me over time and finally a few words on where you can find the models and where"},{"timestamp":"3:18:37","text":"can you use them so the first one I would say is for any of the biggest proprietary models you just have to go"},{"timestamp":"3:18:42","text":"to the website of that LM provider so for example for open a that's uh chat I believe actually works now uh so"},{"timestamp":"3:18:49","text":"that's for open AI now for or you know for um for Gemini"},{"timestamp":"3:18:54","text":"I think it's gem. google.com or AI Studio I think they have two for some reason that I don't fly understand no"},{"timestamp":"3:19:01","text":"one does um for the open weights models like deep SE CL Etc you have to go to"},{"timestamp":"3:19:06","text":"some kind of an inference provider of LMS so my favorite one is together together. a and I showed you that when"},{"timestamp":"3:19:11","text":"you go to the playground of together. a then you can sort of pick lots of different models and all of these are"},{"timestamp":"3:19:17","text":"open models of different types and you can talk to them here as an example um now if you'd like to use a"},{"timestamp":"3:19:24","text":"base model like um you know a base model then this is where I think it's not as common to find base models even on these"},{"timestamp":"3:19:31","text":"inference providers they are all targeting assistants and chat and so I think even here I can't I couldn't see"},{"timestamp":"3:19:37","text":"base models here so for base models I usually go to hyperbolic because they serve my llama 3.1 base and I love that"},{"timestamp":"3:19:45","text":"model and you can just talk to it here so as far as I know this is this is a good place for a base model and I wish"},{"timestamp":"3:19:51","text":"more people hosted base models because they are useful and interesting to work with in some cases finally you can also"},{"timestamp":"3:19:57","text":"take some of the models that are smaller and you can run them locally and so for example deep seek the biggest model"},{"timestamp":"3:20:04","text":"you're not going to be able to run locally on your MacBook but there are smaller versions of the deep seek model that are what's called distilled and"},{"timestamp":"3:20:11","text":"then also you can run these models at smaller Precision so not at the native Precision of for example fp8 on deep"},{"timestamp":"3:20:17","text":"seek or you know bf16 llama but much much lower than that um and don't worry"},{"timestamp":"3:20:23","text":"if you don't fully understand those details but you can run smaller versions that have been distilled and then at even lower precision and then you can"},{"timestamp":"3:20:29","text":"fit them on your uh computer and so you can actually run pretty okay models on"},{"timestamp":"3:20:35","text":"your laptop and my favorite I think place I go to usually is LM studio uh which is basically an app you can get"},{"timestamp":"3:20:42","text":"and I think it kind of actually looks really ugly and it's I don't like that it shows you all these models that are basically not that useful like everyone"},{"timestamp":"3:20:48","text":"just wants to run deep seek so I don't know why they give you these 500 different types of models they're really complicated to search for and you have"},{"timestamp":"3:20:54","text":"to choose different distillations and different uh precisions and it's all really confusing but once you actually"},{"timestamp":"3:21:00","text":"understand how it works and that's a whole separate video then you can actually load up a model like here I loaded up a llama 3 uh2 instruct 1"},{"timestamp":"3:21:08","text":"billion and um you can just talk to it so I ask for Pelican jokes and I can ask"},{"timestamp":"3:21:14","text":"for another one and it gives me another one Etc all of this that happens here is locally on your computer so we're not"},{"timestamp":"3:21:20","text":"actually going to anywhere anyone else this is running on the GPU on the MacBook Pro so that's very nice and you"},{"timestamp":"3:21:26","text":"can then eject the model when you're done and that frees up the ram so LM studio is probably like my favorite one"},{"timestamp":"3:21:33","text":"even though I don't I think it's got a lot of uiux issues and it's really geared towards uh professionals almost"},{"timestamp":"3:21:39","text":"uh but if you watch some videos on YouTube I think you can figure out how to how to use this interface uh so those are a few words on"},{"timestamp":"3:21:45","text":"where to find them so let me now loop back around to where we started the question was when we go to chashi pta.com and we enter some kind of a"},{"timestamp":"3:21:53","text":"query and we hit go what exactly is happening here what are we seeing what"},{"timestamp":"3:21:59","text":"are we talking to how does this work and I hope that this video gave you some appreciation for some of the under the"},{"timestamp":"3:22:06","text":"hood details of how these models are trained and what this is that is coming back so in particular we now know that"},{"timestamp":"3:22:12","text":"your query is taken and is first chopped up into tokens so we go to to tick"},{"timestamp":"3:22:18","text":"tokenizer and here where is the place in the in the um sort of format that is for"},{"timestamp":"3:22:24","text":"the user query we basically put in our query right there so our query goes into"},{"timestamp":"3:22:31","text":"what we discussed here is the conversation protocol format which is this way that we maintain conversation"},{"timestamp":"3:22:36","text":"objects so this gets inserted there and then this whole thing ends up being just a token sequence a onedimensional token"},{"timestamp":"3:22:43","text":"sequence under the hood so Chachi PT saw this token sequence and then when we hit"},{"timestamp":"3:22:48","text":"go it basically continues appending tokens into this list it continues the"},{"timestamp":"3:22:53","text":"sequence it acts like a token autocomplete so in particular it gave us this response so we can basically just"},{"timestamp":"3:23:00","text":"put it here and we see the tokens that it continued uh these are the tokens that it continued with"},{"timestamp":"3:23:06","text":"roughly now the question becomes okay why are these the tokens that the model responded with what are"},{"timestamp":"3:23:12","text":"these tokens where are they coming from uh what are we talking to and how do we program this system and so that's where"},{"timestamp":"3:23:19","text":"we shifted gears and we talked about the under thehood pieces of it so the first stage of this process and there are"},{"timestamp":"3:23:25","text":"three stages is the pre-training stage which fundamentally has to do with just knowledge acquisition from the internet"},{"timestamp":"3:23:30","text":"into the parameters of this neural network and so the neural net internalizes a lot of Knowledge from the"},{"timestamp":"3:23:37","text":"internet but where the personality really comes in is in the process of supervised fine-tuning here and so what"},{"timestamp":"3:23:44","text":"what happens here is that basically the a company like openai will curate a large data set of conversations like say"},{"timestamp":"3:23:51","text":"1 million conversation across very diverse topics and there will be conversations between a human and an"},{"timestamp":"3:23:57","text":"assistant and even though there's a lot of synthetic data generation used throughout this entire process and a lot"},{"timestamp":"3:24:02","text":"of llm help and so on fundamentally this is a human data curation task with lots"},{"timestamp":"3:24:08","text":"of humans involved and in particular these humans are data labelers hired by open AI who are given labeling"},{"timestamp":"3:24:14","text":"instructions that they learn and they task is to create ideal assistant responses for any arbitrary prompts so"},{"timestamp":"3:24:21","text":"they are teaching the neural network by example how to respond to"},{"timestamp":"3:24:27","text":"prompts so what is the way to think about what came back here like what is"},{"timestamp":"3:24:32","text":"this well I think the right way to think about it is that this is the neural network simulation of a data labeler at"},{"timestamp":"3:24:40","text":"openai so it's as if I gave this query to a data Li open and this data labeler"},{"timestamp":"3:24:47","text":"first reads all of the labeling instructions from open Ai and then spends 2 hours writing up the ideal"},{"timestamp":"3:24:53","text":"assistant response to this query and uh giving it to me now we're not actually"},{"timestamp":"3:24:59","text":"doing that right because we didn't wait two hours so what we're getting here is a neural network simulation of that"},{"timestamp":"3:25:05","text":"process and we have to keep in mind that these neural networks don't function like human brains do they are different"},{"timestamp":"3:25:12","text":"what's easy or hard for them is different from what's easy or hard for humans and so we really are just getting"},{"timestamp":"3:25:17","text":"a simulation so here I shown you this is a token stream and this is fundamentally"},{"timestamp":"3:25:23","text":"the neural network with a bunch of activations and neurons in between this is a fixed mathematical expression that"},{"timestamp":"3:25:28","text":"mixes inputs from tokens with parameters of the model and they get mixed up and"},{"timestamp":"3:25:35","text":"get you the next token in a sequence but this is a finite amount of compute that happens for every single token and so"},{"timestamp":"3:25:41","text":"this is some kind of a lossy simulation of a human that is kind of like restricted in this way and so whatever"},{"timestamp":"3:25:49","text":"the humans write the language model is kind of imitating on this token level with only"},{"timestamp":"3:25:55","text":"this this specific computation for every single token and"},{"timestamp":"3:26:00","text":"sequence we also saw that as a result of this and the cognitive differences the models will suffer in a variety of ways"},{"timestamp":"3:26:08","text":"and uh you have to be very careful with their use so for example we saw that they will suffer from hallucinations and"},{"timestamp":"3:26:14","text":"they also we have the sense of a Swiss model of the LM capabilities where basically there's like holes in the"},{"timestamp":"3:26:20","text":"cheese sometimes the models will just arbitrarily like do something dumb uh so"},{"timestamp":"3:26:25","text":"even though they're doing lots of magical stuff sometimes they just can't so maybe you're not giving them enough tokens to think and maybe they're going"},{"timestamp":"3:26:32","text":"to just make stuff up because they're mental arithmetic breaks uh maybe they are suddenly unable to count number of"},{"timestamp":"3:26:38","text":"letters um or maybe they're unable to tell you that 911 9.11 is smaller than"},{"timestamp":"3:26:43","text":"9.9 and it looks kind of dumb and so so it's a Swiss cheese capability and we have to be careful with that and we saw"},{"timestamp":"3:26:49","text":"the reasons for that but fundamentally this is how we think of what came back it's again a"},{"timestamp":"3:26:56","text":"simulation of this neural network of a human data labeler following the"},{"timestamp":"3:27:03","text":"labeling instructions at open a so that's what we're getting back now I do"},{"timestamp":"3:27:09","text":"think that the uh things change a little bit when you actually go and reach for one of the thinking models like o03 mini"},{"timestamp":"3:27:17","text":"and the reason for that is that GPT 40 basically doesn't do reinforcement"},{"timestamp":"3:27:23","text":"learning it does do rhf but I've told you that rhf is not RL there's no"},{"timestamp":"3:27:29","text":"there's no uh time for magic in there it's just a little bit of a fine-tuning is the way to look at it but these"},{"timestamp":"3:27:35","text":"thinking models they do use RL so they go through this third state stage of"},{"timestamp":"3:27:41","text":"perfecting their thinking process and discovering new thinking strategies and uh"},{"timestamp":"3:27:46","text":"solutions to problem solving that look a little bit like your internal monologue in your head and they practice that on a"},{"timestamp":"3:27:53","text":"large collection of practice problems that companies like openi create and curate and um then make available to the"},{"timestamp":"3:28:00","text":"LMS so when I come here and I talked to a thinking model and I put in this"},{"timestamp":"3:28:05","text":"question what we're seeing here is not anymore just the straightforward simulation of a human data labeler like"},{"timestamp":"3:28:11","text":"this is actually kind of new unique and interesting um and of course open is not showing us the under thehood thinking"},{"timestamp":"3:28:18","text":"and the chains of thought that are underlying the reasoning here but we know that such a thing exists and this"},{"timestamp":"3:28:24","text":"is a summary of it and what we're getting here is actually not just an imitation of a human data labeler it's"},{"timestamp":"3:28:29","text":"actually something that is kind of new and interesting and exciting in the sense that it is a function of thinking"},{"timestamp":"3:28:35","text":"that was emergent in a simulation it's not just imitating human data labeler it comes from this reinforcement learning"},{"timestamp":"3:28:41","text":"process and so here we're of course not giving it a chance to shine because this is not a mathematical or a reasoning"},{"timestamp":"3:28:46","text":"problem this is just some kind of a sort of creative writing problem roughly speaking and I think it's um it's a a"},{"timestamp":"3:28:54","text":"question an open question as to whether the thinking strategies that are developed inside verifiable domains"},{"timestamp":"3:29:02","text":"transfer and are generalizable to other domains that are unverifiable such as"},{"timestamp":"3:29:07","text":"create writing the extent to which that transfer happens is unknown in the field I would say so we're not sure if we are"},{"timestamp":"3:29:14","text":"able to do RL on everything that is very verifiable and see the benefits of that on things that are unverifiable like"},{"timestamp":"3:29:20","text":"this prompt so that's an open question the other thing that's interesting is that this reinforcement learning here is"},{"timestamp":"3:29:26","text":"still like way too new primordial and nent so we're just seeing like the"},{"timestamp":"3:29:31","text":"beginnings of the hints of greatness uh in the reasoning problems we're seeing something that is in principle capable"},{"timestamp":"3:29:38","text":"of something like the equivalent of move 37 but not in the game of Go but in open"},{"timestamp":"3:29:44","text":"domain thinking and problem solving in principle this Paradigm is capable of doing something really cool new and"},{"timestamp":"3:29:50","text":"exciting something even that no human has thought of before in principle these models are capable of analogies no human"},{"timestamp":"3:29:56","text":"has had so I think it's incredibly exciting that these models exist but again it's very early and these are"},{"timestamp":"3:30:02","text":"primordial models for now um and they will mostly shine in domains that are verifiable like math en code Etc so very"},{"timestamp":"3:30:10","text":"interesting to play with and think about and use and then that's roughly it um um I"},{"timestamp":"3:30:16","text":"would say those are the broad Strokes of what's available right now I will say that overall it is an extremely exciting"},{"timestamp":"3:30:23","text":"time to be in the field personally I use these models all the time daily uh tens or hundreds of"},{"timestamp":"3:30:28","text":"times because they dramatically accelerate my work I think a lot of people see the same thing I think we're going to see a huge amount of wealth"},{"timestamp":"3:30:34","text":"creation as a result of these models be aware of some of their shortcomings even"},{"timestamp":"3:30:40","text":"with RL models they're going to suffer from some of these use it as a tool in a toolbox don't trust it fully because"},{"timestamp":"3:30:47","text":"they will randomly do dumb things they will randomly hallucinate they will randomly skip over some mental arithmetic and not get it right um they"},{"timestamp":"3:30:55","text":"randomly can't count or something like that so use them as tools in the toolbox check their work and own the product of"},{"timestamp":"3:31:00","text":"your work but use them for inspiration for first draft uh ask them questions"},{"timestamp":"3:31:06","text":"but always check and verify and you will be very successful in your work if you do so uh so I hope this video was useful"},{"timestamp":"3:31:13","text":"and interesting to you I hope you had it fun and uh it's already like very long so I apologize for that but I hope it"},{"timestamp":"3:31:19","text":"was useful and yeah I will see you later"}]);
            const [videoInfo, setVideoInfo] = React.useState({"title":"Deep Dive into LLMs like ChatGPT","url":"https://www.youtube.com/watch?v=7xTGNNLPyMI","thumbnail":"https://img.youtube.com/vi/7xTGNNLPyMI/maxresdefault.jpg","videoId":"7xTGNNLPyMI"});

            const handleURLSubmit = async (url) => {
                try {
                    const response = await fetch('/api/scrape', {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json',
                        },
                        body: JSON.stringify({ url })
                    });
                    const result = await response.json();
                    if (result.success) {
                        setTranscriptData(result.transcriptData);
                        setVideoInfo(result.videoInfo);
                    } else {
                        alert('Failed to get transcript');
                    }
                } catch (error) {
                    console.error('Error scraping transcript:', error);
                    alert('Error getting transcript');
                }
            };

            return (
                <div>
                    <URLInput onSubmit={handleURLSubmit} />
                    {transcriptData && videoInfo && (
                        <TranscriptViewer 
                            videoInfo={videoInfo} 
                            transcriptData={transcriptData} 
                        />
                    )}
                </div>
            );
        };

        // Render the App
        ReactDOM.render(<App />, document.getElementById('root'));
    </script>
</body>
</html>