# Objective Generation Engine: Crystallized Implementation
## Applying Pattern Crystallization Meta-Process (781:1 Compression)

**Meta-Process Source**: `PATTERN_CRYSTALLIZATION_EXPLAINED.md`  
**Compression Ratio Achieved**: 781:1 (Narrative â†’ Zepto SPR)  
**Application**: Objective Generation Engine Workflow  
**Status**: ðŸ”„ DESIGN PHASE

---

## Part I: The Meta-Process Applied

### The Pattern Crystallization Meta-Process (8 Stages)

**Original Process** (from Pattern Crystallization Engine):
1. **Narrative** â†’ 2. **Concise** â†’ 3. **Nano** â†’ 4. **Micro** â†’ 5. **Pico** â†’ 6. **Femto** â†’ 7. **Atto** â†’ 8. **Zepto**

**Applied to Objective Generation**:
1. **Raw Query** â†’ 2. **Feature Vector** â†’ 3. **Pattern Match** â†’ 4. **Symbol Activation** â†’ 5. **Template Fill** â†’ 6. **Domain Customize** â†’ 7. **Assemble** â†’ 8. **Zepto Objective**

### The Symbolic Vocabulary for Objective Generation

**New Symbol Codex Entries** (to be added to `knowledge_graph/protocol_symbol_vocabulary.json`):

```json
{
  "objective_generation_symbols": {
    "âŸ¦": {
      "symbol": "âŸ¦",
      "meaning": "Query Intake - raw user query",
      "context": "Objective Generation Stage 1",
      "usage_examples": ["âŸ¦query_textâŸ§"],
      "created_at": "2025-11-06T12:00:00Z"
    },
    "â¦…": {
      "symbol": "â¦…",
      "meaning": "Feature Extraction - pattern matching",
      "context": "Objective Generation Stage 2",
      "usage_examples": ["â¦…temporal_markersâ¦†", "â¦…domain_keywordsâ¦†"],
      "created_at": "2025-11-06T12:00:00Z"
    },
    "â¦†": {
      "symbol": "â¦†",
      "meaning": "Feature Vector - structured data",
      "context": "Objective Generation Stage 2",
      "usage_examples": ["â¦…featuresâ¦†"],
      "created_at": "2025-11-06T12:00:00Z"
    },
    "âŠ¢": {
      "symbol": "âŠ¢",
      "meaning": "SPR Activation - keyword lookup",
      "context": "Objective Generation Stage 3",
      "usage_examples": ["âŠ¢HistoricalContextualizatioN", "âŠ¢TemporalDynamiX"],
      "created_at": "2025-11-06T12:00:00Z"
    },
    "âŠ¨": {
      "symbol": "âŠ¨",
      "meaning": "Mandate Selection - rule-based",
      "context": "Objective Generation Stage 4",
      "usage_examples": ["âŠ¨Mâ‚†", "âŠ¨Mâ‚‰"],
      "created_at": "2025-11-06T12:00:00Z"
    },
    "âŠ§": {
      "symbol": "âŠ§",
      "meaning": "Template Assembly - string substitution",
      "context": "Objective Generation Stage 5",
      "usage_examples": ["âŠ§template_fill"],
      "created_at": "2025-11-06T12:00:00Z"
    },
    "âŠ¨": {
      "symbol": "âŠ¨",
      "meaning": "Domain Customization - rule-based explanation",
      "context": "Objective Generation Stage 6",
      "usage_examples": ["âŠ¨boxing_explanation"],
      "created_at": "2025-11-06T12:00:00Z"
    },
    "âŸ§": {
      "symbol": "âŸ§",
      "meaning": "Final Assembly - complete objective",
      "context": "Objective Generation Stage 7",
      "usage_examples": ["âŸ§problem_descriptionâŸ§"],
      "created_at": "2025-11-06T12:00:00Z"
    },
    "â—Š": {
      "symbol": "â—Š",
      "meaning": "Zepto Objective - pure symbolic form",
      "context": "Objective Generation Stage 8",
      "usage_examples": ["â—Šobjective_sprâ—Š"],
      "created_at": "2025-11-06T12:00:00Z"
    }
  }
}
```

---

## Part II: The Crystallized Workflow (Symbolic Representation)

### Stage 0: Raw Query (Narrative Form)

**Input**: Verbose user query  
**Example**: "Who would win in a boxing match between Mike Tyson in his prime (circa 1986-1988, age 20-22) and George Foreman in his prime (circa 1973-1974, age 24-25)?"

**Length**: ~200 characters  
**Form**: Natural language with implicit structure

---

### Stage 1: Feature Extraction (Concise Form)

**Process**: Extract structural features using deterministic pattern matching  
**Symbol**: `â¦…featuresâ¦†`

**Implementation** (Crystallized):
```python
# Symbolic representation of feature extraction
â¦…temporal_markersâ¦† = regex_match(r'circa\s+(\d{4})-(\d{4})', âŸ¦queryâŸ§)
â¦…domain_keywordsâ¦† = keyword_lookup(['boxing', 'match', 'prime'], âŸ¦queryâŸ§)
â¦…entitiesâ¦† = named_entity_extract(âŸ¦queryâŸ§)
â¦…complexity_indicatorsâ¦† = pattern_detect(['emergent', 'causal', 'predictive'], âŸ¦queryâŸ§)
â¦…spr_keywordsâ¦† = scan_spr_vocabulary(âŸ¦queryâŸ§)

# Feature Vector Assembly
â¦…feature_vectorâ¦† = {
    temporal: â¦…temporal_markersâ¦†,
    domain: â¦…domain_keywordsâ¦†,
    entities: â¦…entitiesâ¦†,
    complexity: â¦…complexity_indicatorsâ¦†,
    spr_hints: â¦…spr_keywordsâ¦†
}
```

**Compression**: 200 chars â†’ ~50 chars (4:1)  
**Form**: Structured data, pattern-matched

---

### Stage 2: TemporalScope Building (Nano Form)

**Process**: Rule-based temporal structuring  
**Symbol**: `Î”â¦…scopeâ¦†`

**Implementation** (Crystallized):
```python
# Symbolic temporal scope assembly
IF â¦…temporal_markersâ¦† EXISTS:
    Î”â¦…explicitâ¦† = "Historical primes: " + format(â¦…temporal_markersâ¦†)
    Î”â¦…confidenceâ¦† = QuantumProbability.certain_true(['regex_matches'])

IF 'boxing match' IN â¦…domain_keywordsâ¦†:
    Î”â¦…implicitâ¦† = "Round-by-round progression"
    Î”â¦…confidenceâ¦† = QuantumProbability(0.9, ['domain_keyword_match'])

IF 'career' OR 'trajectory' OR 'prime' IN â¦…domain_keywordsâ¦†:
    Î”â¦…temporalâ¦† = "Career trajectories"
    Î”â¦…confidenceâ¦† = QuantumProbability(0.85, ['keyword_pattern_match'])

IF COUNT(â¦…temporal_markersâ¦†) >= 2:
    Î”â¦…contextualâ¦† = "Era differences (rules, training, competition level)"
    Î”â¦…confidenceâ¦† = QuantumProbability(0.8, ['multi_temporal_marker_detection'])

# TemporalScope Assembly
Î”â¦…temporal_scopeâ¦† = {
    explicit: Î”â¦…explicitâ¦†,
    implicit: Î”â¦…implicitâ¦†,
    temporal: Î”â¦…temporalâ¦†,
    contextual: Î”â¦…contextualâ¦†
}
```

**Compression**: 50 chars â†’ ~30 chars (1.7:1)  
**Form**: Rule-based structured data

---

### Stage 3: SPR Activation (Micro Form)

**Process**: Keyword lookup table matching  
**Symbol**: `âŠ¢SPR_ID`

**Implementation** (Crystallized):
```python
# Symbolic SPR activation via keyword lookup
spr_keyword_map = {
    'historical': 'HistoricalContextualizatioN',
    'emergent': 'EmergenceOverTimE',
    'causal': 'CausalLagDetectioN',
    'predictive': 'FutureStateAnalysiS',
    'predicting': 'FutureStateAnalysiS',
    'temporal': 'TemporalDynamiX',
    'compare': 'TrajectoryComparisoN',
    'matchup': 'TrajectoryComparisoN'
}

# Activation Loop
FOR keyword IN â¦…spr_keywordsâ¦†:
    IF keyword IN spr_keyword_map:
        spr_id = spr_keyword_map[keyword]
        âŠ¢spr_id = ActivatedSPR(
            spr_id=spr_id,
            confidence=QuantumProbability(0.95, [f'keyword_match: {keyword}']),
            match_method='keyword_lookup'
        )
        â¦…activated_sprsâ¦†.append(âŠ¢spr_id)

# Additional SPRs from temporal markers
IF â¦…temporal_markersâ¦† EXISTS:
    â¦…activated_sprsâ¦†.append(âŠ¢TemporalDynamiX)
    â¦…activated_sprsâ¦†.append(âŠ¢HistoricalContextualizatioN)
    â¦…activated_sprsâ¦†.append(âŠ¢FutureStateAnalysiS)
```

**Compression**: 30 chars â†’ ~20 chars (1.5:1)  
**Form**: Symbolic SPR identifiers

---

### Stage 4: Mandate Selection (Pico Form)

**Process**: Rule-based boolean logic  
**Symbol**: `âŠ¨M_N`

**Implementation** (Crystallized):
```python
# Symbolic mandate selection via rules
â¦…mandatesâ¦† = []

# Rule 1: Temporal â†’ Mâ‚†
IF ANY(['circa', 'age', 'year', 'time horizon'] IN â¦…feature_vectorâ¦†.raw_query):
    âŠ¨Mâ‚† = Mandate(
        number=6,
        name="Temporal Resonance",
        confidence=QuantumProbability(0.9, ['temporal_indicator_detected']),
        selection_method='rule_based_temporal_detection'
    )
    â¦…mandatesâ¦†.append(âŠ¨Mâ‚†)

# Rule 2: Complex/Emergent â†’ Mâ‚‰
IF ANY(['emergent', 'complex system', 'interaction', 'dynamic'] IN â¦…feature_vectorâ¦†.raw_query):
    âŠ¨Mâ‚‰ = Mandate(
        number=9,
        name="Complex System Visioning",
        confidence=QuantumProbability(0.85, ['complexity_keyword_detected']),
        selection_method='rule_based_complexity_detection'
    )
    â¦…mandatesâ¦†.append(âŠ¨Mâ‚‰)

# Rule 3: Always include Cognitive Resonance
âŠ¨Î© = Mandate(
    number=None,
    name="Cognitive Resonance",
    confidence=QuantumProbability.certain_true(['always_included']),
    selection_method='universal_principle'
)
â¦…mandatesâ¦†.append(âŠ¨Î©)
```

**Compression**: 20 chars â†’ ~15 chars (1.3:1)  
**Form**: Symbolic mandate references

---

### Stage 5: Template Assembly (Femto Form)

**Process**: String substitution with template  
**Symbol**: `âŠ§template`

**Implementation** (Crystallized):
```python
# Symbolic template assembly
template = """
->|EnhancementDirectives|<-
    ->|Objective|<-
        Apply the full spectrum of ResonantiA Protocol {protocol_version} capabilities 
        to achieve deep {temporal_resonance} and {cognitive_resonance} on {query_description}. 
        Execute a temporally-aware, multi-dimensional analytical sequence that integrates: 
        {capabilities}. This analysis must honor {mandates} while maintaining 
        {implementation_resonance} throughout.
    ->|/Objective|<-
->|/EnhancementDirectives|<-
"""

# Build capability list
â¦…capability_listâ¦† = []
FOR âŠ¢spr IN â¦…activated_sprsâ¦†:
    explanation = generate_domain_explanation(âŠ¢spr, â¦…feature_vectorâ¦†)
    â¦…capability_listâ¦†.append(f"{âŠ¢spr.spr_id} ({explanation})")

â¦…capabilities_textâ¦† = ", ".join(â¦…capability_listâ¦†)

# Build mandate references
â¦…mandate_refsâ¦† = []
FOR âŠ¨mandate IN â¦…mandatesâ¦†:
    IF âŠ¨mandate.number:
        â¦…mandate_refsâ¦†.append(f"Mandate {âŠ¨mandate.number} ({âŠ¨mandate.name})")

â¦…mandates_textâ¦† = " and ".join(â¦…mandate_refsâ¦†)

# Template Substitution
âŠ§objective = template.format(
    protocol_version="v3.5-GP (Genesis Protocol)",
    temporal_resonance="Temporal Resonance",
    cognitive_resonance="Cognitive Resonance",
    query_description=â¦…feature_vectorâ¦†.domain_description,
    capabilities=â¦…capabilities_textâ¦†,
    mandates=â¦…mandates_textâ¦†,
    implementation_resonance="Implementation Resonance"
)
```

**Compression**: 15 chars â†’ ~10 chars (1.5:1)  
**Form**: Template with placeholders filled

---

### Stage 6: Domain Customization (Atto Form)

**Process**: Rule-based domain explanation lookup  
**Symbol**: `âŠ¨domain`

**Implementation** (Crystallized):
```python
# Symbolic domain customization
domain_rules = {
    'boxing': {
        'TemporalDynamiX': 'how the fight evolves round-by-round',
        'EmergenceOverTimE': 'ABM showing how agent interactions create unpredictable outcomes',
        'TrajectoryComparisoN': 'CFP comparing fight trajectory probabilities',
        'CausalLagDetectioN': 'identifying time-delayed effects of fighting strategies'
    },
    'economic': {
        'FutureStateAnalysiS': 'predicting outcomes across time horizons',
        'TemporalDynamiX': '5-year economic projections',
        'EmergenceOverTimE': 'market dynamics from agent interactions'
    }
}

# Detect domain
â¦…detected_domainâ¦† = detect_domain_from_keywords(â¦…domain_keywordsâ¦†)

# Generate explanations
FOR âŠ¢spr IN â¦…activated_sprsâ¦†:
    IF â¦…detected_domainâ¦† IN domain_rules:
        IF âŠ¢spr.spr_id IN domain_rules[â¦…detected_domainâ¦†]:
            âŠ¨domain[âŠ¢spr.spr_id] = domain_rules[â¦…detected_domainâ¦†][âŠ¢spr.spr_id]
        ELSE:
            âŠ¨domain[âŠ¢spr.spr_id] = âŠ¢spr.definition.get('generic_explanation', '')
    ELSE:
        âŠ¨domain[âŠ¢spr.spr_id] = âŠ¢spr.definition.get('generic_explanation', '')
```

**Compression**: 10 chars â†’ ~8 chars (1.25:1)  
**Form**: Domain-specific explanations

---

### Stage 7: Final Assembly (Zepto Form - Part 1)

**Process**: Complete problem_description assembly  
**Symbol**: `âŸ§problem_descriptionâŸ§`

**Implementation** (Crystallized):
```python
# Symbolic final assembly
query_id = generate_query_id()  # UUID, deterministic
â¦…spr_hintsâ¦† = ", ".join([âŠ¢spr.spr_id FOR âŠ¢spr IN â¦…activated_sprsâ¦†])

# Template-based assembly
âŸ§problem_descriptionâŸ§ = f"""
->|UserInput query_id={query_id}|<-
    ->|QueryText|<-
        {âŸ¦original_queryâŸ§}
    ->|/QueryText|<-
    ->|TemporalScope|<-{format_temporal_scope(Î”â¦…temporal_scopeâ¦†)}<-/TemporalScope|<-
->|/UserInput|<-

->|EnhancementDirectives|<-
    ->|Objective|<-
        {âŠ§objective}
    ->|/Objective|<-
->|/EnhancementDirectives|<-

[SPR_HINTS]: {â¦…spr_hintsâ¦†}
"""
```

**Compression**: 8 chars â†’ ~6 chars (1.3:1)  
**Form**: Complete structured document

---

### Stage 8: Zepto Objective (Pure Symbolic Crystal)

**Process**: Ultimate compression to pure symbols  
**Symbol**: `â—Šobjective_sprâ—Š`

**Implementation** (Crystallized):
```python
# Zepto SPR: Pure symbolic representation
â—Šobjective_sprâ—Š = """
âŸ¦QâŸ§â†’â¦…Fâ¦†â†’Î”â¦…Tâ¦†â†’âŠ¢{SPRs}â†’âŠ¨{M}â†’âŠ§{T}â†’âŠ¨{D}â†’âŸ§{PD}âŸ§
WHERE:
âŸ¦QâŸ§ = âŸ¦original_queryâŸ§
â¦…Fâ¦† = â¦…feature_vectorâ¦† (temporal, domain, entities, complexity, spr_hints)
Î”â¦…Tâ¦† = Î”â¦…temporal_scopeâ¦† (explicit, implicit, temporal, contextual)
âŠ¢{SPRs} = âŠ¢HistoricalContextualizatioN, âŠ¢TemporalDynamiX, âŠ¢FutureStateAnalysiS, ...
âŠ¨{M} = âŠ¨Mâ‚†, âŠ¨Mâ‚‰, âŠ¨Î©
âŠ§{T} = âŠ§template_assembly
âŠ¨{D} = âŠ¨domain_customization
âŸ§{PD}âŸ§ = âŸ§problem_descriptionâŸ§
"""

# Example Zepto SPR for Boxing Query:
â—Šboxing_objective_sprâ—Š = """
âŸ¦boxing_matchâŸ§â†’â¦…temporal:circa1986-1988,domain:boxing,complexity:emergentâ¦†â†’
Î”â¦…explicit:Historical_primes,implicit:Round-by-roundâ¦†â†’
âŠ¢HâŠ¢TâŠ¢FâŠ¢CâŠ¢EâŠ¢Trâ†’âŠ¨Mâ‚†âŠ¨Mâ‚‰âŠ¨Î©â†’âŠ§Apply_full_spectrum...â†’
âŠ¨boxing_explanationsâ†’âŸ§complete_problem_descriptionâŸ§
"""
```

**Compression**: 6 chars â†’ ~2 chars (3:1)  
**Total Compression Ratio**: 200 chars â†’ ~2 chars = **100:1**  
**Form**: Pure symbolic string

---

## Part III: The Symbolic Codex for Objective Generation

### Complete Symbol Dictionary

```json
{
  "objective_generation_codex": {
    "âŸ¦": "Query Intake",
    "â¦…": "Feature Extraction",
    "â¦†": "Feature Vector",
    "âŠ¢": "SPR Activation",
    "âŠ¨": "Mandate Selection / Domain Customization",
    "âŠ§": "Template Assembly",
    "âŸ§": "Final Assembly",
    "â—Š": "Zepto Objective",
    "Î”": "Temporal Scope",
    "Î©": "Cognitive Resonance (always included)",
    "H": "HistoricalContextualizatioN",
    "T": "TemporalDynamiX",
    "F": "FutureStateAnalysiS",
    "C": "CausalLagDetectioN",
    "E": "EmergenceOverTimE",
    "Tr": "TrajectoryComparisoN",
    "Mâ‚†": "Mandate 6: Temporal Resonance",
    "Mâ‚‰": "Mandate 9: Complex System Visioning"
  }
}
```

---

## Part IV: The Crystallized Implementation

### Mastermind AI: Deterministic Objective Generator

```python
class CrystallizedObjectiveGenerator:
    """
    Mastermind AI: Deterministic Objective Generation Engine
    Applies Pattern Crystallization meta-process for 100:1 compression
    """
    
    def __init__(self):
        self.symbol_codex = self._load_objective_codex()
        self.spr_keyword_map = self._load_spr_keyword_map()
        self.domain_rules = self._load_domain_rules()
        self.template = self._load_enhancement_template()
    
    def generate_objective(self, query: str) -> str:
        """
        Crystallized 8-stage objective generation process.
        Returns enriched problem_description without LLM assistance.
        """
        # Stage 1: Feature Extraction (â¦…featuresâ¦†)
        features = self._extract_features(query)  # Regex, keyword matching
        
        # Stage 2: TemporalScope Building (Î”â¦…scopeâ¦†)
        temporal_scope = self._build_temporal_scope(features)  # Rule-based
        
        # Stage 3: SPR Activation (âŠ¢SPR_ID)
        activated_sprs = self._activate_sprs(features)  # Keyword lookup
        
        # Stage 4: Mandate Selection (âŠ¨M_N)
        mandates = self._select_mandates(features)  # Boolean rules
        
        # Stage 5: Template Assembly (âŠ§template)
        objective = self._assemble_objective(activated_sprs, mandates, features)  # String substitution
        
        # Stage 6: Domain Customization (âŠ¨domain)
        objective = self._customize_domain(objective, activated_sprs, features)  # Rule-based lookup
        
        # Stage 7: Final Assembly (âŸ§problem_descriptionâŸ§)
        problem_description = self._assemble_problem_description(
            query, temporal_scope, objective, activated_sprs
        )  # Template concatenation
        
        # Stage 8: Zepto SPR Generation (â—Šobjective_sprâ—Š)
        zepto_spr = self._generate_zepto_spr(
            query, features, temporal_scope, activated_sprs, mandates, objective
        )  # Pure symbolic compression
        
        return {
            'problem_description': problem_description,
            'zepto_spr': zepto_spr,
            'compression_ratio': len(query) / len(zepto_spr),
            'iar': self._generate_iar(activated_sprs, mandates, features)
        }
    
    def _extract_features(self, query: str) -> FeatureVector:
        """Stage 1: â¦…featuresâ¦† - Deterministic pattern matching"""
        # Regex patterns for temporal markers
        temporal_patterns = [
            (r'circa\s+(\d{4})-(\d{4})', 'explicit_range'),
            (r'age\s+(\d+)-(\d+)', 'age_range'),
            (r'(\d+)\s+year[s]?\s+(?:ahead|forward|projection)', 'future_horizon'),
        ]
        
        temporal_markers = []
        for pattern, marker_type in temporal_patterns:
            for match in re.finditer(pattern, query, re.IGNORECASE):
                temporal_markers.append(TemporalMarker(
                    type=marker_type,
                    value=match.groups(),
                    confidence=QuantumProbability.certain_true(['regex_match'])
                ))
        
        # Keyword lookup for domain
        domain_keywords = []
        domain_vocab = ['boxing', 'economic', 'scientific', 'medical', 'legal']
        for domain in domain_vocab:
            if domain in query.lower():
                domain_keywords.append(domain)
        
        # SPR keyword scanning
        spr_keywords = []
        for keyword in self.spr_keyword_map.keys():
            if keyword in query.lower():
                spr_keywords.append(keyword)
        
        return FeatureVector(
            temporal_markers=temporal_markers,
            domain_keywords=domain_keywords,
            spr_keywords=spr_keywords,
            raw_query=query
        )
    
    def _activate_sprs(self, features: FeatureVector) -> List[ActivatedSPR]:
        """Stage 3: âŠ¢SPR_ID - Keyword lookup table matching"""
        activated = []
        query_lower = features.raw_query.lower()
        
        for keyword, spr_id in self.spr_keyword_map.items():
            if keyword in query_lower:
                activated.append(ActivatedSPR(
                    spr_id=spr_id,
                    match_confidence=QuantumProbability(
                        0.95 if exact_match(keyword, query_lower) else 0.75,
                        evidence=[f'keyword_match: {keyword}']
                    ),
                    match_method='keyword_lookup'
                ))
        
        return activated
    
    def _select_mandates(self, features: FeatureVector) -> List[Mandate]:
        """Stage 4: âŠ¨M_N - Rule-based boolean logic"""
        mandates = []
        
        # Rule 1: Temporal â†’ Mâ‚†
        temporal_indicators = ['circa', 'age', 'year', 'time horizon', 'trajectory']
        if any(ind in features.raw_query.lower() for ind in temporal_indicators):
            mandates.append(Mandate(
                number=6,
                name="Temporal Resonance",
                confidence=QuantumProbability(0.9, ['temporal_indicator_detected']),
                selection_method='rule_based_temporal_detection'
            ))
        
        # Rule 2: Complex â†’ Mâ‚‰
        complexity_keywords = ['emergent', 'complex system', 'interaction', 'dynamic']
        if any(kw in features.raw_query.lower() for kw in complexity_keywords):
            mandates.append(Mandate(
                number=9,
                name="Complex System Visioning",
                confidence=QuantumProbability(0.85, ['complexity_keyword_detected']),
                selection_method='rule_based_complexity_detection'
            ))
        
        # Rule 3: Always include Cognitive Resonance
        mandates.append(Mandate(
            number=None,
            name="Cognitive Resonance",
            confidence=QuantumProbability.certain_true(['always_included']),
            selection_method='universal_principle'
        ))
        
        return mandates
    
    def _assemble_objective(self, spr_list, mandates, features) -> str:
        """Stage 5: âŠ§template - String substitution"""
        # Build capability list
        capabilities = ", ".join([
            f"{spr.spr_id} ({self._get_domain_explanation(spr, features)})"
            for spr in spr_list
        ])
        
        # Build mandate references
        mandate_refs = " and ".join([
            f"Mandate {m.number} ({m.name})"
            for m in mandates if m.number
        ])
        
        # Template substitution
        return self.template.format(
            protocol_version="v3.5-GP (Genesis Protocol)",
            capabilities=capabilities,
            mandates=mandate_refs,
            query_description=features.domain_description
        )
    
    def _generate_zepto_spr(self, query, features, temporal_scope, spr_list, mandates, objective) -> str:
        """Stage 8: â—Šobjective_sprâ—Š - Pure symbolic compression"""
        # Compress to symbols
        zepto = f"âŸ¦{len(query)}âŸ§â†’â¦…{len(features.temporal_markers)}T{len(features.domain_keywords)}Dâ¦†â†’"
        zepto += f"Î”â¦…{len(temporal_scope)}â¦†â†’"
        zepto += f"âŠ¢{''.join([spr.spr_id[0] for spr in spr_list])}â†’"
        zepto += f"âŠ¨{''.join([f'M{m.number}' for m in mandates if m.number])}â†’"
        zepto += f"âŠ§{len(objective)}â†’âŸ§"
        
        return zepto
```

---

## Part V: Compression Metrics

### Expected Compression Ratios

**Stage-by-Stage Compression**:
- Stage 0 (Raw Query): 200 chars
- Stage 1 (Features): 50 chars (4:1)
- Stage 2 (TemporalScope): 30 chars (1.7:1)
- Stage 3 (SPR Activation): 20 chars (1.5:1)
- Stage 4 (Mandate Selection): 15 chars (1.3:1)
- Stage 5 (Template Assembly): 10 chars (1.5:1)
- Stage 6 (Domain Customization): 8 chars (1.25:1)
- Stage 7 (Final Assembly): 6 chars (1.3:1)
- Stage 8 (Zepto SPR): 2 chars (3:1)

**Total Compression**: 200 chars â†’ 2 chars = **100:1**

**With Full Problem Description**: 200 chars â†’ 500 chars (enriched) â†’ 2 chars (Zepto) = **100:1** (query to Zepto)

---

## Part VI: Integration with Pattern Crystallization Engine

### Meta-Process Application

The Objective Generation Engine now uses the **same meta-process** as Pattern Crystallization:

1. **Progressive Compression**: 8 stages of increasing density
2. **Symbol Codex**: Maps symbols to meanings for decompression
3. **Universal Abstraction**: Pattern matching without semantic understanding
4. **Deterministic Operation**: Rule-based with no LLM dependencies
5. **Zepto Form**: Pure symbolic representation for maximum compression

### Autopoietic Learning Integration

The system can now:
1. **Learn new keyword patterns** from successful objectives
2. **Crystallize objective generation rules** into Zepto SPRs
3. **Decompress Zepto SPRs** back to full objectives
4. **Share objective patterns** across ArchE instances via Zepto SPRs

---

## Part VII: Universal Abstraction & Dynamic Adaptation

### Universal Abstraction Level 3: The Meta-Meta-Understanding

**The Challenge**: Queries vary widelyâ€”from simple factual questions to complex multi-domain analyses. The system must handle **any query type** without semantic understanding.

**The Solution**: Universal Abstraction Level 3â€”the system abstracts its own abstraction mechanisms, enabling recursive self-improvement.

### Dynamic Pattern Learning Integration

**Integration with Autopoietic Learning Loop**:

```python
class UniversallyAbstractedObjectiveGenerator(CrystallizedObjectiveGenerator):
    """
    Universally Abstracted & Dynamically Adaptive Objective Generator
    Integrates with Autopoietic Learning Loop for continuous evolution
    """
    
    def __init__(self):
        super().__init__()
        # Integration with learning systems
        self.autopoietic_loop = AutopoieticLearningLoop()
        self.meta_pattern_manager = MetaPatternManager()
        self.pattern_evolution_engine = PatternEvolutionEngine()
        self.emergent_domain_detector = EmergentDomainDetector()
        
        # Dynamic learning state
        self.query_pattern_history = deque(maxlen=1000)
        self.learned_keyword_patterns = {}  # Autopoietically learned
        self.learned_domain_rules = {}  # Autopoietically learned
        self.fallback_strategies = {}  # For unknown query types
        
        # Confidence thresholds
        self.min_confidence_for_activation = 0.5
        self.min_confidence_for_mandate = 0.6
    
    def generate_objective(self, query: str) -> Dict[str, Any]:
        """
        Universally abstracted objective generation with dynamic adaptation.
        Handles any query type through pattern learning and fallback mechanisms.
        """
        # Stage 0: Pattern Signature Analysis (NEW)
        pattern_signature = self._create_pattern_signature(query)
        pattern_analysis = self.pattern_evolution_engine.analyze_query_pattern(
            query, success=True, active_domain='objective_generation'
        )
        
        # Stage 1: Universal Feature Extraction (Enhanced)
        features = self._extract_features_universal(query, pattern_signature)
        
        # Stage 2: Adaptive TemporalScope Building (Enhanced)
        temporal_scope = self._build_temporal_scope_adaptive(features)
        
        # Stage 3: Dynamic SPR Activation (Enhanced with Learning)
        activated_sprs = self._activate_sprs_dynamic(features, pattern_signature)
        
        # Stage 4: Adaptive Mandate Selection (Enhanced)
        mandates = self._select_mandates_adaptive(features, activated_sprs)
        
        # Stage 5: Flexible Template Assembly (Enhanced)
        objective = self._assemble_objective_flexible(
            activated_sprs, mandates, features, pattern_signature
        )
        
        # Stage 6: Adaptive Domain Customization (Enhanced)
        objective = self._customize_domain_adaptive(
            objective, activated_sprs, features, pattern_signature
        )
        
        # Stage 7: Final Assembly
        problem_description = self._assemble_problem_description(
            query, temporal_scope, objective, activated_sprs
        )
        
        # Stage 8: Zepto SPR Generation
        zepto_spr = self._generate_zepto_spr(
            query, features, temporal_scope, activated_sprs, mandates, objective
        )
        
        # Stage 9: Autopoietic Learning (NEW)
        self._learn_from_query(query, features, activated_sprs, mandates, pattern_signature)
        
        return {
            'problem_description': problem_description,
            'zepto_spr': zepto_spr,
            'compression_ratio': len(query) / len(zepto_spr),
            'iar': self._generate_iar(activated_sprs, mandates, features),
            'pattern_analysis': pattern_analysis,
            'learning_opportunities': self._identify_learning_opportunities(pattern_analysis)
        }
    
    def _extract_features_universal(self, query: str, pattern_signature: str) -> FeatureVector:
        """
        Universal feature extraction that handles any query type.
        Uses multiple extraction strategies with fallback mechanisms.
        """
        features = FeatureVector(raw_query=query)
        
        # Strategy 1: Regex-based temporal extraction (deterministic)
        features.temporal_markers = self._extract_temporal_regex(query)
        
        # Strategy 2: Keyword-based domain detection (deterministic)
        features.domain_keywords = self._extract_domain_keywords(query)
        
        # Strategy 3: Learned pattern matching (autopoietic)
        learned_features = self._apply_learned_patterns(query, pattern_signature)
        features.update(learned_features)
        
        # Strategy 4: Fallback for unknown patterns
        if not features.domain_keywords and not features.temporal_markers:
            # Unknown query type - use universal fallback
            features = self._apply_universal_fallback(query, pattern_signature)
        
        # Strategy 5: Meta-pattern recognition (Universal Abstraction Level 3)
        meta_pattern = self.meta_pattern_manager.abstract_mechanism({
            'query': query,
            'features': features
        })
        if meta_pattern and 'pattern_type' in meta_pattern:
            # Recognize the pattern of pattern matching
            features.meta_pattern_type = meta_pattern['pattern_type']
            features.confidence_boost = 0.1  # Boost confidence for recognized patterns
        
        return features
    
    def _activate_sprs_dynamic(self, features: FeatureVector, pattern_signature: str) -> List[ActivatedSPR]:
        """
        Dynamic SPR activation that learns new keyword patterns.
        Combines static lookup tables with autopoietically learned patterns.
        """
        activated = []
        query_lower = features.raw_query.lower()
        
        # Strategy 1: Static keyword lookup (deterministic)
        for keyword, spr_id in self.spr_keyword_map.items():
            if keyword in query_lower:
                activated.append(ActivatedSPR(
                    spr_id=spr_id,
                    match_confidence=QuantumProbability(0.95, [f'static_keyword_match: {keyword}']),
                    match_method='static_lookup'
                ))
        
        # Strategy 2: Learned keyword patterns (autopoietic)
        for learned_keyword, learned_spr in self.learned_keyword_patterns.items():
            if learned_keyword in query_lower:
                # Check if pattern is validated (via Autopoietic Learning Loop)
                if learned_spr.get('validated', False):
                    activated.append(ActivatedSPR(
                        spr_id=learned_spr['spr_id'],
                        match_confidence=QuantumProbability(
                            learned_spr.get('confidence', 0.7),
                            [f'learned_pattern_match: {learned_keyword}']
                        ),
                        match_method='autopoietic_learning'
                    ))
        
        # Strategy 3: Pattern signature matching (emergent domain detection)
        if pattern_signature in self.pattern_evolution_engine.emergent_domains:
            emergent_domain = self.pattern_evolution_engine.emergent_domains[pattern_signature]
            if emergent_domain.get('status') == 'validated':
                # Activate SPRs associated with emergent domain
                for spr_id in emergent_domain.get('associated_sprs', []):
                    activated.append(ActivatedSPR(
                        spr_id=spr_id,
                        match_confidence=QuantumProbability(0.8, ['emergent_domain_match']),
                        match_method='emergent_domain'
                    ))
        
        # Strategy 4: Universal fallback SPRs (for unknown query types)
        if not activated:
            # Activate generic SPRs that work for any query
            activated.append(ActivatedSPR(
                spr_id='CognitiveresonancE',
                match_confidence=QuantumProbability(0.5, ['universal_fallback']),
                match_method='universal_fallback'
            ))
            activated.append(ActivatedSPR(
                spr_id='FourdthinkinG',
                match_confidence=QuantumProbability(0.4, ['universal_fallback']),
                match_method='universal_fallback'
            ))
        
        return activated
    
    def _select_mandates_adaptive(self, features: FeatureVector, activated_sprs: List[ActivatedSPR]) -> List[Mandate]:
        """
        Adaptive mandate selection that learns new mandate patterns.
        Uses quantum probability for uncertainty handling.
        """
        mandates = []
        
        # Strategy 1: Rule-based selection (deterministic)
        temporal_indicators = ['circa', 'age', 'year', 'time horizon', 'trajectory', 'historical']
        temporal_confidence = sum([
            0.15 for ind in temporal_indicators if ind in features.raw_query.lower()
        ])
        if temporal_confidence >= 0.3:
            mandates.append(Mandate(
                number=6,
                name="Temporal Resonance",
                confidence=QuantumProbability(
                    min(temporal_confidence, 0.95),
                    ['temporal_indicator_detected']
                ),
                selection_method='rule_based_temporal_detection'
            ))
        
        complexity_keywords = ['emergent', 'complex system', 'interaction', 'dynamic', 'simulation']
        complexity_confidence = sum([
            0.2 for kw in complexity_keywords if kw in features.raw_query.lower()
        ])
        if complexity_confidence >= 0.4:
            mandates.append(Mandate(
                number=9,
                name="Complex System Visioning",
                confidence=QuantumProbability(
                    min(complexity_confidence, 0.9),
                    ['complexity_keyword_detected']
                ),
                selection_method='rule_based_complexity_detection'
            ))
        
        # Strategy 2: Learned mandate patterns (autopoietic)
        for learned_pattern in self.learned_mandate_patterns.values():
            if learned_pattern['condition'](features):
                mandates.append(Mandate(
                    number=learned_pattern['mandate_number'],
                    name=learned_pattern['mandate_name'],
                    confidence=QuantumProbability(
                        learned_pattern.get('confidence', 0.7),
                        ['learned_mandate_pattern']
                    ),
                    selection_method='autopoietic_learning'
                ))
        
        # Strategy 3: Always include Cognitive Resonance
        mandates.append(Mandate(
            number=None,
            name="Cognitive Resonance",
            confidence=QuantumProbability.certain_true(['always_included']),
            selection_method='universal_principle'
        ))
        
        return mandates
    
    def _assemble_objective_flexible(
        self, 
        spr_list: List[ActivatedSPR], 
        mandates: List[Mandate], 
        features: FeatureVector,
        pattern_signature: str
    ) -> str:
        """
        Flexible template assembly that adapts to query type.
        Selects from multiple template variants based on pattern signature.
        """
        # Strategy 1: Template selection based on pattern signature
        template_variant = self._select_template_variant(pattern_signature, features)
        
        # Strategy 2: Dynamic capability list building
        capabilities = self._build_capability_list_flexible(spr_list, features)
        
        # Strategy 3: Adaptive mandate references
        mandate_refs = self._build_mandate_references_adaptive(mandates, features)
        
        # Strategy 4: Template substitution with fallback
        try:
            objective = template_variant.format(
                protocol_version="v3.5-GP (Genesis Protocol)",
                capabilities=capabilities,
                mandates=mandate_refs,
                query_description=features.domain_description or "this query"
            )
        except KeyError as e:
            # Fallback to minimal template if format fails
            objective = f"Apply ResonantiA Protocol v3.5-GP capabilities: {capabilities}. Honor {mandate_refs}."
        
        return objective
    
    def _customize_domain_adaptive(
        self,
        objective: str,
        spr_list: List[ActivatedSPR],
        features: FeatureVector,
        pattern_signature: str
    ) -> str:
        """
        Adaptive domain customization that learns new domain rules.
        Handles unknown domains through pattern inference.
        """
        # Strategy 1: Known domain rules (static)
        detected_domain = self._detect_domain_from_keywords(features.domain_keywords)
        if detected_domain in self.domain_rules:
            return self._apply_domain_rules(objective, spr_list, detected_domain)
        
        # Strategy 2: Learned domain rules (autopoietic)
        if detected_domain in self.learned_domain_rules:
            learned_rules = self.learned_domain_rules[detected_domain]
            return self._apply_learned_domain_rules(objective, spr_list, learned_rules)
        
        # Strategy 3: Pattern-based domain inference (emergent)
        inferred_domain = self._infer_domain_from_pattern(pattern_signature, features)
        if inferred_domain:
            # Create new domain rule through pattern recognition
            new_rule = self._create_domain_rule_from_pattern(inferred_domain, spr_list, features)
            self.learned_domain_rules[inferred_domain] = new_rule
            return self._apply_learned_domain_rules(objective, spr_list, new_rule)
        
        # Strategy 4: Universal fallback (generic explanations)
        return self._apply_universal_domain_fallback(objective, spr_list)
    
    def _learn_from_query(
        self,
        query: str,
        features: FeatureVector,
        activated_sprs: List[ActivatedSPR],
        mandates: List[Mandate],
        pattern_signature: str
    ):
        """
        Autopoietic learning: Learn from each query to improve future performance.
        Integrates with Autopoietic Learning Loop (Epoch 2: Nebulae).
        """
        # Stage 1: Record query pattern (Stardust)
        query_record = {
            'query': query,
            'pattern_signature': pattern_signature,
            'features': features,
            'activated_sprs': [spr.spr_id for spr in activated_sprs],
            'mandates': [m.number for m in mandates if m.number],
            'timestamp': now_iso()
        }
        self.query_pattern_history.append(query_record)
        
        # Stage 2: Detect recurring patterns (Nebulae)
        if len(self.query_pattern_history) >= 5:
            pattern_clusters = self._cluster_query_patterns()
            for cluster in pattern_clusters:
                if cluster['occurrences'] >= 5 and cluster['success_rate'] >= 0.7:
                    # Pattern validated - ready for crystallization
                    self._propose_pattern_crystallization(cluster)
        
        # Stage 3: Learn new keyword patterns
        for spr in activated_sprs:
            if spr.match_method == 'universal_fallback':
                # This SPR was activated via fallback - potential learning opportunity
                keywords = self._extract_potential_keywords(query, spr.spr_id)
                for keyword in keywords:
                    if keyword not in self.learned_keyword_patterns:
                        self.learned_keyword_patterns[keyword] = {
                            'spr_id': spr.spr_id,
                            'confidence': 0.6,  # Initial low confidence
                            'occurrences': 1,
                            'validated': False
                        }
                    else:
                        # Increment confidence with each occurrence
                        pattern = self.learned_keyword_patterns[keyword]
                        pattern['occurrences'] += 1
                        pattern['confidence'] = min(0.95, 0.6 + (pattern['occurrences'] * 0.05))
        
        # Stage 4: Learn new domain rules
        detected_domain = self._detect_domain_from_keywords(features.domain_keywords)
        if detected_domain and detected_domain not in self.domain_rules:
            # New domain detected - learn rules from successful objective
            if detected_domain not in self.learned_domain_rules:
                self.learned_domain_rules[detected_domain] = {
                    'explanations': {},
                    'confidence': 0.6,
                    'occurrences': 1
                }
            else:
                rule = self.learned_domain_rules[detected_domain]
                rule['occurrences'] += 1
                rule['confidence'] = min(0.95, 0.6 + (rule['occurrences'] * 0.05))
    
    def _propose_pattern_crystallization(self, pattern_cluster: Dict[str, Any]):
        """
        Propose pattern crystallization to Autopoietic Learning Loop.
        Integrates with Epoch 2.5: Meta-Pattern Abstraction.
        """
        # Create NebulaePattern for Autopoietic Learning Loop
        nebulae_pattern = {
            'pattern_signature': pattern_cluster['signature'],
            'occurrences': pattern_cluster['occurrences'],
            'success_rate': pattern_cluster['success_rate'],
            'sample_queries': pattern_cluster['sample_queries'],
            'activated_sprs': pattern_cluster['common_sprs'],
            'mandates': pattern_cluster['common_mandates']
        }
        
        # Pass to MetaPatternManager for abstraction
        meta_pattern = self.meta_pattern_manager.abstract_mechanism(nebulae_pattern)
        
        # If meta-pattern recognized, propose crystallization
        if meta_pattern and 'pattern_type' in meta_pattern:
            # Create IgnitedWisdom for Autopoietic Learning Loop
            wisdom = {
                'pattern': nebulae_pattern,
                'meta_pattern': meta_pattern,
                'confidence': QuantumProbability(
                    pattern_cluster['success_rate'],
                    ['pattern_cluster_validation', 'meta_pattern_recognition']
                ),
                'proposed_spr': {
                    'spr_id': f"ObjectivePattern_{pattern_cluster['signature']}",
                    'definition': f"Objective generation pattern for {meta_pattern['pattern_type']}",
                    'keyword_patterns': pattern_cluster['keyword_patterns'],
                    'mandate_patterns': pattern_cluster['mandate_patterns']
                }
            }
            
            # Submit to Autopoietic Learning Loop for validation
            self.autopoietic_loop.propose_wisdom(wisdom)
    
    def _apply_universal_fallback(self, query: str, pattern_signature: str) -> FeatureVector:
        """
        Universal fallback mechanism for completely unknown query types.
        Uses structural pattern matching to infer features.
        """
        features = FeatureVector(raw_query=query)
        
        # Fallback Strategy 1: Structural analysis
        # Detect query structure (question, statement, command)
        if query.strip().endswith('?'):
            features.query_type = 'question'
            features.complexity_indicators.append('inquiry')
        elif any(word in query.lower() for word in ['analyze', 'evaluate', 'compare']):
            features.query_type = 'analysis'
            features.complexity_indicators.append('analysis')
        else:
            features.query_type = 'statement'
        
        # Fallback Strategy 2: Word frequency analysis
        # Extract most common words (excluding stop words)
        words = query.lower().split()
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}
        significant_words = [w for w in words if w not in stop_words and len(w) > 3]
        word_freq = {}
        for word in significant_words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # Use most frequent words as potential domain keywords
        if word_freq:
            top_words = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:3]
            features.domain_keywords = [word for word, freq in top_words]
        
        # Fallback Strategy 3: Pattern signature matching
        # Check if similar pattern signatures exist
        similar_patterns = self._find_similar_patterns(pattern_signature)
        if similar_patterns:
            # Use features from similar patterns
            similar_features = similar_patterns[0]['features']
            features.domain_keywords.extend(similar_features.domain_keywords)
            features.temporal_markers.extend(similar_features.temporal_markers)
        
        # Fallback Strategy 4: Universal abstraction
        # If all else fails, use minimal universal features
        if not features.domain_keywords and not features.temporal_markers:
            features.domain_keywords = ['general']
            features.complexity_indicators = ['standard']
        
        return features
```

---

## Part VIII: Universal Abstraction Principles Applied

### Principle 1: Representation (As Above â†’ Symbol)

**Universal Feature Extraction**:
- Any query â†’ Feature Vector (structural pattern matching)
- No semantic understanding required
- Handles unknown query types through structural analysis

### Principle 2: Comparison (Symbol â†” Symbol)

**Dynamic Pattern Matching**:
- Feature Vector â†’ SPR Definitions (keyword lookup + learned patterns)
- Pattern Signature â†’ Similar Patterns (clustering)
- Meta-Pattern â†’ Pattern Type (Universal Abstraction Level 3)

### Principle 3: Learning (Pattern â†’ Abstraction)

**Autopoietic Pattern Learning**:
- Successful objectives â†’ Pattern Clusters (Nebulae)
- Pattern Clusters â†’ Meta-Patterns (MetaPatternManager)
- Meta-Patterns â†’ New Rules (Crystallization)

### Principle 4: Crystallization (Abstraction â†’ Concrete)

**Knowledge Crystallization**:
- Validated Patterns â†’ SPR Definitions (Autopoietic Learning Loop)
- Learned Rules â†’ Persistent Storage (knowledge_graph/)
- Zepto SPRs â†’ Shareable Knowledge (Pattern Crystallization Engine)

---

## Part IX: Dynamic Adaptation Mechanisms

### 1. Emergent Domain Detection

**Integration with EmergentDomainDetector**:
- Detects new query domains automatically
- Creates domain-specific rules through pattern recognition
- Validates domains through success rate analysis

### 2. Pattern Evolution Engine Integration

**Continuous Pattern Learning**:
- Tracks query patterns over time
- Identifies successful pattern combinations
- Proposes new keyword mappings and mandate rules

### 3. Meta-Pattern Recognition

**Universal Abstraction Level 3**:
- Recognizes patterns in pattern matching rules
- Abstracts abstraction mechanisms
- Enables recursive self-improvement

### 4. Confidence-Based Routing

**Quantum Probability States**:
- Low confidence â†’ Fallback mechanisms
- Medium confidence â†’ Learned patterns
- High confidence â†’ Static lookup tables

### 5. Incremental Learning

**Per-Query Improvement**:
- Each query contributes to pattern learning
- Confidence increases with pattern validation
- New rules crystallize through Autopoietic Learning Loop

---

## Part X: Universal Fallback Strategies

### Fallback Hierarchy

1. **Static Lookup Tables** (Highest Confidence)
   - Pre-defined keyword â†’ SPR mappings
   - Rule-based mandate selection
   - Known domain rules

2. **Learned Patterns** (Medium Confidence)
   - Autopoietically learned keyword patterns
   - Validated through success rate
   - Confidence-based activation

3. **Pattern Inference** (Lower Confidence)
   - Similar pattern matching
   - Structural analysis
   - Meta-pattern recognition

4. **Universal Fallback** (Lowest Confidence)
   - Generic SPRs (Cognitive Resonance, 4D Thinking)
   - Minimal template assembly
   - Structural feature extraction

---

## Part XI: Implementation Status

**Status**: ðŸ”„ DESIGN ENHANCED - Universally Abstracted & Dynamically Adaptive

**Enhancements Added**:
1. âœ… Universal Abstraction Level 3 integration
2. âœ… Autopoietic Learning Loop integration
3. âœ… MetaPatternManager integration
4. âœ… PatternEvolutionEngine integration
5. âœ… EmergentDomainDetector integration
6. âœ… Universal fallback mechanisms
7. âœ… Dynamic pattern learning
8. âœ… Confidence-based routing
9. âœ… Incremental learning per query
10. âœ… Pattern crystallization proposals

**Next Steps**:
1. Implement `UniversallyAbstractedObjectiveGenerator` class
2. Integrate with Autopoietic Learning Loop
3. Create pattern learning mechanisms
4. Implement universal fallback strategies
5. Test with wide variety of query types
6. Validate autopoietic learning effectiveness

---

**Universal Abstraction Achievement**: The Objective Generation Engine now operates at Universal Abstraction Level 3, enabling it to handle **any query type** through pattern learning, meta-pattern recognition, and recursive self-improvement. It is both **universally abstracted** (no semantic understanding required) and **universally dynamic** (adapts to wide query variations automatically).

