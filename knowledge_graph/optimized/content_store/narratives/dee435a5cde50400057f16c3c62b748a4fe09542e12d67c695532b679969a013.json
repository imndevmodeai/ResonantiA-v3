{"content": "TERM: Class: IARValidator\n\nDEFINITION:\nValidates IAR structure compliance per crystallized artifacts specification\n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/workflow_engine.py, type: python_class\n\nIMPLEMENTATION CODE (workflow_engine.py) - First 30KB:\n```python\n# ResonantiA Protocol v3.0 - workflow_engine.py\n# Orchestrates the execution of defined workflows (Process Blueprints).\n# Manages context, dependencies, conditions, action execution, and error handling.\n# Critically handles Integrated Action Reflection (IAR) results by storing\n# the complete action output dictionary (including 'reflection') in the\n# context.\n\nimport json\nimport os\nimport logging\nimport copy\nimport time\nimport re\nimport uuid\nimport tempfile\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional, Set, Union, Tuple, Callable\nfrom jinja2 import Environment, meta, exceptions # Import Jinja2\n\n# --- Standardized Imports ---\n# This single block ensures robust relative imports within the package.\ntry:\n    from . import config\n    from .action_registry import execute_action, main_action_registry\n    from .spr_manager import SPRManager\n    from .error_handler import handle_action_error\n    from .action_context import ActionContext\n    from .workflow_recovery import WorkflowRecoveryHandler\n    from .recovery_actions import (\n        analyze_failure,\n        fix_template,\n        fix_action,\n        validate_workflow,\n        validate_action,\n        self_heal_output\n    )\n    from .system_genesis_tool import perform_system_genesis_action\n    from .qa_tools import run_code_linter, run_workflow_suite\n    from .output_handler import print_tagged_execution, print_tagged_results, display_output, display_workflow_start, display_workflow_progress, display_task_result, display_workflow_complete, display_workflow_error\n    from .custom_json import dumps\n    from .thought_trail import log_to_thought_trail\nexcept ImportError as e:\n    # This block should ideally not be hit if run as part of the package,\n    # but serves as a fallback and clear error indicator.\n    import logging\n    _import_error_msg = str(e)  # Capture error message as string\n    logging.getLogger(__name__).critical(f\"A critical relative import failed in workflow_engine: {e}\", exc_info=True)\n    # Set WorkflowOptimizer to None so code can check for it\n    WorkflowOptimizer = None\n    # Define dummy fallbacks to prevent outright crashing where possible\n    def execute_action(*args, **kwargs): return {\"error\": f\"Action Registry not available due to import error: {_import_error_msg}\"}\n    # Add other necessary fallbacks here as needed...\n    class SPRManager: pass\n    # Ensure decorator name is defined to avoid NameError in class annotations\n    def log_to_thought_trail(func): \n        return func\n\n# Optional import for WorkflowOptimizer (requires networkx)\ntry:\n    from .workflow_optimizer import WorkflowOptimizer\nexcept ImportError:\n    WorkflowOptimizer = None\n\nimport ast\nfrom datetime import datetime\n\n# ============================================================================\n# TEMPORAL CORE INTEGRATION (CANONICAL DATETIME SYSTEM)\n# ============================================================================\nfrom Three_PointO_ArchE.temporal_core import now, now_iso, ago, from_now, format_log, format_filename\n\n\n# Attempt to import numpy for numeric type checking in _compare_values,\n# optional\ntry:\n    import numpy as np\nexcept ImportError:\n    np = None  # Set to None if numpy is not available\n    logging.info(\n        \"Numpy not found, some numeric type checks in _compare_values might be limited.\")\n\nlogger = logging.getLogger(__name__)\n\n\n# === IAR COMPLIANCE ENHANCEMENT ===\n# Crystallized Artifacts Implementation - ARTIFACT 4A\n\nclass IARValidator:\n    \"\"\"Validates IAR structure compliance per crystallized artifacts specification\"\"\"\n    \n    def __init__(self):\n        self.required_fields = [\n            'status', 'summary', 'confidence', \n            'alignment_check', 'potential_issues',\n            'raw_output_preview'\n        ]\n        self.enhanced_fields = [\n            'tactical_resonance', 'crystallization_potential'\n        ]\n    \n    def validate_structure(self, iar_data):\n        \"\"\"Validate IAR structure meets all requirements\"\"\"\n        if not isinstance(iar_data, dict):\n            return False, [\"IAR must be a dictionary\"]\n        \n        missing_fields = []\n        for field in self.required_fields:\n            if field not in iar_data:\n                missing_fields.append(field)\n        \n        issues = []\n        if missing_fields:\n            issues.extend(\n                [f\"Missing required field: {field}\" for field in missing_fields])\n        \n        # Validate confidence is float between 0-1\n        confidence = iar_data.get('confidence')\n        if confidence is not None:\n            if not isinstance(\n    confidence, (int, float)) or not (\n        0.0 <= confidence <= 1.0):\n                issues.append(\"Confidence must be float between 0.0 and 1.0\")\n        \n        # Validate status is valid\n        status = iar_data.get('status')\n        if status not in ['Success', 'Partial', 'Failed']:\n            issues.append(\"Status must be 'Success', 'Partial', or 'Failed'\")\n        \n        return len(issues) == 0, issues\n    \n    def validate_enhanced_fields(self, iar_data):\n        \"\"\"Validate enhanced IAR fields for tactical resonance\"\"\"\n        enhanced_issues = []\n        \n        for field in self.enhanced_fields:\n            if field in iar_data:\n                value = iar_data[field]\n                if not isinstance(\n    value, (int, float)) or not (\n        0.0 <= value <= 1.0):\n                    enhanced_issues.append(\n    f\"{field} must be float between 0.0 and 1.0\")\n        \n        return len(enhanced_issues) == 0, enhanced_issues\n\n\nclass ResonanceTracker:\n    \"\"\"Tracks tactical resonance and crystallization metrics\"\"\"\n    \n    def __init__(self):\n        self.execution_history = []\n        self.resonance_metrics = {\n            'avg_tactical_resonance': 0.0,\n            'avg_crystallization_potential': 0.0,\n            'total_executions': 0\n        }\n    \n    def record_execution(self, task_id, iar_data, context):\n        \"\"\"Record task execution for resonance tracking\"\"\"\n        execution_record = {\n            'timestamp': now_iso(),\n            'task_id': task_id,\n            'status': iar_data.get('status'),\n            'confidence': iar_data.get('confidence', 0.0),\n            'tactical_resonance': iar_data.get('tactical_resonance', 0.0),\n            'crystallization_potential': iar_data.get('crystallization_potential', 0.0)\n        }\n        \n        self.execution_history.append(execution_record)\n        self._update_metrics()\n        \n        return execution_record\n    \n    def _update_metrics(self):\n        \"\"\"Update aggregate resonance metrics\"\"\"\n        if not self.execution_history:\n            return\n        \n        # Last 100 executions\n        recent_executions = self.execution_history[-100:]\n\n        tactical_scores = [ex.get('tactical_resonance', 0.0)\n                                  for ex in recent_executions]\n        crystallization_scores = [\n    ex.get(\n        'crystallization_potential',\n         0.0) for ex in recent_executions]\n        \n        self.resonance_metrics = {\n            'avg_tactical_resonance': sum(tactical_scores) / len(tactical_scores),\n            'avg_crystallization_potential': sum(crystallization_scores) / len(crystallization_scores),\n            'total_executions': len(self.execution_history)\n        }\n    \n    def get_resonance_report(self):\n        \"\"\"Get current resonance metrics report\"\"\"\n        return {\n            'current_metrics': self.resonance_metrics,\n            'recent_trend': self._calculate_trend(),\n            'compliance_score': self._calculate_compliance_score()\n        }\n    \n    def _calculate_trend(self):\n        \"\"\"Calculate resonance trend over recent executions\"\"\"\n        if len(self.execution_history) < 10:\n            return \"insufficient_data\"\n        \n        recent_10 = self.execution_history[-10:]\n        older_10 = self.execution_history[-20:-10]\n        \n        recent_avg = sum(ex.get('tactical_resonance', 0.0)\n                         for ex in recent_10) / 10\n        older_avg = sum(ex.get('tactical_resonance', 0.0)\n                        for ex in older_10) / 10\n        \n        if recent_avg > older_avg + 0.05:\n            return \"improving\"\n        elif recent_avg < older_avg - 0.05:\n            return \"declining\"\n        else:\n            return \"stable\"\n    \n    def _calculate_compliance_score(self):\n        \"\"\"Calculate overall IAR compliance score\"\"\"\n        if not self.execution_history:\n            return 0.0\n        \n        recent_executions = self.execution_history[-50:]\n        successful_executions = [\n    ex for ex in recent_executions if ex.get('status') == 'Success']\n        \n        success_rate = len(successful_executions) / len(recent_executions)\n        avg_confidence = sum(ex.get('confidence', 0.0)\n                             for ex in successful_executions) / max(len(successful_executions), 1)\n        avg_resonance = self.resonance_metrics['avg_tactical_resonance']\n        \n        # Weighted compliance score\n        compliance_score = (success_rate * 0.4) + \\\n                            (avg_confidence * 0.3) + (avg_resonance * 0.3)\n        return min(compliance_score, 1.0)\n\n# === Session State Manager ===\ntry:\n    from .session_state_manager import load_session_state, save_session_state, append_fact\n    from .context_superposition import create_context_bundle, merge_bundles\n    from .prefetch_manager import trigger_predictive_prefetch\n    from .sirc_autonomy import maybe_autorun_sirc\n    from .causal_digest import build_flux_annotated_digest\nexcept Exception:\n    # Fallback no-ops if module not available\n    def load_session_state():\n        return {\"facts_ledger\": [], \"updated_at\": now_iso()}\n    def save_session_state(state):\n        return None\n    def append_fact(state, fact):\n        state.setdefault(\"facts_ledger\", []).append({**fact, \"ts\": now_iso()})\n    def create_context_bundle(spr_def, runtime_context, initial_context):\n        return {\"spr_id\": spr_def.get(\"spr_id\"), \"created_at\": now_iso()}\n    def merge_bundles(bundles):\n        return {\"spr_index\": [b.get(\"spr_id\") for b in bundles]}\n\n\ndef _execute_standalone_workflow(workflow_definition: Dict[str, Any], initial_context: Dict[str, Any], parent_run_id: str, action_registry: Dict[str, Callable]) -> Dict[str, Any]:\n    \"\"\"\n    Executes a workflow definition in-memory, for use by meta-actions like for_each.\n    This is a simplified version of the main run_workflow loop, moved outside the class\n    to prevent circular dependencies.\n    \"\"\"\n    run_id = f\"{parent_run_id}_sub_{uuid.uuid4().hex[:8]}\"\n    initial_context[\"workflow_run_id\"] = run_id\n    \n    runtime_context = {\n        \"initial_context\": initial_context,\n        \"workflow_run_id\": run_id,\n        \"workflow_definition\": workflow_definition,\n    }\n    \n    tasks = workflow_definition.get('tasks', {})\n    task_statuses = {key: \"pending\" for key in tasks}\n    \n    sorted_tasks = list(tasks.keys()) \n\n    logger.info(f\"Starting standalone sub-workflow (Run ID: {run_id}).\")\n    start_time = time.time()\n\n    for task_key in sorted_tasks:\n        task_info = tasks[task_key]\n        action_type = task_info.get(\"action_type\")\n        \n        # This is a critical simplification: we can't use the full resolver here\n        # without re-introducing the dependency. We'll manually resolve for now.\n        resolved_inputs = {}\n        for k, v in task_info.get('inputs', {}).items():\n            if isinstance(v, str) and v == \"{{ item }}\":\n                resolved_inputs[k] = initial_context.get('item')\n            else:\n                resolved_inputs[k] = v\n\n        action_func = action_registry.get(action_type)\n        if not action_func:\n            error_result = {\"error\": f\"Sub-workflow failed: Action type '{action_type}' not found.\"}\n            runtime_context[task_key] = self._ensure_iar_compliance(error_result, action_context_obj)\n            task_statuses[task_key] = \"failed\"\n            break\n\n        try:\n            # The arguments must be passed as keyword arguments\n            # to align with the action wrappers in action_registry.py\n            result = action_func(**resolved_inputs)\n            if not isinstance(result, dict):\n                result = {\"output\": result}\n            runtime_context[task_key] = self._ensure_iar_compliance(result, action_context_obj)\n            task_statuses[task_key] = \"completed\"\n        except Exception as e:\n            error_msg = f\"Sub-workflow task '{task_key}' failed: {e}\"\n            logger.error(error_msg, exc_info=True)\n            error_result = {\"error\": error_msg}\n            runtime_context[task_key] = self._ensure_iar_compliance(error_result, action_context_obj)\n            task_statuses[task_key] = \"failed\"\n            break\n\n    end_time = time.time()\n    run_duration = round(end_time - start_time, 2)\n    \n    final_status = \"Completed\" if all(s == \"completed\" for s in task_statuses.values()) else \"Failed\"\n    \n    # Can't call _summarize_run directly, so construct a simplified summary\n    return {\n        \"workflow_name\": workflow_definition.get(\"name\", \"sub-workflow\"),\n        \"run_id\": run_id,\n        \"status\": final_status,\n        \"duration\": run_duration,\n        \"task_statuses\": task_statuses,\n        \"results\": runtime_context\n    }\n\n\nclass IARCompliantWorkflowEngine:\n    \"\"\"Enhanced workflow engine with IAR compliance and recovery support.\"\"\"\n\n    def __init__(self, workflows_dir: str = \"workflows\", spr_manager=None, event_callback: Optional[Callable] = None):\n        self.workflows_dir = workflows_dir\n        self.spr_manager = spr_manager\n        self.event_callback = event_callback  # <-- ADDED\n        self.last_workflow_name = None\n        self.action_registry = main_action_registry.actions.copy()  # Use centralized registry\n        self.recovery_handler = None\n        self.current_run_id = None\n        self.current_workflow = None\n        self.iar_validator = IARValidator()\n        self.resonance_tracker = ResonanceTracker()\n        self.jinja_env = Environment() # Initialize Jinja2 environment\n\n        # Register standard actions\n        self.register_action(\"display_output\", display_output)\n        self.register_action(\"perform_system_genesis_action\",\n     perform_system_genesis_action)\n        self.register_action(\"run_code_linter\", run_code_linter)\n        self.register_action(\"run_workflow_suite\", run_workflow_suite)\n\n        # Register recovery actions\n        self.register_recovery_actions()\n        logger.info(\n            \"IARCompliantWorkflowEngine initialized with full vetting capabilities\")\n\n    @log_to_thought_trail\n    def register_action(self, action_type: str, action_func: Callable) -> None:\n        \"\"\"Register an action function with the engine.\"\"\"\n        main_action_registry.register_action(\n            action_type,\n            action_func,\n        )\n        self.action_registry = main_action_registry.actions.copy()  # Update local copy\n        logger.debug(f\"Registered action: {action_type}\")\n\n    def register_recovery_actions(self) -> None:\n        \"\"\"Register recovery-related actions.\"\"\"\n        self.register_action(\"analyze_failure\", analyze_failure)\n        self.register_action(\"fix_template\", fix_template)\n        self.register_action(\"fix_action\", fix_action)\n        self.register_action(\"validate_workflow\", validate_workflow)\n        self.register_action(\"validate_action\", validate_action)\n        self.register_action(\"self_heal_output\", self_heal_output)\n        # Register the new for_each meta-action\n        self.register_action(\"for_each\", self._execute_for_each_task_wrapper)\n\n    def _execute_for_each_task_wrapper(self, inputs: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Wrapper for for_each action that can be called from action registry.\n        Creates a minimal ActionContext for the actual implementation.\n        \"\"\"\n        # Create a minimal ActionContext for the for_each implementation\n        context_for_action = ActionContext(\n            workflow_name=\"for_each_wrapper\",\n            task_name=\"for_each\",\n            runtime_context=kwargs.get('runtime_context', {}),\n            initial_context=kwargs.get('initial_context', {})\n        )\n        return self._execute_for_each_task(inputs, context_for_action)\n\n    def _execute_for_each_task(self, inputs: Dict[str, Any], context_for_action: ActionContext) -> Dict[str, Any]:\n        \"\"\"\n        Executes a sub-workflow for each item in a list.\n        This is a meta-action handled directly by the engine.\n        \"\"\"\n        logger.info(f\"Executing for_each task in workflow '{context_for_action.workflow_name}'\")\n        \n        items_list = inputs.get('items')\n        sub_workflow_def = inputs.get('workflow')\n\n        if not isinstance(items_list, list):\n            return {\"error\": f\"Input 'items' for for_each must be a list, but got {type(items_list)}.\"}\n        if not isinstance(sub_workflow_def, dict) or 'tasks' not in sub_workflow_def:\n            return {\"error\": \"Input 'workflow' for for_each must be a valid workflow definition with a 'tasks' section.\"}\n\n        all_results = []\n        \n        for index, item in enumerate(items_list):\n            logger.info(f\"Executing for_each iteration {index + 1}/{len(items_list)}.\")\n            \n            # The initial context for the sub-workflow includes the main context\n            # plus the current item.\n            sub_initial_context = copy.deepcopy(context_for_action.runtime_context.get('initial_context', {}))\n            sub_initial_context['item'] = item\n            sub_initial_context['loop_index'] = index\n            \n            # Run the sub-workflow directly without creating a new engine instance\n            sub_workflow_result = _execute_standalone_workflow(\n                workflow_definition=sub_workflow_def,\n                initial_context=sub_initial_context,\n                parent_run_id=context_for_action.run_id,\n                action_registry=self.action_registry\n            )\n            all_results.append(sub_workflow_result)\n        \n        logger.info(\"Completed all for_each iterations.\")\n        return {\"results\": all_results}\n    \n    @log_to_thought_trail\n    def _execute_task(self, task: Dict[str, Any],\n                      runtime_context: Dict[str, Any], initial_context: Dict[str, Any] = None, task_key: str = None, workflow_name: str = None, run_id: str = None) -> Dict[str, Any]:\n        \"\"\"Execute a single task with proper action type handling.\"\"\"\n        action_type = task.get(\"action_type\")\n        if not action_type:\n            raise ValueError(\"Task must specify action_type\")\n\n        if action_type not in self.action_registry:\n            raise ValueError(f\"Unknown action type: {action_type}\")\n\n        action_func = self.action_registry[action_type]\n        \n        # Build combined context for template resolution (initial_context + runtime_context)\n        combined_context = {**(initial_context or {}), **runtime_context}\n        \n        # Use the robust resolver that supports embedded placeholders and context paths\n        resolved_inputs = self._resolve_inputs(\n            task.get('inputs'),\n            runtime_context,  # runtime_context\n            initial_context,\n            task_key\n        )\n        \n        # Create ActionContext for actions that need it (like string_template_action)\n        action_context_obj = ActionContext(\n            task_key=task_key or \"unknown\",\n            action_name=task_key or \"unknown\",\n            action_type=action_type,\n            workflow_name=workflow_name or \"unknown\",\n            run_id=run_id or str(uuid.uuid4()),\n            attempt_number=1,\n            max_attempts=1,\n            execution_start_time=now(),\n            runtime_context=combined_context,\n            initial_context=initial_context or {}\n        )\n        \n        # Pass context_for_action to actions that need it\n        if action_type == \"string_template\":\n            resolved_inputs[\"context_for_action\"] = action_context_obj\n\n        try:\n            result = action_func(**resolved_inputs)\n            if not isinstance(result, dict):\n                result = {\"output\": result}\n            return result\n        except Exception as e:\n            error_msg = f\"Task '{task.get('description', 'Unknown')}' failed: {str(e)}\"\n            logger.error(error_msg)\n            \n            # Enhanced error reporting with input validation\n            if \"Missing required input\" in str(e):\n                logger.error(f\"Input validation failed for task. Provided inputs: {list(resolved_inputs.keys())}\")\n            elif \"NoneType\" in str(e):\n                logger.error(f\"Null value error - check if previous task outputs are properly formatted\")\n            \n            raise ValueError(error_msg)\n\n    def _resolve_template_variables(self, inputs: Dict[str, Any], results: Dict[str, Any], initial_context: Dict[str, Any] = None) -> Dict[str, Any]:\n        \"\"\"Resolve template variables in task inputs.\"\"\"\n        resolved = {}\n        initial_context = initial_context or {}\n        for key, value in inputs.items():\n            if isinstance(value, str) and \"{{\" in value and \"}}\" in value:\n                # Extract task and field from template\n                template = value.strip(\"{} \")\n                \n                # NEW: Add support for a default filter\n                default_value = None\n                if \"|\" in template:\n                    parts = template.split(\"|\", 1)\n                    template = parts[0].strip()\n                    filter_part = parts[1].strip()\n                    if filter_part.startswith(\"default(\"):\n                        default_value = filter_part[len(\"default(\"):-1].strip()\n                        # Basic handling for quotes\n                        if default_value.startswith((\"'\", '\"')) and default_value.endswith((\"'\", '\"')):\n                            default_value = default_value[1:-1]\n\n                parts = template.split(\".\", 1)\n                \n                # NEW: Add support for initial_context via `context.` prefix\n                if parts[0] == 'context':\n                    context_key = parts[1] if len(parts) > 1 else None\n                    # Check if key exists directly in initial_context (e.g., model, provider)\n                    if context_key and context_key in initial_context:\n                        resolved[key] = initial_context[context_key]\n                    # Also check if there's a nested 'context' dict\n                    elif context_key and 'context' in initial_context and isinstance(initial_context['context'], dict):\n                        if context_key in initial_context['context']:\n                            resolved[key] = initial_context['context'][context_key]\n                        else:\n                            resolved[key] = default_value if default_value is not None else value\n                    else:\n                        resolved[key] = default_value if default_value is not None else value\n                    continue\n\n                if len(parts) == 2:\n                    task_id, field_path = parts\n                else:\n                    task_id, field_path = parts[0], \"\"\n                \n                # Support nested field resolution (e.g., result.patterns)\n                if task_id in results:\n                    field_value = results[task_id]\n                    if field_path:\n                        for subfield in field_path.split('.'):\n                            if isinstance(field_value, dict) and subfield in field_value:\n                                field_value = field_value[subfield]\n                            else:\n                                field_value = None\n                                break\n                    if field_value is not None:\n                        resolved[key] = field_value\n                    else:\n                        resolved[key] = default_value if default_value is not None else value\n                else:\n                    resolved[key] = default_value if default_value is not None else value\n            else:\n                resolved[key] = value\n        return resolved\n\n    def get_resonance_dashboard(self) -> Dict[str, Any]:\n        \"\"\"Get the current resonance dashboard.\"\"\"\n        return {\n            \"run_id\": self.current_run_id,\n            \"workflow\": self.last_workflow_name,\n            \"resonance_report\": self.resonance_tracker.get_resonance_report(),\n            \"iar_validation\": self.iar_validator.get_validation_status() if hasattr(self.iar_validator, 'get_validation_status') else None\n        }\n\n    @log_to_thought_trail\n    def load_workflow(self, workflow_name: str) -> Dict[str, Any]:\n        \"\"\"Load workflow definition from file.\"\"\"\n        # Ensure the workflow path is absolute by joining with the engine's workflows_dir\n        if not os.path.isabs(workflow_name):\n            # Dynamic path resolution - check multiple possible locations\n            engine_dir = os.path.dirname(os.path.abspath(__file__))\n            possible_paths = [\n                workflow_name,  # Direct path\n                os.path.join(self.workflows_dir, os.path.basename(workflow_name)),  # In workflows dir\n                os.path.join(engine_dir, \"workflows\", os.path.basename(workflow_name)), # Relative to the engine\n                os.path.join(os.path.dirname(engine_dir), os.path.basename(workflow_name)),  # Project root\n                os.path.join(os.getcwd(), os.path.basename(workflow_name))  # Current working directory\n            ]\n            \n            workflow_path = None\n            for path in possible_paths:\n                if os.path.exists(path):\n                    workflow_path = path\n                    break\n            \n            if workflow_path is None:\n                raise FileNotFoundError(f\"Workflow file not found. Searched: {possible_paths}\")\n        else:\n            workflow_path = workflow_name\n\n        try:\n            logger.info(f\"Attempting to load workflow definition from: {workflow_path}\")\n            with open(workflow_path, 'r') as f:\n                workflow = json.load(f)\n\n            # Validate workflow structure\n            if \"tasks\" not in workflow:\n                raise ValueError(\"Workflow must contain 'tasks' section\")\n            \n            # Ensure workflow has a proper name\n            if \"name\" not in workflow and \"workflow_name\" not in workflow:\n                workflow[\"name\"] = f\"Unnamed Workflow ({os.path.basename(workflow_path)})\"\n            elif \"name\" not in workflow and \"workflow_name\" in workflow:\n                workflow[\"name\"] = workflow[\"workflow_name\"]\n\n            # Validate each task\n            for task_id, task in workflow[\"tasks\"].items():\n                if \"action_type\" not in task:\n                    raise ValueError(f\"Task '{task_id}' is missing required 'action_type'\")\n                if \"description\" not in task:\n                    raise ValueError(f\"Task '{task_id}' is missing required 'description'\")\n                if \"inputs\" not in task:\n                    raise ValueError(f\"Task '{task_id}' is missing required 'inputs'\")\n\n                # Verify action is registered\n                action_type = task[\"action_type\"]\n                if action_type not in self.action_registry:\n                    raise ValueError(f\"Action type '{action_type}' is not registered in the engine\")\n\n            self.last_workflow_name = workflow.get(\"name\", \"Unnamed Workflow\")\n            return workflow\n\n        except FileNotFoundError:\n            logger.error(f\"Workflow file not found at the specified path: {workflow_path}\")\n            raise\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to decode JSON from workflow file {workflow_path}: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error loading workflow file {workflow_path}: {str(e)}\")\n            raise\n\n    def _extract_var_path(self, template_content: str) -> str:\n        \"\"\"Extracts the core variable path from a template string, ignoring filters.\"\"\"\n        # The variable path is the part before the first pipe '|'\n        return template_content.split('|', 1)[0].strip()\n\n    def _parse_filters(self, template_content: str) -> list:\n        \"\"\"Parses filter syntax (e.g., | filter(arg1, 'arg2')) from a template string.\"\"\"\n        parts = template_content.split('|')[1:] # Get all parts after the variable path\n        filters = []\n        filter_regex = re.compile(r\"^\\s*(\\w+)\\s*(?:\\((.*?)\\))?\\s*$\") # Matches 'filter(args)' or 'filter'\n\n        for part in parts:\n            match = filter_regex.match(part)\n            if match:\n                name, args_str = match.groups()\n                args = []\n                if args_str:\n                    # This is a simplified arg parser; it splits by comma and handles basic quotes\n                    # A more robust solution might use shlex or a dedicated parsing library\n                    try:\n                        # Attempt to parse as a JSON list to handle quotes and types\n                        args = json.loads(f'[{args_str}]')\n                    except json.JSONDecodeError:\n                        # Fallback for non-standard JSON arguments\n                        args = [a.strip() for a in args_str.split(',')]\n                filters.append({\"name\": name, \"args\": args})\n        return filters\n\n    def _get_value_from_path(self, path: str, context: Dict[str, Any]) -> Any:\n        \"\"\"Gets a value from a nested dictionary using a dot-separated path.\"\"\"\n        keys = path.split('.')\n        value = context\n        for key in keys:\n            if isinstance(value, dict) and key in value:\n                value = value[key]\n            else:\n                return None # Path does not exist\n        return value\n\n    def _resolve_value(self, value: Any, context: Dict[str, Any]) -> Any:\n        \"\"\"\n        Rec...\n```\n\nEXAMPLE APPLICATION:\nValidates IAR structure compliance per crystallized artifacts specification\n\nCATEGORY: CodeKnowledge\n\nRELATIONSHIPS:\ntype: FromCodebase; source: /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/workflow_engine.py; source_type: python_class"}