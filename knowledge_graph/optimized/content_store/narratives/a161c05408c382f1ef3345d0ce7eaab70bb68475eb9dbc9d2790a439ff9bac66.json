{"content": "TERM: Function: ensure_directories\n\nDEFINITION:\nCreates necessary directories defined in config.py if they don't exist.\n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/main.py, type: python_function\n\nFULL IMPLEMENTATION CODE (main.py):\n```python\n# --- START OF FILE Three_PointO_ArchE/main.py ---\n# ResonantiA Protocol v3.0 - main.py\n# Example entry point demonstrating initialization and execution of the Arche system.\n# Handles workflow execution via IARCompliantWorkflowEngine and manages IAR-inclusive results.\n\nimport logging\nimport os\nimport json\nimport argparse\nimport sys\nimport time\nimport uuid # For unique workflow run IDs\nfrom typing import Optional, Dict, Any, Union # Added for type hinting clarity\n\n# --- Helper function to truncate values for summary ---\ndef truncate_value(value: Any, max_len: int = 70) -> str:\n    \"\"\"Converts a value to string and truncates it if too long.\"\"\"\n    try:\n        s_value = str(value)\n        if len(s_value) > max_len:\n            return s_value[:max_len-3] + \"...\"\n        return s_value\n    except Exception:\n        return \"[Unrepresentable Value]\"\n# --- End helper function ---\n\n# Setup logging FIRST using the centralized configuration\ntry:\n    # Assumes config and logging_config are in the same package directory\n    from . import config # Use relative import within the package\n    from .logging_config import setup_logging\n    setup_logging() # Initialize logging based on config settings\nexcept ImportError as cfg_imp_err:\n    # Basic fallback logging if config files are missing during setup\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', stream=sys.stdout)\n    logging.warning(f\"Could not import config/logging_config via relative import: {cfg_imp_err}. Using basic stdout logging.\", exc_info=True)\nexcept Exception as log_setup_e:\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', stream=sys.stdout)\n    logging.error(f\"Error setting up logging from logging_config.py: {log_setup_e}. Using basic config.\", exc_info=True)\n\n# Now import other core ResonantiA modules AFTER logging is configured\ntry:\n    from .workflow_engine import IARCompliantWorkflowEngine as IARCompliantWorkflowEngine\n    from .spr_manager import SPRManager\n    from .sirc_intake_handler import SIRCIntakeHandler\n    # config already imported above\nexcept ImportError as import_err:\n    logging.critical(f\"Failed to import core ResonantiA modules (IARCompliantWorkflowEngine, SPRManager, SIRCIntakeHandler): {import_err}. Check installation and paths.\", exc_info=True)\n    sys.exit(1) # Critical failure if core components cannot be imported\n\nlogger = logging.getLogger(__name__) # Get logger specifically for this module\n\ndef ensure_directories():\n    \"\"\"Creates necessary directories defined in config.py if they don't exist.\"\"\"\n    # Fetches paths from the config module\n    dirs_to_check = [\n        getattr(config, 'LOG_DIR', 'logs'),\n        getattr(config, 'OUTPUT_DIR', 'outputs'),\n        getattr(config, 'WORKFLOW_DIR', 'workflows'),\n        getattr(config, 'KNOWLEDGE_GRAPH_DIR', 'knowledge_graph'),\n        getattr(config, 'MODEL_SAVE_DIR', 'outputs/models') # Includes subdirectory for models\n    ]\n    logger.info(f\"Ensuring base directories exist: {dirs_to_check}\")\n    for d in dirs_to_check:\n        if d and isinstance(d, str): # Check if path is valid string\n            try:\n                os.makedirs(d, exist_ok=True) # exist_ok=True prevents error if dir exists\n            except OSError as e:\n                # Log critical error and raise to halt execution if essential dirs can't be made\n                logger.critical(f\"CRITICAL: Failed to create directory {d}: {e}. Check permissions.\", exc_info=True)\n                raise\n        else:\n            logger.warning(f\"Skipping invalid directory path configured: {d}\")\n\n    # Specifically ensure the SPR definitions file exists, creating an empty list if not\n    spr_file = getattr(config, 'SPR_JSON_FILE', None)\n    if spr_file and isinstance(spr_file, str):\n        if not os.path.exists(spr_file):\n            try:\n                spr_dir = os.path.dirname(spr_file)\n                if spr_dir: os.makedirs(spr_dir, exist_ok=True)\n                with open(spr_file, 'w', encoding='utf-8') as f:\n                    json.dump([], f) # Create file with an empty JSON list\n                logger.info(f\"Created empty SPR definitions file at {spr_file}\")\n            except IOError as e:\n                logger.error(f\"Could not create empty SPR file at {spr_file}: {e}\")\n            except Exception as e:\n                logger.error(f\"Unexpected error ensuring SPR file exists: {e}\", exc_info=True)\n    else:\n        logger.warning(\"SPR_JSON_FILE not configured or invalid in config.py.\")\n\ndef find_last_successful_run_id(output_dir: str) -> Optional[str]:\n    \"\"\"Finds the most recent successfully completed workflow run ID.\"\"\"\n    try:\n        result_files = [f for f in os.listdir(output_dir) if f.startswith(\"result_\") and f.endswith(\".json\")]\n        result_files.sort(key=lambda f: os.path.getmtime(os.path.join(output_dir, f)), reverse=True)\n\n        for filename in result_files:\n            filepath = os.path.join(output_dir, filename)\n            with open(filepath, 'r') as f:\n                result_data = json.load(f)\n                if result_data.get(\"workflow_status\") == \"Completed Successfully\":\n                    run_id = result_data.get(\"workflow_run_id\")\n                    if run_id:\n                        logger.info(f\"Found last successful run_id: {run_id} from file {filename}\")\n                        return run_id\n    except Exception as e:\n        logger.error(f\"Could not determine last successful run ID: {e}\", exc_info=True)\n    return None\n\ndef handle_sirc_directive(args):\n    \"\"\"Handler for SIRC directive processing with DirectiveClarificationProtocoL.\"\"\"\n    logger.info(f\"--- Processing SIRC directive: {args.directive[:100]}... ---\")\n    \n    try:\n        # Initialize SPR Manager for SIRC handler\n        spr_manager = SPRManager(getattr(config, 'SPR_JSON_FILE', None))\n        \n        # Initialize SIRC Intake Handler\n        sirc_handler = SIRCIntakeHandler(spr_manager=spr_manager)\n        \n        # Process directive through clarification protocol\n        clarification_result = sirc_handler.process_directive(args.directive)\n        \n        # Display results\n        print(\"\\n=== SIRC DIRECTIVE CLARIFICATION RESULT ===\")\n        print(f\"Original Directive: {clarification_result['original_directive']}\")\n        print(f\"Finalized Objective: {clarification_result['finalized_objective']}\")\n        print(f\"Clarity Score: {clarification_result['clarity_score']:.2f}\")\n        print(f\"Clarification Needed: {clarification_result['clarification_needed']}\")\n        \n        if 'resonance_validation' in clarification_result:\n            print(\"\\nResonance Validation:\")\n            for key, value in clarification_result['resonance_validation'].items():\n                print(f\"  {key}: {value}\")\n        \n        # Save results\n        # Use new config container if present\n        try:\n            output_dir = config.CONFIG.paths.outputs.as_posix()\n        except Exception:\n            output_dir = getattr(config, 'OUTPUT_DIR', 'outputs')\n        os.makedirs(output_dir, exist_ok=True)\n        output_filename = os.path.join(output_dir, f\"sirc_clarification_{uuid.uuid4().hex[:8]}.json\")\n        \n        with open(output_filename, 'w', encoding='utf-8') as f:\n            json.dump(clarification_result, f, indent=2, default=str)\n        logger.info(f\"SIRC clarification result saved to {output_filename}\")\n        \n        # If clarification successful and execution ready, offer to proceed with SIRC\n        if clarification_result.get('clarity_score', 0) > 0.85:\n            print(f\"\\n✓ Objective clarity threshold met (score: {clarification_result['clarity_score']:.2f})\")\n            print(\"Ready for SIRC Phase 3: Harmonization Check\")\n            \n            # Here we would normally proceed to full SIRC processing\n            # For now, we'll just log the readiness\n            logger.info(\"Directive successfully clarified and ready for SIRC continuation\")\n        else:\n            print(f\"\\n⚠ Objective clarity below threshold (score: {clarification_result['clarity_score']:.2f})\")\n            print(\"Additional clarification may be needed before SIRC processing\")\n            \n    except Exception as e:\n        logger.error(f\"Error processing SIRC directive: {e}\", exc_info=True)\n        print(f\"ERROR: Failed to process SIRC directive: {e}\")\n\ndef handle_run_workflow(args):\n    \"\"\"Handler for the 'run-workflow' command.\"\"\"\n    logger.info(f\"--- Received command to run workflow: {args.workflow_name} ---\")\n    initial_context = {}\n    if args.context_file:\n        try:\n            with open(args.context_file, 'r') as f:\n                initial_context = json.load(f)\n            logger.info(f\"Loaded initial context from: {args.context_file}\")\n        except (FileNotFoundError, json.JSONDecodeError) as e:\n            logger.error(f\"Error loading context file {args.context_file}: {e}\")\n            # Decide if you want to exit or run with an empty context\n            return \n\n    try:\n        engine = IARCompliantWorkflowEngine()\n\n        # Construct the full path to the workflow file\n        workflow_dir = getattr(config, 'WORKFLOW_DIR', 'workflows')\n        workflow_path = os.path.join(workflow_dir, args.workflow_name)\n        \n        final_context = engine.run_workflow(workflow_path, initial_context)\n        \n        # Determine the final status for logging\n        final_status = final_context.get(\"workflow_status\", \"Unknown\")\n\n        # Save the final context to a file\n        try:\n            output_dir = config.CONFIG.paths.outputs.as_posix()\n        except Exception:\n            output_dir = getattr(config, 'OUTPUT_DIR', 'outputs')\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Sanitize workflow name for the filename and add run_id\n        sanitized_name = os.path.splitext(os.path.basename(args.workflow_name))[0].replace(\" \", \"_\")\n        run_id = final_context.get('workflow_run_id', 'no_run_id')\n        output_filename = os.path.join(output_dir, f\"result_{sanitized_name}_{run_id}.json\")\n\n        try:\n            with open(output_filename, 'w', encoding='utf-8') as f:\n                # Use default=str to handle potential non-serializable types gracefully (e.g., numpy types)\n                json.dump(final_context, f, indent=2, default=str)\n            logger.info(f\"Final result saved successfully to {output_filename}\")\n        except TypeError as json_err:\n            # Handle cases where the result dictionary contains objects JSON can't serialize directly\n            logger.error(f\"Could not serialize final result to JSON: {json_err}. Result likely contains non-standard objects (e.g., complex numbers, custom classes). Saving string representation as fallback.\", exc_info=True)\n            fallback_filename = output_filename.replace('.json', '_error_repr.txt')\n            try:\n                with open(fallback_filename, 'w', encoding='utf-8') as f:\n                    f.write(f\"Original JSON serialization error: {json_err}\\n\\n\")\n                    f.write(\"--- Full Result (repr) ---\\n\")\n                    f.write(repr(final_context)) # Write the Python representation\n                logger.info(f\"String representation saved to {fallback_filename}\")\n            except Exception as write_err:\n                logger.error(f\"Could not write fallback string representation: {write_err}\")\n        except IOError as io_err:\n            logger.error(f\"Could not write final result to {output_filename}: {io_err}\")\n        except Exception as save_err:\n            logger.error(f\"Unexpected error saving final result: {save_err}\", exc_info=True)\n\n        # --- Print Summary to Console ---\n        # Provides a quick overview of the execution outcome\n        print(\"\\n--- Workflow Final Result Summary (v3.0) ---\")\n        try:\n            summary = {}\n            summary['workflow_name'] = args.workflow_name\n            summary['workflow_run_id'] = run_id\n            summary['overall_status'] = final_status\n            summary['run_duration_sec'] = final_context.get('workflow_run_duration_sec', 'N/A')\n\n            # Summarize status and IAR reflection highlights for each task\n            task_statuses = final_context.get('task_statuses', {})\n            summary['task_summary'] = {}\n            for task_id, status in task_statuses.items():\n                task_result = final_context.get(task_id, {})\n                reflection = task_result.get('reflection', {}) if isinstance(task_result, dict) else {}\n                \n                inputs_preview = {}\n                if isinstance(task_result, dict) and 'resolved_inputs' in task_result and isinstance(task_result['resolved_inputs'], dict):\n                    for k, v in task_result['resolved_inputs'].items():\n                        inputs_preview[k] = truncate_value(v)\n                \n                outputs_preview = {}\n                if isinstance(task_result, dict):\n                    # Exclude known meta-keys and reflection from outputs preview\n                    excluded_output_keys = ['reflection', 'resolved_inputs', 'error', 'status', 'start_time', 'end_time', 'duration_sec', 'attempt_number']\n                    for k, v in task_result.items():\n                        if k not in excluded_output_keys:\n                            outputs_preview[k] = truncate_value(v)\n\n                summary['task_summary'][task_id] = {\n                    \"status\": status,\n                    \"inputs_preview\": inputs_preview if inputs_preview else \"N/A\",\n                    \"outputs_preview\": outputs_preview if outputs_preview else \"N/A\",\n                    \"reflection_status\": reflection.get('status', 'N/A'),\n                    \"reflection_confidence\": reflection.get('confidence', 'N/A'),\n                    \"reflection_issues\": reflection.get('potential_issues', None),\n                    \"error\": truncate_value(task_result.get('error', None)) # Also truncate error messages\n                }\n            # Print the summary dict as formatted JSON\n            print(json.dumps(summary, indent=2, default=str))\n        except Exception as summary_e:\n            print(f\"(Could not generate summary: {summary_e})\")\n            print(f\"Full results saved to {output_filename} (or fallback file).\")\n        print(\"---------------------------------------------\\n\")\n\n    except Exception as e:\n        logger.critical(f\"A critical error occurred in the main execution block: {e}\", exc_info=True)\n\ndef main(workflow_to_run: str, initial_context_json: Optional[str] = None):\n    \"\"\"\n    Main execution function: Initializes system, runs workflow, saves results.\n    \"\"\"\n    logger.info(\"--- Arche System Initialization (ResonantiA Protocol v3.0) ---\")\n    logger.warning(\"Keyholder Override ('IMnDEVmode') is conceptually active for this session (as per protocol doc).\") # Note: Actual check might be needed elsewhere\n\n    # Ensure directories exist before initializing components that might need them\n    try:\n        ensure_directories()\n    except Exception as dir_e:\n        # If directory creation failed, log critical and exit\n        logger.critical(f\"Failed to ensure necessary directories: {dir_e}. Exiting.\")\n        sys.exit(1)\n\n    # Initialize core components\n    try:\n        spr_manager = SPRManager(getattr(config, 'SPR_JSON_FILE', None)) # Pass configured path\n        logger.info(f\"SPR Manager initialized. Loaded {len(spr_manager.sprs)} SPRs from '{spr_manager.filepath}'.\")\n    except (ValueError, TypeError) as spr_init_err: # Catch config errors specifically\n        logger.critical(f\"Failed to initialize SPR Manager due to configuration issue (SPR_JSON_FILE): {spr_init_err}. Exiting.\", exc_info=True)\n        sys.exit(1)\n    except Exception as spr_e:\n        logger.critical(f\"Unexpected error initializing SPR Manager: {spr_e}. Exiting.\", exc_info=True)\n        sys.exit(1)\n\n    try:\n        # Pass the initialized SPR manager to the engine if needed (e.g., for SPR context)\n        workflow_engine = IARCompliantWorkflowEngine(spr_manager=spr_manager)\n        logger.info(\"Workflow Engine initialized.\")\n    except Exception as wf_e:\n        logger.critical(f\"Failed to initialize Workflow Engine: {wf_e}. Exiting.\", exc_info=True)\n        sys.exit(1)\n\n    # --- Prepare Initial Context ---\n    initial_context: Dict[str, Any] = {}\n\n    # initial_context_json = \"{\\\"raw_user_query\\\": \\\"Can you provide an InnovativE SolutioN for energy crisis?\\\", \\\"user_id\\\": \\\"cli_keyholder_IMnDEVmode\\\", \\\"protocol_version\\\": \\\"3.0\\\"}\" # HARCODED FOR TEST\n\n    if initial_context_json:\n        try:\n            # Load context from JSON string argument\n            initial_context = json.loads(initial_context_json)\n            if not isinstance(initial_context, dict):\n                # Ensure the loaded JSON is actually a dictionary\n                raise json.JSONDecodeError(\"Initial context must be a JSON object (dictionary).\", initial_context_json, 0)\n            logger.info(\"Loaded initial context from command line argument.\")\n        except json.JSONDecodeError as e:\n            logger.error(f\"Invalid JSON provided for initial context: {e}. Starting with minimal context including error.\", exc_info=True)\n            initial_context = {\"error_loading_context\": f\"Invalid JSON: {e}\", \"raw_context_input\": initial_context_json}\n\n    # Add/ensure essential context variables\n    initial_context[\"user_id\"] = initial_context.get(\"user_id\", \"cli_keyholder_IMnDEVmode\") # Example user ID\n    initial_context[\"workflow_run_id\"] = initial_context.get(\"workflow_run_id\", f\"run_{uuid.uuid4().hex}\") # Unique ID for this run\n    initial_context[\"protocol_version\"] = \"3.0\" # Stamp the protocol version\n    \n    # Find and inject the last successful run ID for analysis workflows\n    last_run_id = find_last_successful_run_id(config.OUTPUT_DIR)\n    if last_run_id:\n        initial_context[\"last_successful_run_id\"] = last_run_id\n\n    # --- Execute Workflow ---\n    logger.info(f\"Attempting to execute workflow: '{workflow_to_run}' (Run ID: {initial_context['workflow_run_id']})\")\n    final_result: Dict[str, Any] = {}\n    try:\n        # Core execution call\n        final_result = workflow_engine.run_workflow(workflow_to_run, initial_context)\n        logger.info(f\"Workflow '{workflow_engine.last_workflow_name or workflow_to_run}' execution finished.\") # Use name loaded by engine if available\n\n        # --- Save Full Results ---\n        # Construct a unique filename for the results\n        base_workflow_name = os.path.basename(workflow_to_run).replace('.json', '')\n        output_filename = os.path.join(config.OUTPUT_DIR, f\"result_{base_workflow_name}_{initial_context['workflow_run_id']}.json\")\n\n        logger.info(f\"Attempting to save full final result dictionary to {output_filename}\")\n        try:\n            with open(output_filename, 'w', encoding='utf-8') as f:\n                # Use default=str to handle potential non-serializable types gracefully (e.g., numpy types)\n                json.dump(final_result, f, indent=2, default=str)\n            logger.info(f\"Final result saved successfully.\")\n        except TypeError as json_err:\n            # Handle cases where the result dictionary contains objects JSON can't serialize directly\n            logger.error(f\"Could not serialize final result to JSON: {json_err}. Result likely contains non-standard objects (e.g., complex numbers, custom classes). Saving string representation as fallback.\", exc_info=True)\n            fallback_filename = output_filename.replace('.json', '_error_repr.txt')\n            try:\n                with open(fallback_filename, 'w', encoding='utf-8') as f:\n                    f.write(f\"Original JSON serialization error: {json_err}\\n\\n\")\n                    f.write(\"--- Full Result (repr) ---\\n\")\n                    f.write(repr(final_result)) # Write the Python representation\n                logger.info(f\"String representation saved to {fallback_filename}\")\n            except Exception as write_err:\n                logger.error(f\"Could not write fallback string representation: {write_err}\")\n        except IOError as io_err:\n            logger.error(f\"Could not write final result to {output_filename}: {io_err}\")\n        except Exception as save_err:\n            logger.error(f\"Unexpected error saving final result: {save_err}\", exc_info=True)\n\n        # --- Print Summary to Console ---\n        # Provides a quick overview of the execution outcome\n        print(\"\\n--- Workflow Final Result Summary (v3.0) ---\")\n        try:\n            summary = {}\n            summary['workflow_name'] = workflow_engine.last_workflow_name or workflow_to_run\n            summary['workflow_run_id'] = initial_context['workflow_run_id']\n            summary['overall_status'] = final_result.get('workflow_status', 'Unknown')\n            summary['run_duration_sec'] = final_result.get('workflow_run_duration_sec', 'N/A')\n\n            # Summarize status and IAR reflection highlights for each task\n            task_statuses = final_result.get('task_statuses', {})\n            summary['task_summary'] = {}\n            for task_id, status in task_statuses.items():\n                task_result = final_result.get(task_id, {})\n                reflection = task_result.get('reflection', {}) if isinstance(task_result, dict) else {}\n                \n                inputs_preview = {}\n                if isinstance(task_result, dict) and 'resolved_inputs' in task_result and isinstance(task_result['resolved_inputs'], dict):\n                    for k, v in task_result['resolved_inputs'].items():\n                        inputs_preview[k] = truncate_value(v)\n                \n                outputs_preview = {}\n                if isinstance(task_result, dict):\n                    # Exclude known meta-keys and reflection from outputs preview\n                    excluded_output_keys = ['reflection', 'resolved_inputs', 'error', 'status', 'start_time', 'end_time', 'duration_sec', 'attempt_number']\n                    for k, v in task_result.items():\n                        if k not in excluded_output_keys:\n                            outputs_preview[k] = truncate_value(v)\n\n                summary['task_summary'][task_id] = {\n                    \"status\": status,\n                    \"inputs_preview\": inputs_preview if inputs_preview else \"N/A\",\n                    \"outputs_preview\": outputs_preview if outputs_preview else \"N/A\",\n                    \"reflection_status\": reflection.get('status', 'N/A'),\n                    \"reflection_confidence\": reflection.get('confidence', 'N/A'),\n                    \"reflection_issues\": reflection.get('potential_issues', None),\n                    \"error\": truncate_value(task_result.get('error', None)) # Also truncate error messages\n                }\n            # Print the summary dict as formatted JSON\n            print(json.dumps(summary, indent=2, default=str))\n        except Exception as summary_e:\n            print(f\"(Could not generate summary: {summary_e})\")\n            print(f\"Full results saved to {output_filename} (or fallback file).\")\n        print(\"---------------------------------------------\\n\")\n\n    except FileNotFoundError as e:\n        # Handle case where the specified workflow file doesn't exist\n        logger.error(f\"Workflow file '{workflow_to_run}' not found in '{config.WORKFLOW_DIR}': {e}\")\n        print(f\"ERROR: Workflow file '{workflow_to_run}' not found in '{config.WORKFLOW_DIR}'. Please check the filename and path.\")\n        sys.exit(1)\n    except (ValueError, TypeError) as setup_err:\n        # Handle errors likely related to configuration or workflow structure\n        logger.critical(f\"Workflow execution failed due to configuration or setup error: {setup_err}\", exc_info=True)\n        print(f\"ERROR: Workflow setup failed. Check configuration ({config.__file__}) and workflow structure ({workflow_to_run}). Error: {setup_err}\")\n        sys.exit(1)\n    except Exception as exec_err:\n        # Catch any other unexpected errors during workflow execution\n        logger.critical(f\"An critical unexpected error occurred during workflow execution: {exec_err}\", exc_info=True)\n        print(f\"ERROR: Workflow execution failed unexpectedly. Check logs at {config.LOG_FILE}. Error: {exec_err}\")\n        sys.exit(1)\n\n    logger.info(\"--- Arche System Shutdown ---\")\n\n# Ensure the package can be found if running the script directly\n# This might be less necessary when running with `python -m` but kept for broader compatibility\npackage_dir = os.path.dirname(__file__) # Directory of main.py (e.g., .../ResonantiA/3.0ArchE)\nproject_root = os.path.abspath(os.path.join(package_dir, '..')) # Project root (e.g., .../ResonantiA)\nif project_root not in sys.path:\n    sys.path.insert(0, project_root) # Add project root to Python path\n\n# --- Command Line Argument Parsing ---\nif __name__ == \"__main__\":\n    # Setup command-line argument parsing\n    parser = argparse.ArgumentParser(description=\"ResonantiA v3.0 Arche System - Main Executor\")\n    \n    # Using subparsers for different commands like 'run-workflow'\n    subparsers = parser.add_subparsers(dest=\"command\", required=True, help=\"Available commands\")\n    \n    # --- run-workflow Sub-parser ---\n    parser_run = subparsers.add_parser(\"run-workflow\", help=\"Execute a specified workflow.\")\n    parser_run.add_argument(\n        \"workflow_name\", \n        type=str,\n        help=\"The filename of the workflow to execute (e.g., 'quality_assurance_workflow.json'). Must be located in the configured WORKFLOW_DIR.\"\n    )\n    parser_run.add_argument(\n        \"--context-file\",\n        type=str,\n        default=None,\n        help=\"Path to a JSON file containing the initial context for the workflow run.\"\n    )\n    # Register the handler for the 'run-workflow' command\n    parser_run.set_defaults(func=handle_run_workflow)\n\n    # --- sirc-directive Sub-parser ---\n    parser_sirc = subparsers.add_parser(\"sirc-directive\", help=\"Process a strategic directive through SIRC intake handler with DirectiveClarificationProtocoL.\")\n    parser_sirc.add_argument(\n        \"directive\",\n        type=str,\n        help=\"The strategic directive to process (equivalent to keyy=('IMnDEVmode') directive input)\"\n    )\n    # Register the handler for the 'sirc-directive' command\n    parser_sirc.set_defaults(func=handle_sirc_directive)\n\n    # --- Other sub-parsers for different commands can be added here ---\n    # For example, a future 'manage-spr' command\n\n    args = parser.parse_args()\n\n    # Execute the function associated with the chosen command\n    if hasattr(args, 'func'):\n        args.func(args)\n    else:\n        parser.print_help()\n        sys.exit(1)\n\n# --- END OF FILE Three_PointO_ArchE/main.py --- \n```\n\nEXAMPLE APPLICATION:\nCreates necessary directories defined in config.py if they don't exist.\n\nCATEGORY: CodeKnowledge\n\nRELATIONSHIPS:\ntype: FromCodebase; source: /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/main.py; source_type: python_function"}