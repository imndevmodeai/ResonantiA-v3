{"content": "TERM: Class: SymbolCodexEntry\n\nDEFINITION:\nA single entry in the Symbol Codex.\n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/pattern_crystallization_engine.py, type: python_class\n\nIMPLEMENTATION CODE (pattern_crystallization_engine.py) - First 30KB:\n```python\n\"\"\"\nPattern Crystallization Engine\nThe Alchemical Distiller - Extracts pure SPRs from verbose ThoughtTrails\n\nThis module implements the complete crystallization process that transforms\nverbose narratives into hyper-dense symbolic SPRs (Zepto form), enabling\nmassive compression while preserving essential meaning.\n\nThe engine embodies Universal Abstraction in its purest form: the ability\nto represent complex concepts as symbols, compare and manipulate them\naccording to deterministic rules, and crystallize entire analyses into\nhyper-dense symbolic strings.\n\"\"\"\n\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass, asdict\nimport json\nfrom pathlib import Path\nimport datetime\nimport re\n\n# Import temporal core for canonical timestamps\ntry:\n    from .temporal_core import now_iso\nexcept ImportError:\n    def now_iso():\n        return datetime.datetime.utcnow().isoformat() + 'Z'\n\n# Import LLM tool for intelligent summarization and symbol generation\ntry:\n    from .llm_tool import generate_text_llm\n    from .llm_providers import get_llm_provider\n    LLM_AVAILABLE = True\nexcept ImportError:\n    LLM_AVAILABLE = False\n    generate_text_llm = None\n    get_llm_provider = None\n\n\n@dataclass\nclass CompressionStage:\n    \"\"\"Represents a single stage in the crystallization process.\"\"\"\n    stage_name: str  # \"Narrative\", \"Concise\", \"Nano\", \"Micro\", \"Pico\", \"Femto\", \"Atto\", \"Zepto\"\n    content: str\n    compression_ratio: float\n    symbol_count: int\n    timestamp: str\n\n\n@dataclass\nclass SymbolCodexEntry:\n    \"\"\"\n    Enhanced Symbol Codex Entry for preserving nuanced knowledge.\n    \n    Stores not just the meaning, but also:\n    - Original patterns/phrases that map to this symbol\n    - Relationships to other concepts\n    - Critical specifics that must be preserved\n    - Generalizable patterns\n    - Contextual variations\n    \"\"\"\n    symbol: str\n    meaning: str  # Core definition\n    context: str  # Domain/context\n    usage_examples: List[str]  # Example SPRs using this symbol\n    \n    # Enhanced fields for nuanced knowledge preservation\n    original_patterns: List[str] = None  # Original phrases/terms that compress to this symbol\n    relationships: Dict[str, str] = None  # Related symbols/concepts: {\"type\": \"related_symbol\"}\n    critical_specifics: List[str] = None  # Specific details that must be preserved (not generalized)\n    generalizable_patterns: List[str] = None  # Patterns that can be generalized\n    contextual_variations: Dict[str, str] = None  # Different meanings in different contexts\n    decompression_template: str = None  # Template for reconstructing nuanced knowledge\n    \n    created_at: str = None\n    \n    def __post_init__(self):\n        \"\"\"Initialize optional fields with defaults.\"\"\"\n        if self.original_patterns is None:\n            self.original_patterns = []\n        if self.relationships is None:\n            self.relationships = {}\n        if self.critical_specifics is None:\n            self.critical_specifics = []\n        if self.generalizable_patterns is None:\n            self.generalizable_patterns = []\n        if self.contextual_variations is None:\n            self.contextual_variations = {}\n        if self.decompression_template is None:\n            self.decompression_template = self.meaning\n\n\nclass PatternCrystallizationEngine:\n    \"\"\"\n    The Alchemical Distiller. Extracts pure SPRs from verbose ThoughtTrails.\n    \n    This engine implements the complete crystallization process:\n    1. Narrative Analysis\n    2. Progressive Compression (8 stages)\n    3. Symbol Codex Generation\n    4. SPR Integration\n    5. Decompression Validation\n    \"\"\"\n    \n    def __init__(self, symbol_codex_path: str = \"knowledge_graph/symbol_codex.json\", \n                 protocol_vocabulary_path: str = \"knowledge_graph/protocol_symbol_vocabulary.json\"):\n        \"\"\"Initialize the Crystallization Engine.\"\"\"\n        self.codex_path = Path(symbol_codex_path)\n        self.protocol_vocab_path = Path(protocol_vocabulary_path)\n        self.codex = self._load_codex()\n        self.protocol_vocab = self._load_protocol_vocabulary()\n        # Merge protocol vocabulary into main codex\n        self.codex.update(self.protocol_vocab)\n        self.compression_history: List[CompressionStage] = []\n        \n    def _load_codex(self) -> Dict[str, SymbolCodexEntry]:\n        \"\"\"Load the Symbol Codex from persistent storage.\n        \n        Handles concurrent access gracefully with retries for parallel processing.\n        \"\"\"\n        if not self.codex_path.exists():\n            return {}\n        \n        # Retry logic for concurrent file access (parallel processing)\n        max_retries = 3\n        retry_delay = 0.1  # 100ms\n        \n        for attempt in range(max_retries):\n            try:\n                # Use file locking for safe concurrent reads\n                import fcntl\n                with open(self.codex_path, 'r', encoding='utf-8') as f:\n                    try:\n                        fcntl.flock(f.fileno(), fcntl.LOCK_SH)  # Shared lock for reading\n                        data = json.load(f)\n                        fcntl.flock(f.fileno(), fcntl.LOCK_UN)  # Release lock\n                    except (OSError, AttributeError):\n                        # fcntl not available on Windows, fall back to regular read\n                        f.seek(0)\n                        data = json.load(f)\n                    \n                    return {\n                        symbol: SymbolCodexEntry(**entry)\n                        for symbol, entry in data.items()\n                    }\n            except (json.JSONDecodeError, ValueError) as e:\n                # JSON parse error - might be due to concurrent write or corruption\n                if attempt < max_retries - 1:\n                    import time\n                    time.sleep(retry_delay * (2 ** attempt))  # Exponential backoff\n                    continue\n                # Last attempt failed - return empty codex (will be rebuilt)\n                return {}\n            except Exception as e:\n                # Other errors (file not found, permission, etc.) - return empty\n                return {}\n        \n        return {}\n    \n    def _load_protocol_vocabulary(self) -> Dict[str, SymbolCodexEntry]:\n        \"\"\"Load protocol-specific symbol vocabulary.\n        \n        Handles concurrent access gracefully with retries for parallel processing.\n        \"\"\"\n        if not self.protocol_vocab_path.exists():\n            return {}\n        \n        # Retry logic for concurrent file access (parallel processing)\n        max_retries = 3\n        retry_delay = 0.1  # 100ms\n        \n        for attempt in range(max_retries):\n            try:\n                # Use file locking for safe concurrent reads\n                import fcntl\n                with open(self.protocol_vocab_path, 'r', encoding='utf-8') as f:\n                    try:\n                        fcntl.flock(f.fileno(), fcntl.LOCK_SH)  # Shared lock for reading\n                        data = json.load(f)\n                        fcntl.flock(f.fileno(), fcntl.LOCK_UN)  # Release lock\n                    except (OSError, AttributeError):\n                        # fcntl not available on Windows, fall back to regular read\n                        f.seek(0)\n                        data = json.load(f)\n                    \n                    vocab = {}\n                    # Flatten nested structure (protocol_core_symbols, mandate_symbols, etc.)\n                    for category, symbols in data.items():\n                        for symbol, entry in symbols.items():\n                            vocab[symbol] = SymbolCodexEntry(**entry)\n                    return vocab\n            except (json.JSONDecodeError, ValueError) as e:\n                # JSON parse error - might be due to concurrent write or corruption\n                if attempt < max_retries - 1:\n                    import time\n                    time.sleep(retry_delay * (2 ** attempt))  # Exponential backoff\n                    continue\n                # Last attempt failed - return empty vocab\n                return {}\n            except Exception as e:\n                # Other errors - return empty vocab\n                return {}\n        \n        return {}\n    \n    def _save_codex(self):\n        \"\"\"Save the Symbol Codex to persistent storage.\"\"\"\n        self.codex_path.parent.mkdir(parents=True, exist_ok=True)\n        with open(self.codex_path, 'w', encoding='utf-8') as f:\n            json.dump(\n                {\n                    symbol: asdict(entry)\n                    for symbol, entry in self.codex.items()\n                },\n                f,\n                indent=2,\n                ensure_ascii=False\n            )\n    \n    def distill_to_spr(\n        self,\n        thought_trail_entry: str,\n        target_stage: str = \"Zepto\"\n    ) -> Tuple[str, Dict[str, SymbolCodexEntry], List[CompressionStage]]:\n        \"\"\"\n        Performs the multi-stage distillation of a narrative into a symbolic SPR.\n        \n        Russian Doll Architecture: Creates nested layers of compression, each preserving\n        different levels of detail. All layers are stored for progressive retrieval.\n        \n        Args:\n            thought_trail_entry: The verbose narrative to compress\n            target_stage: The final compression stage (\"Concise\", \"Nano\", \"Micro\", \"Pico\", \"Femto\", \"Atto\", \"Zepto\")\n            \n        Returns:\n            Tuple of (pure_spr_string, new_codex_entries, compression_stages)\n            - pure_spr_string: The final Zepto SPR (innermost doll)\n            - new_codex_entries: Enhanced symbol codex with nuanced knowledge\n            - compression_stages: All compression layers (Russian dolls) for layered retrieval\n        \"\"\"\n        # Reset compression history for new run\n        self.compression_history = []\n        initial_len = len(thought_trail_entry)\n        \n        # Stage 0: Narrative layer (outermost Russian doll - complete original content)\n        self.compression_history.append(CompressionStage(\n            stage_name=\"Narrative\",\n            content=thought_trail_entry,\n            compression_ratio=1.0,  # No compression - complete preservation\n            symbol_count=len(thought_trail_entry),\n            timestamp=now_iso()\n        ))\n        \n        # Stage 1: Narrative to Concise Form (LLM-assisted summarization)\n        concise_form = self._summarize(thought_trail_entry)\n        self.compression_history.append(CompressionStage(\n            stage_name=\"Concise\",\n            content=concise_form,\n            compression_ratio=initial_len / len(concise_form) if concise_form else 1.0,\n            symbol_count=len(concise_form),\n            timestamp=now_iso()\n        ))\n        \n        # Stage 2-7: Progressive Symbolic Substitution\n        current_form = concise_form\n        stages = [\"Nano\", \"Micro\", \"Pico\", \"Femto\", \"Atto\"]\n        \n        for stage in stages:\n            if target_stage == stage:\n                break\n            current_form = self._symbolize(current_form, stage)\n            self.compression_history.append(CompressionStage(\n                stage_name=stage,\n                content=current_form,\n                compression_ratio=initial_len / len(current_form) if current_form else 1.0,\n                symbol_count=len(current_form),\n                timestamp=now_iso()\n            ))\n        \n        # Stage 8: Final Crystallization (Zepto)\n        if target_stage == \"Zepto\":\n            pure_spr = self._finalize_crystal(current_form)\n            self.compression_history.append(CompressionStage(\n                stage_name=\"Zepto\",\n                content=pure_spr,\n                compression_ratio=initial_len / len(pure_spr) if pure_spr else 1.0,\n                symbol_count=len(pure_spr),\n                timestamp=now_iso()\n            ))\n        else:\n            pure_spr = current_form\n        \n        # Stage 9: Generate/Update Codex\n        new_codex_entries = self._update_codex(pure_spr, thought_trail_entry)\n        \n        # Save updated codex\n        self._save_codex()\n        \n        # Return SPR, codex entries, AND compression stages (Russian dolls)\n        return pure_spr, new_codex_entries, self.compression_history.copy()\n    \n    def _summarize(self, narrative: str) -> str:\n        \"\"\"\n        Stage 1: Narrative to Concise Form - AGGRESSIVE MULTI-PASS COMPRESSION.\n        \n        Uses iterative LLM-assisted summarization to achieve 0.1-1% of original length.\n        Each pass becomes more aggressive, targeting deeper compression.\n        \"\"\"\n        if not LLM_AVAILABLE or not get_llm_provider:\n            # Fallback: Basic length reduction\n            return narrative[:max(len(narrative)//2, 100)]\n        \n        current_text = narrative\n        target_ratio = 0.01  # Target 1% of original (100:1 compression)\n        max_passes = 3  # Maximum number of compression passes\n        min_compression_per_pass = 0.5  # Each pass must compress by at least 50%\n        \n        for pass_num in range(max_passes):\n            try:\n                if get_llm_provider:\n                    provider = get_llm_provider(\"groq\")\n                    \n                    # Calculate target length for this pass\n                    current_length = len(current_text)\n                    target_length = max(int(current_length * target_ratio), 100)\n                    \n                    # Progressively more aggressive prompts\n                    if pass_num == 0:\n                        compression_target = \"10-20%\"\n                        detail_level = \"preserve all key concepts, principles, and technical details\"\n                    elif pass_num == 1:\n                        compression_target = \"5-10%\"\n                        detail_level = \"preserve only core concepts and essential relationships\"\n                    else:  # pass_num == 2\n                        compression_target = \"1-2%\"\n                        detail_level = \"preserve only the most critical information and relationships\"\n                    \n                    prompt = f\"\"\"You are the Pattern Crystallization Engine's aggressive summarization stage (Pass {pass_num + 1}/{max_passes}).\n\nYour task is to EXTREMELY compress the following narrative:\n1. Remove ALL allegorical language, descriptive prose, examples, and verbose explanations\n2. {detail_level}\n3. Use dense, technical language with minimal words\n4. Eliminate redundancy completely\n5. Use abbreviations and symbols where possible\n\nTarget: Reduce to {compression_target} of current length ({current_length} chars → target: {target_length} chars).\n\nCurrent text ({current_length} chars):\n{current_text[:8000]}{'...' if len(current_text) > 8000 else ''}\n\nCompressed form (dense, no redundancy, preserve critical logic only):\"\"\"\n                    \n                    compressed = provider.generate(\n                        prompt=prompt,\n                        model=\"llama-3.3-70b-versatile\",\n                        temperature=0.1,  # Very low for maximum compression\n                        max_tokens=min(target_length, 4000)  # Limit output tokens\n                    )\n                    \n                    if compressed and len(compressed.strip()) > 0:\n                        compressed = compressed.strip()\n                        # Only use if we got meaningful compression\n                        if len(compressed) < current_length * min_compression_per_pass:\n                            current_text = compressed\n                            print(f\"  Pass {pass_num + 1}: {current_length:,} → {len(current_text):,} chars ({current_length/len(current_text):.1f}:1)\")\n                            # If we've achieved target ratio, stop\n                            if len(current_text) <= target_length:\n                                break\n                        else:\n                            print(f\"  Pass {pass_num + 1}: Compression insufficient ({len(compressed):,} chars), stopping\")\n                            break\n                    else:\n                        print(f\"  Pass {pass_num + 1}: Empty response, stopping\")\n                        break\n            except Exception as e:\n                print(f\"Warning: LLM summarization pass {pass_num + 1} failed: {e}\")\n                if pass_num == 0:\n                    # Only fallback on first pass failure\n                    return narrative[:max(len(narrative)//2, 100)]\n                break\n        \n        return current_text\n    \n    def _symbolize(self, text: str, stage: str) -> str:\n        \"\"\"\n        Stage 2-7: Progressive Symbolic Substitution - LLM-FREE RULE-BASED COMPRESSION.\n        \n        Applies progressively more aggressive rule-based compression at each stage.\n        Each stage compresses more than the previous, creating true Russian Doll layers.\n        \"\"\"\n        import re\n        result = text\n        original_len = len(result)\n        \n        # Stage-specific compression aggressiveness\n        stage_levels = {\n            \"Nano\": 1,    # Light compression\n            \"Micro\": 2,   # Moderate compression\n            \"Pico\": 3,    # Aggressive compression\n            \"Femto\": 4,   # Very aggressive compression\n            \"Atto\": 5     # Maximum compression before Zepto\n        }\n        compression_level = stage_levels.get(stage, 1)\n        \n        # Pass 1: Direct symbol substitutions from codex (longest meaning first)\n        # Prioritize protocol vocabulary symbols\n        protocol_symbols = [(s, e) for s, e in self.codex.items() if s in self.protocol_vocab]\n        other_symbols = [(s, e) for s, e in self.codex.items() if s not in self.protocol_vocab]\n        sorted_symbols = protocol_symbols + other_symbols\n        \n        for symbol, entry in sorted_symbols:\n            # Try multiple phrase matching strategies\n            meaning_full = entry.meaning\n            meaning_short = meaning_full.split(' - ')[0].strip()\n            \n            # Strategy 1: Direct phrase matching (case-insensitive)\n            if meaning_short and len(meaning_short) > 3 and symbol:\n                try:\n                    escaped_meaning = re.escape(meaning_short)\n                    pattern = re.compile(escaped_meaning, re.IGNORECASE)\n                    escaped_symbol = symbol.replace('\\\\', '\\\\\\\\').replace('$', '\\\\$')\n                    result = pattern.sub(escaped_symbol, result)\n                except re.error:\n                    continue\n            \n            # Strategy 2: Key term matching (for higher compression levels)\n            if compression_level >= 2:\n                key_terms = meaning_short.split()\n                for term in key_terms:\n                    if len(term) > 4:  # Only substantial terms\n                        # Replace whole words that match key terms\n                        pattern = re.compile(r'\\b' + re.escape(term) + r'\\b', re.IGNORECASE)\n                        result = pattern.sub(symbol, result)\n        \n        # Pass 2: Aggressive symbol replacement (always applied)\n        result = self._apply_aggressive_symbol_replacement(result)\n        \n        # Pass 3: Progressive text compression based on stage level\n        if compression_level >= 2:\n            # Remove common connecting words\n            common_words = ['the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', \n                          'that', 'with', 'have', 'this', 'from', 'when', 'where', 'what', \n                          'which', 'will', 'would', 'could', 'should', 'may', 'might']\n            for word in common_words:\n                result = re.sub(r'\\b' + word + r'\\b', '', result, flags=re.IGNORECASE)\n            result = re.sub(r'\\s+', ' ', result).strip()\n        \n        if compression_level >= 3:\n            # Abbreviate common phrases\n            abbreviations = {\n                r'\\baction\\s+context\\b': 'AC',\n                r'\\bworkflow\\s+engine\\b': 'WE',\n                r'\\btask\\s+metadata\\b': 'TM',\n                r'\\bexecution\\s+state\\b': 'ES',\n                r'\\bruntime\\s+context\\b': 'RC',\n                r'\\bknowledge\\s+graph\\b': 'KG',\n                r'\\bsymbol\\s+codex\\b': 'SC',\n            }\n            for pattern, abbrev in abbreviations.items():\n                result = re.sub(pattern, abbrev, result, flags=re.IGNORECASE)\n        \n        if compression_level >= 4:\n            # Remove articles and prepositions\n            articles_preps = ['a', 'an', 'the', 'in', 'on', 'at', 'to', 'of', 'for', \n                            'with', 'by', 'from', 'as', 'is', 'was', 'be', 'been']\n            for word in articles_preps:\n                result = re.sub(r'\\b' + word + r'\\b', '', result, flags=re.IGNORECASE)\n            result = re.sub(r'\\s+', ' ', result).strip()\n        \n        if compression_level >= 5:\n            # Maximum compression: Keep only key terms and symbols\n            # Extract capitalized words and symbols, remove lowercase words\n            words = result.split()\n            key_terms = []\n            for word in words:\n                # Keep: symbols, capitalized words, abbreviations, numbers\n                if (any(c in word for c in 'ΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩÆ') or\n                    word[0].isupper() or \n                    word.isupper() or\n                    word.isdigit() or\n                    len(word) <= 2):  # Keep short abbreviations\n                    key_terms.append(word)\n            result = ' '.join(key_terms)\n            result = re.sub(r'\\s+', ' ', result).strip()\n        \n        # Ensure we actually compressed (if not, apply fallback compression)\n        if len(result) >= original_len * 0.95:  # Less than 5% compression\n            # Fallback: aggressive word removal\n            words = result.split()\n            # Keep only: symbols, capitalized words, words > 4 chars\n            filtered = [w for w in words if \n                       any(c in w for c in 'ΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩÆ') or\n                       w[0].isupper() or len(w) > 4]\n            result = ' '.join(filtered) if filtered else result\n            result = re.sub(r'\\s+', ' ', result).strip()\n        \n        return result\n    \n    def _apply_aggressive_symbol_replacement(self, text: str) -> str:\n        \"\"\"Apply aggressive rule-based symbol replacement for common patterns.\"\"\"\n        result = text\n        \n        # Common protocol phrase patterns (case-insensitive matching)\n        replacements = {\n            # Core concepts\n            \"cognitive resonance\": \"Ω\",\n            \"temporal resonance\": \"Δ\",\n            \"integrated action reflection\": \"Φ\",\n            \"iar\": \"Φ\",\n            \"sparse priming representation\": \"Θ\",\n            \"spr\": \"Θ\",\n            \"pattern crystallization\": \"Π\",\n            \"as above so below\": \"Λ\",\n            \"thought trail\": \"Σ\",\n            \"thoughttrail\": \"Σ\",\n            \"arche\": \"Æ\",\n            # System components\n            \"guardian points\": \"G\",\n            \"guardian pointS\": \"G\",\n            \"workflow engine\": \"W\",\n            \"knowledge graph\": \"K\",\n            \"symbol codex\": \"C\",\n            \"knowledge network oneness\": \"KnO\",\n            \"kno\": \"KnO\",\n            # Common technical terms\n            \"definition\": \"D\",\n            \"implementation\": \"I\",\n            \"preservation\": \"P\",\n            \"format\": \"F\",\n            \"mechanism\": \"M\",\n            \"system\": \"S\",\n            \"process\": \"P\",\n            \"protocol\": \"P\",\n            \"mandate\": \"M\",\n            # Action types\n            \"action reflection\": \"Φ\",\n            \"metacognitive shift\": \"MS\",\n            \"sirc\": \"SIRC\",\n            \"insight solidification\": \"IS\",\n        }\n        \n        # Apply replacements (case-insensitive)\n        import re\n        for phrase, symbol in replacements.items():\n            # Case-insensitive replacement\n            pattern = re.compile(re.escape(phrase), re.IGNORECASE)\n            result = pattern.sub(symbol, result)\n        \n        # Add mandate replacements (using Unicode subscripts)\n        mandate_subscripts = ['₁', '₂', '₃', '₄', '₅', '₆', '₇', '₈', '₉', '₁₀', '₁₁', '₁₂']\n        for i in range(1, 13):\n            subscript = mandate_subscripts[i-1]\n            result = re.sub(rf'\\b[Mm]andate\\s+{i}\\b', f'M{subscript}', result, flags=re.IGNORECASE)\n            result = re.sub(rf'\\bM{i}\\b', f'M{subscript}', result)\n        \n        # Remove common English words that shouldn't be in symbolic form\n        common_words = ['the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'that', 'with', 'have', 'this', 'from', 'when', 'where', 'what', 'which']\n        for word in common_words:\n            result = re.sub(rf'\\b{word}\\b', '', result, flags=re.IGNORECASE)\n        \n        # Clean up multiple spaces\n        result = re.sub(r'\\s+', ' ', result).strip()\n        \n        return result\n    \n    def _finalize_crystal(self, symbolic_form: str) -> str:\n        \"\"\"\n        Stage 8: Final Crystallization.\n        \n        Applies final optimizations to create the pure Zepto SPR:\n        - Removes remaining linguistic artifacts\n        - Converts remaining text to symbols\n        - Optimizes symbol density\n        - Validates symbolic integrity\n        \n        CRITICAL: This must produce actual symbolic output, not readable text.\n        \"\"\"\n        result = symbolic_form\n        \n        # Step 1: Remove whitespace\n        result = ''.join(result.split()).strip()\n        \n        # Step 2: If result is still readable text (contains lowercase letters, spaces, common words),\n        # apply aggressive symbolization to convert it to actual symbols\n        if len(result) > 50 or self._is_readable_text(result):\n            # Apply aggressive symbol replacement one more time\n            result = self._apply_aggressive_symbol_replacement(result)\n            \n            # If still readable, try to extract key concepts and create minimal symbolic representation\n            if self._is_readable_text(result) and len(result) > 20:\n                # Extract key symbols that are already present\n                existing_symbols = re.findall(r'[ΓΣΔΘΛΞΠΦΨΩΑΒΕΗΙΚΜΝΟΡΤΥΧÆ]', result)\n                \n                # Try to create a minimal symbolic representation\n                # Use first letters of key words as fallback symbols\n                words = re.findall(r'[A-Z][a-z]+', result)\n                if words and len(existing_symbols) == 0:\n                    # No existing symbols, create minimal representation from key concepts\n                    key_concepts = [w[0].upper() for w in words[:5]]\n                    result = '|'.join(key_concepts) if len(key_concepts) > 1 else key_concepts[0] if key_concepts else 'Ξ'\n                elif existing_symbols:\n                    # Use existing symbols, combine them\n                    result = '|'.join(existing_symbols[:5])\n                else:\n                    # Ultimate fallback: single symbol\n                    result = 'Ξ'\n        \n        # Step 3: Remove any remaining lowercase letters and common words\n        # Keep only symbols, uppercase abbreviations, and special characters\n        result = re.sub(r'[a-z]+', '', result)  # Remove lowercase words\n        result = re.sub(r'\\b(the|and|for|are|but|not|you|all|can|had|her|was|one|our|out|day|get|has|him|his|how)\\b', '', result, flags=re.IGNORECASE)\n        \n        # Step 4: Clean up and optimize\n        result = result.strip()\n        \n        # If result is empty or too long, create minimal symbolic representation\n        if not result or len(result) > 50:\n            # Extract any symbols present\n            symbols = re.findall(r'[ΓΣΔΘΛΞΠΦΨΩΑΒΕΗΙΚΜΝΟΡΤΥΧÆ|]', result)\n            if symbols:\n                result = '|'.join(symbols[:5])\n            else:\n                result = 'Ξ'  # Unknown symbol\n        \n        return result\n    \n    def _is_readable_text(self, text: str) -> bool:\n        \"\"\"Check if text is readable (contains lowercase letters, common words) rather than symbolic.\"\"\"\n        if not text:\n            return False\n        \n        # Count lowercase letters\n        lowercase_count = len([c for c in text if c.islower()])\n        total_alpha = len([c for c in text if c.isalpha()])\n        \n        # If more than 30% lowercase, it's readable text\n        if total_alpha > 0 and lowercase_count / total_alpha > 0.3:\n            return True\n        \n        # Check for common English words\n        common_words = ['the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 'can', 'that', 'with', 'have', 'this']\n        text_lower = text.lower()\n        word_count = sum(1 for word in common_words if word in text_lower)\n        \n        # If contains common words, it's readable\n        if word_count > 0:\n            return True\n        \n        return False\n    \n    def _update_codex(\n        self,\n        pure_spr: str,\n        original_narrative: str\n    ) -> Dict[str, SymbolCodexEntry]:\n        \"\"\"\n        Stage 9: Generate/Update Symbol Codex.\n        \n        Analyzes the pure SPR and original narrative to identify\n        new symbols and their meanings, updating the codex.\n        \"\"\"\n        new_entries = {}\n        \n        # Extract symbols from SPR\n        symbols = self._extract_symbols(pure_spr)\n        \n        # For each symbol, infer enhanced meaning from original narrative\n        for symbol in symbols:\n            if symbol not in self.codex:\n                meaning_data = self._infer_symbol_meaning(symbol, original_narrative)\n                entry = SymbolCodexEntry(\n                    symbol=symbol,\n                    meaning=meaning_data[\"definition\"],\n                    context=meaning_data[\"context\"],\n                    usage_examples=[pure_spr],\n                    original_patterns=meaning_data.get(\"original_patterns\", []),\n                    relationships=meaning_data.get(\"relationships\", {}),\n                    critical_specifics=meaning_data.get(\"critical_specifics\", []),\n                    generalizable_patterns=meaning_data.get(\"genera...\n```\n\nEXAMPLE APPLICATION:\nA single entry in the Symbol Codex.\n\nCATEGORY: CodeKnowledge\n\nRELATIONSHIPS:\ntype: FromCodebase; source: /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/pattern_crystallization_engine.py; source_type: python_class"}