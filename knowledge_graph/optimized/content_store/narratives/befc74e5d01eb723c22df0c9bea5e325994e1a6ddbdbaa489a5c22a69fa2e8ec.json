{"content": "TERM: Red Team Analysis\n\nDEFINITION:\nThe adversarial critique process that attempts to find every possible weakness, questionable assumption, and failure mode in a strategic plan. The Red Team acts as an intelligent adversary whose goal is to break the strategy, not support it. This mirrors cybersecurity red-team exercises where security experts attack their own systems to find vulnerabilities before malicious actors do. In ArchE, Red Team Analysis uses high-temperature (0.8) LLM generation to encourage creative adversarial thinking.\n\nBLUEPRINT DETAILS:\nUses temperature 0.8 to encourage creative failure mode identification. Prompts frame the LLM as a 'risk analysis expert' to avoid safety filter triggers while maintaining adversarial rigor.\n\nEXAMPLE APPLICATION:\nRed Team attacking a cloud migration strategy might identify: single-point-of-failure in DNS provider, vendor lock-in risks with proprietary APIs, skill gap in team for managing distributed systems, cost overrun risk if data egress exceeds projections.\n\nCATEGORY: AdversarialValidation\n\nRELATIONSHIPS:\ntype: AdversarialCritique; part_of: HighStakesVettinG; produces: RedTeamReporT; enables: VulnerabilityIdentificatioN; inspired_by: CybersecurityRedTeaminG; confidence: high"}