{"content": "TERM: Class: IARManager\n\nDEFINITION:\nClass: IARManager\n\nManages IAR (Integrated Action Reflection) data for workflow execution.\n\nMethods: __init__, process_iar\n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/workflow_chaining_engine.py, type: python_class\n\nFULL IMPLEMENTATION CODE (workflow_chaining_engine.py):\n```python\n\"\"\"\nWorkflow Chaining Engine - Advanced workflow orchestration with IAR integration\n\"\"\"\nfrom typing import Dict, Any, List, Optional, Set\nfrom datetime import datetime\n\n# ============================================================================\n# TEMPORAL CORE INTEGRATION (CANONICAL DATETIME SYSTEM)\n# ============================================================================\nfrom .temporal_core import now_iso, format_filename, format_log, Timer\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nimport networkx as nx\nfrom rich.console import Console\nfrom .workflow_engine import IARCompliantWorkflowEngine\nfrom .action_registry import main_action_registry\nfrom .output_handler import display_workflow_progress\n\nconsole = Console()\n\nclass WorkflowChainingEngine:\n    \"\"\"\n    Advanced workflow engine that handles complex workflow chaining with IAR integration,\n    parallel processing, and conditional execution.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the workflow chaining engine with its core components.\"\"\"\n        self.workflow_engine = IARCompliantWorkflowEngine()\n        self.action_registry = main_action_registry\n        self.execution_graph = nx.DiGraph()\n        self.execution_history = []\n        self.parallel_executor = ThreadPoolExecutor(max_workers=4)\n        self.iar_manager = IARManager()\n    \n    def _validate_workflow(self, workflow: Dict[str, Any]) -> bool:\n        \"\"\"Validate the structure of a workflow definition.\"\"\"\n        if not isinstance(workflow, dict):\n            console.print(\"[red]Workflow is not a dictionary.[/red]\")\n            return False\n        \n        required_keys = [\"name\", \"description\", \"version\", \"tasks\"]\n        for key in required_keys:\n            if key not in workflow:\n                console.print(f\"[red]Missing required workflow key: {key}[/red]\")\n                return False\n        \n        if not isinstance(workflow['tasks'], dict):\n            console.print(\"[red]'tasks' must be a dictionary.[/red]\")\n            return False\n            \n        for task_name, task_def in workflow['tasks'].items():\n            if not isinstance(task_def, dict):\n                console.print(f\"[red]Task definition for '{task_name}' is not a dictionary.[/red]\")\n                return False\n            \n            required_task_keys = [\"description\", \"action_type\", \"inputs\", \"dependencies\"]\n            for key in required_task_keys:\n                if key not in task_def:\n                    console.print(f\"[red]Task '{task_name}' is missing required key: {key}[/red]\")\n                    return False\n        \n        return True\n    \n    async def execute_workflow(self, workflow: Dict, initial_context: Optional[Dict] = None) -> Dict:\n        \"\"\"\n        Execute a workflow with complex chaining patterns.\n        \n        Args:\n            workflow: The workflow definition\n            initial_context: Optional initial context\n            \n        Returns:\n            Dict containing execution results and IAR data\n        \"\"\"\n        # Initialize execution context\n        context = initial_context or {}\n        context['workflow_start_time'] = now_iso()\n        \n        # Validate workflow before execution\n        if not self._validate_workflow(workflow):\n            raise ValueError(\"Invalid workflow definition\")\n        \n        # Build execution graph\n        self._build_execution_graph(workflow)\n        \n        # Execute tasks in topological order\n        results = {}\n        completed_tasks = set()\n        \n        try:\n            for task_name in nx.topological_sort(self.execution_graph):\n                if task_name in completed_tasks:\n                    continue\n                \n                # Check dependencies\n                if not self._check_dependencies(task_name, completed_tasks):\n                    continue\n                \n                # Execute task\n                task_result = await self._execute_task(workflow['tasks'][task_name], context, results)\n                \n                # Process IAR\n                iar_data = self.iar_manager.process_iar(task_name, task_result, context)\n                \n                # Store results\n                results[task_name] = {\n                    'result': task_result,\n                    'iar': iar_data,\n                    'timestamp': now_iso()\n                }\n                \n                completed_tasks.add(task_name)\n                \n                # Check for conditional execution\n                if 'condition' in workflow['tasks'][task_name]:\n                    if not self._evaluate_condition(workflow['tasks'][task_name]['condition'], context, results):\n                        continue\n                \n                # Handle parallel processing\n                if workflow['tasks'][task_name]['action_type'] == 'perform_parallel_action':\n                    await self._handle_parallel_processing(task_name, workflow['tasks'][task_name], context, results)\n                \n                # Handle metacognitive shift\n                if workflow['tasks'][task_name]['action_type'] == 'perform_metacognitive_shift':\n                    await self._handle_metacognitive_shift(task_name, workflow['tasks'][task_name], context, results)\n        \n        except Exception as e:\n            console.print(f\"[red]Error executing workflow: {str(e)}[/red]\")\n            raise\n        \n        finally:\n            # Record execution history\n            self.execution_history.append({\n                'workflow': workflow['name'],\n                'start_time': context['workflow_start_time'],\n                'end_time': now_iso(),\n                'results': results\n            })\n        \n        return results\n    \n    def _build_execution_graph(self, workflow: Dict[str, Any]) -> None:\n        \"\"\"Build the execution graph from workflow definition.\"\"\"\n        self.execution_graph.clear()\n        \n        for task_name, task in workflow['tasks'].items():\n            self.execution_graph.add_node(task_name)\n            \n            for dep in task.get('dependencies', []):\n                self.execution_graph.add_edge(dep, task_name)\n    \n    def _check_dependencies(self, task_name: str, completed_tasks: Set[str]) -> bool:\n        \"\"\"Check if all dependencies for a task are completed.\"\"\"\n        return all(dep in completed_tasks for dep in self.execution_graph.predecessors(task_name))\n    \n    async def _execute_task(self, task: Dict[str, Any], context: Dict[str, Any], results: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Execute a single task with IAR integration.\"\"\"\n        display_workflow_progress(task['description'], \"Starting\")\n        \n        try:\n            # Resolve inputs\n            resolved_inputs = self._resolve_inputs(task['inputs'], context, results)\n            \n            # Execute action\n            action_result = await self.action_registry.execute_action(\n                task['action_type'],\n                resolved_inputs,\n                context\n            )\n            \n            display_workflow_progress(task['description'], \"Completed\")\n            return action_result\n        \n        except Exception as e:\n            display_workflow_progress(task['description'], f\"Failed: {str(e)}\")\n            raise\n    \n    def _resolve_path(self, path: str, context: Dict[str, Any], results: Dict[str, Any]) -> Any:\n        \"\"\"Resolve a dot-notation path from context or results.\"\"\"\n        components = path.split('.')\n        \n        if components[0] == 'context':\n            current = context\n            path_components = components[1:]\n        elif components[0] in results:\n            current = results\n            path_components = components\n        else:\n            return None\n\n        for comp in path_components:\n            if isinstance(current, dict):\n                current = current.get(comp)\n            else:\n                return None\n        \n        return current\n\n    def _resolve_inputs(self, inputs: Dict[str, Any], context: Dict[str, Any], results: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Resolve input values from context and previous results.\"\"\"\n        resolved = {}\n        for key, value in inputs.items():\n            if isinstance(value, str) and value.startswith('{{') and value.endswith('}}'):\n                path = value[2:-2].strip()\n                resolved[key] = self._resolve_path(path, context, results)\n            else:\n                resolved[key] = value\n        return resolved\n    \n    def _evaluate_condition(self, condition: str, context: Dict[str, Any], results: Dict[str, Any]) -> bool:\n        \"\"\"Evaluate a condition string against results.\"\"\"\n        import re\n        try:\n            eval_condition = condition\n            placeholders = re.findall(r\"{{(.*?)}}\", condition)\n            \n            for placeholder in placeholders:\n                path = placeholder.strip()\n                resolved_value = self._resolve_path(path, context, results)\n                \n                # Use repr() to get a string representation that works with eval()\n                eval_condition = eval_condition.replace(f\"{{{{{placeholder}}}}}\", repr(resolved_value))\n            \n            return bool(eval(eval_condition))\n        except Exception as e:\n            console.print(f\"[yellow]Warning: Failed to evaluate condition: {str(e)}[/yellow]\")\n            return False\n    \n    async def _handle_parallel_processing(self, task_name: str, task: Dict[str, Any], context: Dict[str, Any], results: Dict[str, Any]) -> None:\n        \"\"\"Handle parallel processing tasks.\"\"\"\n        tasks = task['inputs']['tasks']\n        \n        # Create tasks for parallel execution\n        async def execute_parallel_task(task_def: Dict[str, Any]) -> Dict[str, Any]:\n            return await self.action_registry.execute_action(\n                task_def['action'],\n                task_def['data'],\n                context\n            )\n        \n        # Execute tasks in parallel\n        parallel_results = await asyncio.gather(\n            *[execute_parallel_task(task_def) for task_def in tasks]\n        )\n        \n        # Store results\n        results[task_name] = {\n            'parallel_results': dict(zip([t['name'] for t in tasks], parallel_results)),\n            'timestamp': now_iso()\n        }\n    \n    async def _handle_metacognitive_shift(self, task_name: str, task: Dict[str, Any], context: Dict[str, Any], results: Dict[str, Any]) -> None:\n        \"\"\"Handle metacognitive shift tasks.\"\"\"\n        shift_result = await self.action_registry.execute_action(\n            'perform_metacognitive_shift',\n            {\n                'context': task['inputs']['context'],\n                'threshold': task['inputs']['threshold']\n            },\n            context\n        )\n        \n        # Store shift results\n        results[task_name] = {\n            'shift_result': shift_result,\n            'timestamp': now_iso()\n        }\n\nclass IARManager:\n    \"\"\"Manages IAR (Integrated Action Reflection) data for workflow execution.\"\"\"\n    \n    def __init__(self):\n        self.iar_validator = IARValidator()\n        self.resonance_tracker = ResonanceTracker()\n    \n    def process_iar(self, task_id: str, result: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Process IAR data for a task execution.\"\"\"\n        # Validate IAR structure\n        is_valid, issues = self.iar_validator.validate_structure(result)\n        if not is_valid:\n            raise ValueError(f\"Invalid IAR structure: {issues}\")\n        \n        # Track resonance\n        self.resonance_tracker.record_execution(task_id, result, context)\n        \n        # Update context\n        if 'iar_history' not in context:\n            context['iar_history'] = []\n        context['iar_history'].append(result)\n        \n        return result\n\nclass IARValidator:\n    \"\"\"Validates the structure and content of IAR data.\"\"\"\n    \n    def validate_structure(self, iar_data: Dict[str, Any]) -> (bool, List[str]):\n        \"\"\"Validate the structure of IAR data.\"\"\"\n        issues = []\n        if not isinstance(iar_data, dict):\n            issues.append(\"IAR data is not a dictionary.\")\n            return False, issues\n        \n        required_keys = ['action_name', 'inputs', 'outputs', 'status', 'timestamp']\n        for key in required_keys:\n            if key not in iar_data:\n                issues.append(f\"Missing required key: {key}\")\n        \n        return not issues, issues\n\nclass ResonanceTracker:\n    \"\"\"Tracks the resonance of workflow execution.\"\"\"\n    \n    def __init__(self):\n        self.resonance_history = []\n    \n    def record_execution(self, task_id: str, result: Dict[str, Any], context: Dict[str, Any]) -> None:\n        \"\"\"Record the execution details for resonance tracking.\"\"\"\n        self.resonance_history.append({\n            'task_id': task_id,\n            'result': result,\n            'context': context,\n            'timestamp': now_iso()\n        }) \n```\n\nEXAMPLE APPLICATION:\nClass: IARManager\n\nManages IAR (Integrated Action Reflection) data for workflow execution.\n\nMethods: __init__, process_iar\n\nCATEGORY: CodeKnowledge\n\nRELATIONSHIPS:\ntype: FromCodebase; source: /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/workflow_chaining_engine.py; source_type: python_class"}