{"content": "TERM: Function: periodic_compression_worker\n\nDEFINITION:\nFunction: periodic_compression_worker\n\nBackground worker for periodic ThoughtTrail compression.\nThis should be run in a separate thread or asyncio task.\n\nParameters: \n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/semantic_archiver.py, type: python_function\n\nFULL IMPLEMENTATION CODE (semantic_archiver.py):\n```python\n\"\"\"\nSemanticArchiver Integration: Long-term Memory for ThoughtTrail\n==============================================================\n\nThe SemanticArchiver serves as ArchE's long-term memory system, compressing\nand storing ThoughtTrail entries for future analysis and retrieval.\n\nThis implementation provides:\n- Periodic compression of ThoughtTrail entries\n- Semantic indexing for efficient retrieval\n- Integration with ThoughtTrail for automatic archiving\n- Query capabilities for historical analysis\n\"\"\"\n\nimport json\nimport logging\nimport os\nimport time\nfrom datetime import datetime, timedelta\n\n# ============================================================================\n# TEMPORAL CORE INTEGRATION (CANONICAL DATETIME SYSTEM)\n# ============================================================================\nfrom .temporal_core import now_iso, format_filename, format_log, Timer\nfrom typing import Dict, List, Any, Optional\nfrom pathlib import Path\n\nfrom Three_PointO_ArchE.nexus_interface import NexusInterface\nfrom Three_PointO_ArchE.thought_trail import thought_trail, IAREntry\n\nlogger = logging.getLogger(__name__)\n\nclass SemanticArchiver:\n    \"\"\"\n    The Librarian of Time - ArchE's long-term memory system.\n    \n    Compresses and stores ThoughtTrail entries for future analysis,\n    ensuring that the system's complete history is preserved in a\n    queryable format while preventing memory overflow.\n    \"\"\"\n    \n    def __init__(self, archive_dir: str = \"archives\", compression_interval: int = 3600):\n        \"\"\"\n        Initialize the SemanticArchiver.\n        \n        Args:\n            archive_dir: Directory to store archived entries\n            compression_interval: Seconds between compression cycles\n        \"\"\"\n        self.archive_dir = Path(archive_dir)\n        self.archive_dir.mkdir(exist_ok=True)\n        self.compression_interval = compression_interval\n        self.last_compression = datetime.now()\n        self.nexus = None\n        \n        # Archive statistics\n        self.stats = {\n            'total_archived': 0,\n            'compression_cycles': 0,\n            'last_compression_time': None,\n            'archive_files': []\n        }\n        \n        logger.info(f\"[SemanticArchiver] Initialized with archive_dir={archive_dir}\")\n    \n    def inject_nexus(self, nexus_instance: NexusInterface) -> None:\n        \"\"\"Inject NexusInterface for event communication.\"\"\"\n        self.nexus = nexus_instance\n        logger.info(\"[SemanticArchiver] NexusInterface injected\")\n    \n    def compress_thoughttrail(self) -> Dict[str, Any]:\n        \"\"\"\n        Compress recent ThoughtTrail entries into semantic representations.\n        \n        This is the core archiving process that transforms raw IAR entries\n        into compressed, searchable semantic representations.\n        \n        Returns:\n            Dict containing compression results\n        \"\"\"\n        try:\n            # Get entries that haven't been archived yet\n            recent_entries = thought_trail.get_recent_entries(minutes=60)\n            \n            if not recent_entries:\n                logger.info(\"[SemanticArchiver] No new entries to compress\")\n                return {'status': 'no_entries', 'compressed': 0}\n            \n            # Create semantic representations\n            semantic_entries = []\n            for entry in recent_entries:\n                semantic_entry = self._create_semantic_representation(entry)\n                semantic_entries.append(semantic_entry)\n            \n            # Save to archive file\n            archive_file = self._save_to_archive(semantic_entries)\n            \n            # Update statistics\n            self.stats['total_archived'] += len(semantic_entries)\n            self.stats['compression_cycles'] += 1\n            self.stats['last_compression_time'] = now_iso()\n            self.stats['archive_files'].append(archive_file)\n            \n            # Publish compression event\n            if self.nexus:\n                self.nexus.publish(\"semantic_archiver_compression\", {\n                    'entries_compressed': len(semantic_entries),\n                    'archive_file': archive_file,\n                    'timestamp': now_iso()\n                })\n            \n            logger.info(f\"[SemanticArchiver] Compressed {len(semantic_entries)} entries to {archive_file}\")\n            \n            return {\n                'status': 'success',\n                'compressed': len(semantic_entries),\n                'archive_file': archive_file\n            }\n\n        except Exception as e:\n            logger.error(f\"[SemanticArchiver] Error during compression: {e}\")\n            return {'status': 'error', 'error': str(e)}\n    \n    def _create_semantic_representation(self, entry: IAREntry) -> Dict[str, Any]:\n        \"\"\"\n        Create a semantic representation of an IAR entry.\n        \n        This transforms the raw entry into a compressed, searchable format\n        that preserves the essential information while reducing storage size.\n        \"\"\"\n        # Extract key semantic elements\n        intention = entry.iar.get('intention', '')\n        action = entry.iar.get('action', '')\n        reflection = entry.iar.get('reflection', '')\n        \n        # Create semantic summary\n        semantic_summary = self._generate_semantic_summary(intention, action, reflection)\n        \n        # Extract key terms and concepts\n        key_terms = self._extract_key_terms(intention + ' ' + reflection)\n        \n        # Determine semantic category\n        category = self._categorize_entry(entry)\n        \n        return {\n            'original_task_id': entry.task_id,\n            'timestamp': entry.timestamp,\n            'action_type': entry.action_type,\n            'confidence': entry.confidence,\n            'semantic_summary': semantic_summary,\n            'key_terms': key_terms,\n            'category': category,\n            'intention': intention[:200],  # Truncate for storage\n            'reflection': reflection[:200],  # Truncate for storage\n            'metadata': {\n                'duration_ms': entry.metadata.get('duration_ms', 0),\n                'module': entry.metadata.get('module', 'unknown'),\n                'decorated': entry.metadata.get('decorated', False)\n            }\n        }\n    \n    def _generate_semantic_summary(self, intention: str, action: str, reflection: str) -> str:\n        \"\"\"Generate a semantic summary of the entry.\"\"\"\n        # Simple semantic summarization\n        if 'error' in reflection.lower():\n            return f\"Error in {action}: {reflection[:100]}\"\n        elif 'success' in reflection.lower():\n            return f\"Successful {action}: {intention[:100]}\"\n        else:\n            return f\"{action}: {intention[:100]}\"\n    \n    def _extract_key_terms(self, text: str) -> List[str]:\n        \"\"\"Extract key terms from text.\"\"\"\n        # Simple key term extraction\n        words = text.lower().split()\n        \n        # Filter out common words\n        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n        key_terms = [word for word in words if len(word) > 3 and word not in stop_words]\n        \n        # Return unique terms\n        return list(set(key_terms))[:10]  # Limit to 10 terms\n    \n    def _categorize_entry(self, entry: IAREntry) -> str:\n        \"\"\"Categorize the entry based on its characteristics.\"\"\"\n        action_type = entry.action_type\n        confidence = entry.confidence\n        reflection = entry.iar.get('reflection', '').lower()\n        \n        if 'error' in reflection:\n            return 'error'\n        elif confidence > 0.9:\n            return 'high_confidence'\n        elif confidence < 0.7:\n            return 'low_confidence'\n        elif 'execute' in action_type:\n            return 'execution'\n        elif 'generate' in action_type:\n            return 'generation'\n        elif 'search' in action_type:\n            return 'search'\n        else:\n            return 'general'\n    \n    def _save_to_archive(self, semantic_entries: List[Dict[str, Any]]) -> str:\n        \"\"\"Save semantic entries to archive file.\"\"\"\n        timestamp = format_filename()\n        archive_file = self.archive_dir / f\"thoughttrail_archive_{timestamp}.json\"\n        \n        archive_data = {\n            'metadata': {\n                'created_at': now_iso(),\n                'entries_count': len(semantic_entries),\n                'compression_version': '1.0'\n            },\n            'entries': semantic_entries\n        }\n        \n        with open(archive_file, 'w') as f:\n            json.dump(archive_data, f, indent=2)\n        \n        return str(archive_file)\n    \n    def query_archives(self, criteria: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"\n        Query archived entries based on criteria.\n        \n        Args:\n            criteria: Query criteria (category, key_terms, date_range, etc.)\n            \n        Returns:\n            List of matching archived entries\n        \"\"\"\n        try:\n            results = []\n            \n            # Search through all archive files\n            for archive_file in self.archive_dir.glob(\"thoughttrail_archive_*.json\"):\n                with open(archive_file, 'r') as f:\n                    archive_data = json.load(f)\n                \n                for entry in archive_data.get('entries', []):\n                    if self._matches_archive_criteria(entry, criteria):\n                        results.append(entry)\n            \n            logger.info(f\"[SemanticArchiver] Query returned {len(results)} results\")\n            return results\n\n        except Exception as e:\n            logger.error(f\"[SemanticArchiver] Error querying archives: {e}\")\n            return []\n    \n    def _matches_archive_criteria(self, entry: Dict[str, Any], criteria: Dict[str, Any]) -> bool:\n        \"\"\"Check if archived entry matches query criteria.\"\"\"\n        for key, value in criteria.items():\n            if key == 'category' and entry.get('category') != value:\n                return False\n            elif key == 'action_type' and entry.get('action_type') != value:\n                return False\n            elif key == 'key_terms':\n                if isinstance(value, list):\n                    if not any(term in entry.get('key_terms', []) for term in value):\n                        return False\n                elif value not in entry.get('key_terms', []):\n                    return False\n            elif key == 'date_range':\n                entry_time = datetime.fromisoformat(entry.get('timestamp', ''))\n                start_time = datetime.fromisoformat(value.get('start', '1970-01-01'))\n                end_time = datetime.fromisoformat(value.get('end', '2100-01-01'))\n                if not (start_time <= entry_time <= end_time):\n                    return False\n            elif key == 'confidence_range':\n                confidence = entry.get('confidence', 0)\n                min_conf = value.get('min', 0)\n                max_conf = value.get('max', 1)\n                if not (min_conf <= confidence <= max_conf):\n                    return False\n        \n        return True\n    \n    def get_archive_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive archive statistics.\"\"\"\n        archive_files = list(self.archive_dir.glob(\"thoughttrail_archive_*.json\"))\n        \n        total_entries = 0\n        categories = {}\n        action_types = {}\n        \n        for archive_file in archive_files:\n            try:\n                with open(archive_file, 'r') as f:\n                    archive_data = json.load(f)\n                \n                entries = archive_data.get('entries', [])\n                total_entries += len(entries)\n                \n                for entry in entries:\n                    category = entry.get('category', 'unknown')\n                    action_type = entry.get('action_type', 'unknown')\n                    \n                    categories[category] = categories.get(category, 0) + 1\n                    action_types[action_type] = action_types.get(action_type, 0) + 1\n                    \n            except Exception as e:\n                logger.error(f\"Error reading archive file {archive_file}: {e}\")\n        \n        return {\n            'total_archived_entries': total_entries,\n            'archive_files_count': len(archive_files),\n            'categories': categories,\n            'action_types': action_types,\n            'compression_stats': self.stats,\n            'archive_directory': str(self.archive_dir)\n        }\n    \n    def should_compress(self) -> bool:\n        \"\"\"Check if it's time to compress ThoughtTrail entries.\"\"\"\n        now = datetime.now()\n        time_since_last = (now - self.last_compression).total_seconds()\n        return time_since_last >= self.compression_interval\n    \n    def update_compression_time(self) -> None:\n        \"\"\"Update the last compression time.\"\"\"\n        self.last_compression = datetime.now()\n\n# Global SemanticArchiver instance\nsemantic_archiver = SemanticArchiver()\n\ndef initialize_semantic_archiver(nexus_instance: NexusInterface) -> None:\n    \"\"\"\n    Initialize SemanticArchiver integration.\n    \n    Args:\n        nexus_instance: The NexusInterface instance\n    \"\"\"\n    semantic_archiver.inject_nexus(nexus_instance)\n    logger.info(\"[SemanticArchiver] Integration initialized\")\n\ndef periodic_compression_worker():\n    \"\"\"\n    Background worker for periodic ThoughtTrail compression.\n    This should be run in a separate thread or asyncio task.\n    \"\"\"\n    while True:\n        try:\n            if semantic_archiver.should_compress():\n                result = semantic_archiver.compress_thoughttrail()\n                semantic_archiver.update_compression_time()\n                \n                if result['status'] == 'success':\n                    logger.info(f\"[SemanticArchiver] Periodic compression completed: {result['compressed']} entries\")\n                else:\n                    logger.warning(f\"[SemanticArchiver] Periodic compression failed: {result}\")\n            \n            # Sleep for 5 minutes before checking again\n            time.sleep(300)\n            \n        except Exception as e:\n            logger.error(f\"[SemanticArchiver] Error in compression worker: {e}\")\n            time.sleep(60)  # Wait 1 minute before retrying\n\n__all__ = [\n    'SemanticArchiver',\n    'semantic_archiver',\n    'initialize_semantic_archiver',\n    'periodic_compression_worker'\n]\n```\n\nEXAMPLE APPLICATION:\nFunction: periodic_compression_worker\n\nBackground worker for periodic ThoughtTrail compression.\nThis should be run in a separate thread or asyncio task.\n\nParameters: \n\nCATEGORY: CodeKnowledge\n\nRELATIONSHIPS:\ntype: FromCodebase; source: /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/semantic_archiver.py; source_type: python_function"}