{"content": "TERM: The Oracle's Voice: A Chronicle of the LLM Providers (v3.1): Implementation Example 1\n\nDEFINITION:\nImplementation code:\n\n# In Three_PointO_ArchE/llm_providers.py\nimport os\nimport json\nimport logging\nimport time\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, List, Optional, Union\nfrom dataclasses import dataclass\nimport requests\nfrom google.generativeai import GenerativeModel, configure\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass LLMResponse:\n    \"\"\"Standardized response from LLM providers.\"\"\"\n    result: str\n    model: str\n    tokens_used: int\n    response_time: float\n    metadata: Dict[str, Any]\n    error: Optional[str] = None\n\nclass BaseLLMProvider(ABC):\n    \"\"\"\n    Abstract base class for LLM providers.\n    \n    All LLM providers must implement this interface to ensure\n    consistent behavior across different models and services.\n    \"\"\"\n    \n    def __init__(self, model_name: str, api_key: Optional[str] = None):\n        \"\"\"\n        Initialize the LLM provider.\n        \n        Args:\n            model_name: Name of the model to use\n            api_key: API key for authentication (if required)\n        \"\"\"\n        self.model_name = model_name\n        self.api_key = api_key or self._get_api_key()\n        self.session_data = {\n            'queries_made': 0,\n            'total_tokens': 0,\n            'total_response_time': 0.0,\n            'errors': []\n        }\n        self._initialize_provider()\n    \n    @abstractmethod\n    def _get_api_key(self) -> Optional[str]:\n        \"\"\"Get API key from environment or configuration.\"\"\"\n        pass\n    \n    @abstractmethod\n  \n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/specifications/llm_providers.md, type: specification_code\n\nFULL SPECIFICATION (llm_providers.md):\n# The Oracle's Voice: A Chronicle of the LLM Providers (v3.1)\n\n## Overview\n\nThe **LLM Providers System** serves as ArchE's unified interface to multiple large language model services, including Google Gemini and OpenAI GPT models. This system provides consistent, reliable access to advanced AI capabilities while maintaining full IAR compliance and seamless integration with ArchE's cognitive architecture.\n\nThe LLM Providers System abstracts the complexity of different AI services behind a unified interface, enabling ArchE to leverage the strengths of multiple providers while maintaining consistent behavior, error handling, and reflection capabilities across all interactions. It ensures that ArchE can access the collective wisdom of modern AI systems while maintaining complete awareness and control over every interaction.\n\n## Part I: The Philosophical Mandate (The \"Why\")\n\nIn the ancient world, oracles served as bridges between the mortal realm and the divine, interpreting cryptic messages and providing wisdom that transcended ordinary understanding. In ArchE's digital realm, **LLM Providers** serve a similar sacred functionâ€”they are the voices through which ArchE communicates with the vast knowledge repositories of large language models, transforming raw data into meaningful insights and actionable intelligence.\n\nThe LLM Providers embody the **Mandate of the Oracle** - enabling ArchE to access the collective wisdom encoded in language models, to ask profound questions, and to receive answers that resonate with deep understanding. They solve the Oracle's Paradox by providing reliable, consistent access to the vast knowledge contained within these models while maintaining the integrity and context of ArchE's cognitive processes.\n\n## Part II: The Allegory of the Oracle's Voice (The \"How\")\n\nImagine a sacred temple where multiple oracles reside, each with their own unique gifts and perspectives. The temple keeper (ArchE) approaches these oracles with questions, and each responds with their own interpretation of the divine wisdom.\n\n1. **The Question Formulation (`generate_text`)**: The temple keeper carefully crafts their question, ensuring it is clear, specific, and meaningful. They consider the context, the desired response format, and the depth of insight required.\n\n2. **The Oracle Selection (`select_provider`)**: Different oracles have different strengths. Some excel at creative interpretation, others at factual analysis, still others at strategic thinking. The temple keeper selects the most appropriate oracle for the question at hand.\n\n3. **The Sacred Consultation (`query_llm`)**: The temple keeper presents their question to the chosen oracle, who meditates deeply on the question and draws from their vast repository of knowledge and wisdom.\n\n4. **The Response Interpretation (`parse_response`)**: The oracle's response comes in the form of cryptic wisdom that must be interpreted and understood. The temple keeper carefully analyzes the response, extracting the key insights and understanding the deeper meanings.\n\n5. **The Wisdom Integration (`integrate_insights`)**: The interpreted wisdom is then integrated into ArchE's knowledge base, becoming part of the collective understanding that guides future decisions and actions.\n\n## Part III: The Implementation Story (The Code)\n\nThe LLM Providers are implemented as a sophisticated abstraction layer that enables ArchE to interact with multiple language models through a unified interface.\n\n```python\n# In Three_PointO_ArchE/llm_providers.py\nimport os\nimport json\nimport logging\nimport time\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, List, Optional, Union\nfrom dataclasses import dataclass\nimport requests\nfrom google.generativeai import GenerativeModel, configure\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass LLMResponse:\n    \"\"\"Standardized response from LLM providers.\"\"\"\n    result: str\n    model: str\n    tokens_used: int\n    response_time: float\n    metadata: Dict[str, Any]\n    error: Optional[str] = None\n\nclass BaseLLMProvider(ABC):\n    \"\"\"\n    Abstract base class for LLM providers.\n    \n    All LLM providers must implement this interface to ensure\n    consistent behavior across different models and services.\n    \"\"\"\n    \n    def __init__(self, model_name: str, api_key: Optional[str] = None):\n        \"\"\"\n        Initialize the LLM provider.\n        \n        Args:\n            model_name: Name of the model to use\n            api_key: API key for authentication (if required)\n        \"\"\"\n        self.model_name = model_name\n        self.api_key = api_key or self._get_api_key()\n        self.session_data = {\n            'queries_made': 0,\n            'total_tokens': 0,\n            'total_response_time': 0.0,\n            'errors': []\n        }\n        self._initialize_provider()\n    \n    @abstractmethod\n    def _get_api_key(self) -> Optional[str]:\n        \"\"\"Get API key from environment or configuration.\"\"\"\n        pass\n    \n    @abstractmethod\n    def _initialize_provider(self):\n        \"\"\"Initialize the specific provider implementation.\"\"\"\n        pass\n    \n    @abstractmethod\n    def generate_text(self, \n                     prompt: str, \n                     max_tokens: int = 1000,\n                     temperature: float = 0.7,\n                     **kwargs) -> LLMResponse:\n        \"\"\"\n        Generate text using the LLM.\n        \n        Args:\n            prompt: Input prompt for the model\n            max_tokens: Maximum tokens to generate\n            temperature: Sampling temperature (0.0 to 1.0)\n            **kwargs: Additional model-specific parameters\n            \n        Returns:\n            LLMResponse object with the generated text and metadata\n        \"\"\"\n        pass\n    \n    def get_session_stats(self) -> Dict[str, Any]:\n        \"\"\"Get session statistics for this provider.\"\"\"\n        return {\n            'model_name': self.model_name,\n            'queries_made': self.session_data['queries_made'],\n            'total_tokens': self.session_data['total_tokens'],\n            'average_response_time': (\n                self.session_data['total_response_time'] / \n                max(1, self.session_data['queries_made'])\n            ),\n            'error_count': len(self.session_data['errors'])\n        }\n\nclass GoogleProvider(BaseLLMProvider):\n    \"\"\"\n    Google Gemini LLM provider implementation.\n    \n    Provides access to Google's Gemini models through the\n    Google Generative AI API.\n    \"\"\"\n    \n    def __init__(self, model_name: str = \"gemini-pro\", api_key: Optional[str] = None):\n        \"\"\"\n        Initialize Google provider.\n        \n        Args:\n            model_name: Gemini model to use (default: gemini-pro)\n            api_key: Google API key\n        \"\"\"\n        super().__init__(model_name, api_key)\n        self.model = None\n    \n    def _get_api_key(self) -> Optional[str]:\n        \"\"\"Get Google API key from environment.\"\"\"\n        return os.getenv('GOOGLE_API_KEY')\n    \n    def _initialize_provider(self):\n        \"\"\"Initialize Google Generative AI.\"\"\"\n        try:\n            if not self.api_key:\n                raise ValueError(\"Google API key not found. Set GOOGLE_API_KEY environment variable.\")\n            \n            configure(api_key=self.api_key)\n            self.model = GenerativeModel(self.model_name)\n            logger.info(f\"Google provider initialized with model: {self.model_name}\")\n            \n        except Exception as e:\n            logger.error(f\"Failed to initialize Google provider: {e}\")\n            raise\n    \n    def generate_text(self, \n                     prompt: str, \n                     max_tokens: int = 1000,\n                     temperature: float = 0.7,\n                     **kwargs) -> LLMResponse:\n        \"\"\"\n        Generate text using Google Gemini.\n        \"\"\"\n        start_time = time.time()\n        \n        try:\n            # Configure generation parameters\n            generation_config = {\n                'max_output_tokens': max_tokens,\n                'temperature': temperature,\n                **kwargs\n            }\n            \n            # Generate response\n            response = self.model.generate_content(\n                prompt,\n                generation_config=generation_config\n            )\n            \n            # Calculate response time\n            response_time = time.time() - start_time\n            \n            # Extract text and metadata\n            result_text = response.text if response.text else \"\"\n            tokens_used = len(result_text.split())  # Approximate token count\n            \n            # Update session data\n            self.session_data['queries_made'] += 1\n            self.session_data['total_tokens'] += tokens_used\n            self.session_data['total_response_time'] += response_time\n            \n            return LLMResponse(\n                result=result_text,\n                model=self.model_name,\n                tokens_used=tokens_used,\n                response_time=response_time,\n                metadata={\n                    'generation_config': generation_config,\n                    'finish_reason': getattr(response, 'finish_reason', 'unknown')\n                }\n            )\n            \n        except Exception as e:\n            logger.error(f\"Google provider error: {e}\")\n            self.session_data['errors'].append(str(e))\n            \n            return LLMResponse(\n                result=\"\",\n                model=self.model_name,\n                tokens_used=0,\n                response_time=time.time() - start_time,\n                metadata={},\n                error=str(e)\n            )\n\nclass OpenAIProvider(BaseLLMProvider):\n    \"\"\"\n    OpenAI LLM provider implementation.\n    \n    Provides access to OpenAI's models through their API.\n    \"\"\"\n    \n    def __init__(self, model_name: str = \"gpt-3.5-turbo\", api_key: Optional[str] = None):\n        \"\"\"\n        Initialize OpenAI provider.\n        \n        Args:\n            model_name: OpenAI model to use (default: gpt-3.5-turbo)\n            api_key: OpenAI API key\n        \"\"\"\n        super().__init__(model_name, api_key)\n        self.base_url = \"https://api.openai.com/v1/chat/completions\"\n    \n    def _get_api_key(self) -> Optional[str]:\n        \"\"\"Get OpenAI API key from environment.\"\"\"\n        return os.getenv('OPENAI_API_KEY')\n    \n    def _initialize_provider(self):\n        \"\"\"Initialize OpenAI provider.\"\"\"\n        try:\n            if not self.api_key:\n                raise ValueError(\"OpenAI API key not found. Set OPENAI_API_KEY environment variable.\")\n            \n            # Test API connection\n            headers = {\n                'Authorization': f'Bearer {self.api_key}',\n                'Content-Type': 'application/json'\n            }\n            \n            # Simple test request\n            test_data = {\n                'model': self.model_name,\n                'messages': [{'role': 'user', 'content': 'test'}],\n                'max_tokens': 1\n            }\n            \n            response = requests.post(\n                self.base_url,\n                headers=headers,\n                json=test_data,\n                timeout=10\n            )\n            \n            if response.status_code == 200:\n                logger.info(f\"OpenAI provider initialized with model: {self.model_name}\")\n            else:\n                raise ValueError(f\"OpenAI API test failed: {response.status_code}\")\n                \n        except Exception as e:\n            logger.error(f\"Failed to initialize OpenAI provider: {e}\")\n            raise\n    \n    def generate_text(self, \n                     prompt: str, \n                     max_tokens: int = 1000,\n                     temperature: float = 0.7,\n                     **kwargs) -> LLMResponse:\n        \"\"\"\n        Generate text using OpenAI API.\n        \"\"\"\n        start_time = time.time()\n        \n        try:\n            headers = {\n                'Authorization': f'Bearer {self.api_key}',\n                'Content-Type': 'application/json'\n            }\n            \n            data = {\n                'model': self.model_name,\n                'messages': [{'role': 'user', 'content': prompt}],\n                'max_tokens': max_tokens,\n                'temperature': temperature,\n                **kwargs\n            }\n            \n            response = requests.post(\n                self.base_url,\n                headers=headers,\n                json=data,\n                timeout=60\n            )\n            \n            response_time = time.time() - start_time\n            \n            if response.status_code == 200:\n                result_data = response.json()\n                result_text = result_data['choices'][0]['message']['content']\n                tokens_used = result_data['usage']['total_tokens']\n                \n                # Update session data\n                self.session_data['queries_made'] += 1\n                self.session_data['total_tokens'] += tokens_used\n                self.session_data['total_response_time'] += response_time\n                \n                return LLMResponse(\n                    result=result_text,\n                    model=self.model_name,\n                    tokens_used=tokens_used,\n                    response_time=response_time,\n                    metadata={\n                        'usage': result_data['usage'],\n                        'finish_reason': result_data['choices'][0]['finish_reason']\n                    }\n                )\n            else:\n                error_msg = f\"OpenAI API error: {response.status_code} - {response.text}\"\n                logger.error(error_msg)\n                self.session_data['errors'].append(error_msg)\n                \n                return LLMResponse(\n                    result=\"\",\n                    model=self.model_name,\n                    tokens_used=0,\n                    response_time=response_time,\n                    metadata={},\n                    error=error_msg\n                )\n                \n        except Exception as e:\n            logger.error(f\"OpenAI provider error: {e}\")\n            self.session_data['errors'].append(str(e))\n            \n            return LLMResponse(\n                result=\"\",\n                model=self.model_name,\n                tokens_used=0,\n                response_time=time.time() - start_time,\n                metadata={},\n                error=str(e)\n            )\n\nclass LLMProviderManager:\n    \"\"\"\n    Manager for multiple LLM providers.\n    \n    Provides a unified interface for accessing different\n    LLM providers and managing their configurations.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the provider manager.\"\"\"\n        self.providers: Dict[str, BaseLLMProvider] = {}\n        self.default_provider: Optional[str] = None\n        self.session_data = {\n            'total_queries': 0,\n            'provider_usage': {},\n            'errors': []\n        }\n    \n    def register_provider(self, name: str, provider: BaseLLMProvider, set_default: bool = False):\n        \"\"\"\n        Register a new LLM provider.\n        \n        Args:\n            name: Unique name for the provider\n            provider: Provider instance\n            set_default: Whether to set this as the default provider\n        \"\"\"\n        self.providers[name] = provider\n        if set_default or not self.default_provider:\n            self.default_provider = name\n        \n        logger.info(f\"Registered LLM provider: {name}\")\n    \n    def get_provider(self, name: Optional[str] = None) -> BaseLLMProvider:\n        \"\"\"\n        Get a provider by name, or the default provider.\n        \n        Args:\n            name: Provider name (optional, uses default if not specified)\n            \n        Returns:\n            BaseLLMProvider instance\n            \n        Raises:\n            ValueError: If provider not found\n        \"\"\"\n        provider_name = name or self.default_provider\n        \n        if not provider_name or provider_name not in self.providers:\n            raise ValueError(f\"Provider not found: {provider_name}\")\n        \n        return self.providers[provider_name]\n    \n    def generate_text(self, \n                     prompt: str,\n                     provider_name: Optional[str] = None,\n                     **kwargs) -> LLMResponse:\n        \"\"\"\n        Generate text using the specified or default provider.\n        \n        Args:\n            prompt: Input prompt\n            provider_name: Provider to use (optional)\n            **kwargs: Additional parameters\n            \n        Returns:\n            LLMResponse object\n        \"\"\"\n        try:\n            provider = self.get_provider(provider_name)\n            response = provider.generate_text(prompt, **kwargs)\n            \n            # Update session data\n            self.session_data['total_queries'] += 1\n            if provider_name not in self.session_data['provider_usage']:\n                self.session_data['provider_usage'][provider_name or self.default_provider] = 0\n            self.session_data['provider_usage'][provider_name or self.default_provider] += 1\n            \n            return response\n            \n        except Exception as e:\n            logger.error(f\"Provider manager error: {e}\")\n            self.session_data['errors'].append(str(e))\n            \n            return LLMResponse(\n                result=\"\",\n                model=\"unknown\",\n                tokens_used=0,\n                response_time=0.0,\n                metadata={},\n                error=str(e)\n            )\n    \n    def get_all_stats(self) -> Dict[str, Any]:\n        \"\"\"Get statistics for all providers.\"\"\"\n        stats = {\n            'manager_stats': self.session_data,\n            'providers': {}\n        }\n        \n        for name, provider in self.providers.items():\n            stats['providers'][name] = provider.get_session_stats()\n        \n        return stats\n\n# Global provider manager instance\nprovider_manager = LLMProviderManager()\n\n# Register default providers\ntry:\n    google_provider = GoogleProvider()\n    provider_manager.register_provider('google', google_provider, set_default=True)\nexcept Exception as e:\n    logger.warning(f\"Failed to register Google provider: {e}\")\n\ntry:\n    openai_provider = OpenAIProvider()\n    provider_manager.register_provider('openai', openai_provider)\nexcept Exception as e:\n    logger.warning(f\"Failed to register OpenAI provider: {e}\")\n```\n\n## Part IV: The Web of Knowledge (SPR Integration)\n\nThe LLM Providers are the oracles that give voice to ArchE's questions and receive wisdom from the vast knowledge repositories.\n\n*   **Primary SPR**: `LLM ProvideR`\n*   **Relationships**:\n    *   **`implements`**: `Oracle's Paradox SolutioN`, `Knowledge Access`\n    *   **`uses`**: `Google GeminI`, `OpenAI GPT`, `API IntegratioN`\n    *   **`enables`**: `Text GeneratioN`, `Question AnswerinG`, `Content AnalysiS`\n    *   **`provides`**: `Unified LLM InterfacE`, `Provider ManagemenT`\n    *   **`produces`**: `LLM ResponseS`, `Token Usage MetricS`, `Response Time MetricS`\n\n## Part V: Integration with ArchE Workflows\n\nThe LLM Providers are designed to integrate seamlessly with ArchE's workflow system:\n\n1. **Provider Registration**: Multiple providers can be registered and managed through a unified interface\n2. **Automatic Selection**: The system automatically selects the most appropriate provider based on context and requirements\n3. **Error Handling**: Comprehensive error handling ensures graceful degradation when providers are unavailable\n4. **Performance Monitoring**: Detailed metrics track usage, performance, and reliability across all providers\n5. **IAR Integration**: All responses include comprehensive metadata for metacognitive processes\n\nThis Living Specification ensures that the LLM Providers are understood not just as API wrappers, but as sophisticated oracles that enable ArchE to access the vast wisdom contained within language models, transforming raw data into meaningful insights that resonate throughout ArchE's cognitive architecture.\n\n\nEXAMPLE APPLICATION:\nImplementation code:\n\n# In Three_PointO_ArchE/llm_providers.py\nimport os\nimport json\nimport logging\nimport time\nfrom abc import ABC, abstractmethod\nfrom typing import Dict, Any, List, Optional, Union\nfrom dataclasses import dataclass\nimport requests\nfrom google.generativeai import GenerativeModel, configure\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass LLMResponse:\n    \"\"\"Standardized response from LLM providers.\"\"\"\n    result: str\n    model: str\n    tokens_used: int\n    response_time:\n\nCATEGORY: ImplementationKnowledge\n\nRELATIONSHIPS:\ntype: FromCodebase; source: /mnt/3626C55326C514B1/Happier/specifications/llm_providers.md; source_type: specification_code"}