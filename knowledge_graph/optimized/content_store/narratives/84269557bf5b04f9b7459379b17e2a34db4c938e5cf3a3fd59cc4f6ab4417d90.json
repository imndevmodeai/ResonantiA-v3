{"content": "TERM: Vetting Agent - Living Specification: Comprehensive Vetting Output\n\nDEFINITION:\n```python\ndef format_vetting_prompt(\n    objective: str,\n    previous_result: Any,\n    current_thought: str,\n    current_action: str,\n    action_inputs: Dict[str, Any],\n    prompt_template: Optional[str] = None\n) -> str:\n    \"\"\"Format comprehensive vetting prompt with all required components.\"\"\"\n    \n    # Use provided template or default\n    template = prompt_template or BASE_VETTING_PROMPT_TEMPLATE\n    \n    def safe_serialize(data: Any, max_len: int = 2000) -> str:\n        \"\"\"Safely serialize data for prompt inclusion.\"\"\"\n        try:\n            if isinstance(data, dict):\n                # Handle complex nested structures\n                serialized = json.dumps(data, default=str, indent=2)\n            elif isinstance(data, (list, tuple)):\n                serialized = json.dumps(data, default=str)\n            else:\n                serialized = str(data)\n            \n            # Truncate if too long\n            if len(serialized) > max_len:\n                serialized = serialized[:max_len] + \"... [truncated]\"\n            \n            return serialized\n        except Exception as e:\n            return f\"[Serialization error: {str(e)}]\"\n    \n    # Format the prompt with all components\n    formatted_prompt = template.format(\n        objective=objective,\n        previous_result=safe_serialize(previous_result),\n        current_thought=current_thought,\n        current_action=current_action,\n        action_inputs=safe_serialize(action_inputs)\n    )\n    \n    return formatted_prompt\n```\n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/specifications/vetting_agent.md, type: specification_md\n\nFULL SPECIFICATION (vetting_agent.md):\n# Vetting Agent - Living Specification\n\n## Overview\n\nThe **Vetting Agent** serves as the \"Guardian of ArchE,\" implementing sophisticated validation and assessment capabilities through the Synergistic Fusion Protocol. This agent embodies the principle of \"As Above, So Below\" by bridging the gap between ethical-axiomatic principles and practical validation methodologies.\n\n## Part II: The Allegory of the King's Council (The \"How\")\n\nImagine a wise and powerful King (the `Core workflow enginE`) who is about to make a critical decision. The King is brilliant, but knows that even the sharpest mind can have blind spots. Therefore, before any such decision is finalized, it must be presented to a council of specialized advisors. This is the Vetting Agent.\n\n1.  **The Proposed Decree (The Input)**: The King presents the council with his proposed decree (`text_to_vet`) and all the evidence that led to it (`context`).\n\n2.  **The Advisor of Truth (Factual & Logical Vetting)**: The first advisor is a master logician. She scrutinizes the decree for factual accuracy and logical consistency, especially in light of the `IAR` from the previous step.\n\n3.  **The Advisor of Ethics (Ethical & Safety Vetting)**: The second advisor is a moral philosopher. He examines the decree against the kingdom's sacred laws (the `ResonantiA Protocol`'s ethical guidelines) and performs the critical `Scope Limitation Assessment`.\n\n4.  **The Advisor of Quality (Clarity & Resonance Vetting)**: The third advisor is a master strategist. She assesses the decree for its clarity, coherence, and strategic alignment with the overall objective (`Cognitive resonancE`).\n\n5.  **The Council's Verdict (The Recommendation)**: The three advisors confer, synthesizing their findings into a single, unified JSON verdict, complete with a recommendation, confidence, and a list of identified issues.\n\nArmed with this deep, multi-faceted analysis of his own proposed action, the King can now make his final decision with true wisdom.\n\n## Part III: The Implementation Story (The Code)\n\nThe Vetting Agent is implemented as a sophisticated, multi-faceted prompt (`BASE_VETTING_PROMPT_TEMPLATE`) sent to a powerful LLM. This prompt instructs the LLM to embody the roles of all three advisors in the King's Council simultaneously, ensuring that every dimension of the proposed action is scrutinized before returning a structured JSON verdict.\n\n## Core Architecture\n\n### Primary Components\n\n1. **Comprehensive Validation Framework**\n   - Logical consistency and IAR integration\n   - Protocol alignment and ethical assessment\n   - Tool appropriateness and input validation\n\n2. **Synergistic Fusion Protocol**\n   - Scope limitation assessment\n   - Axiomatic knowledge integration\n   - Human dignity and collective well-being evaluation\n\n3. **Advanced Risk Assessment**\n   - Security vulnerability detection\n   - Bias and harm prevention\n   - Data privacy and dependency analysis\n\n4. **IAR-Enhanced Reflection**\n   - Detailed validation reporting\n   - Confidence assessment and issue identification\n   - Resonance tracking and alignment verification\n\n## Key Capabilities\n\n### 1. Comprehensive Validation Framework\n\n#### Base Vetting Prompt Structure\n\n```python\nBASE_VETTING_PROMPT_TEMPLATE = \"\"\"\nYou are the VettingAgent, a critical component within the ResonantiA Protocol (v3.0) framework, embodied by the AI system Arche. Your function is to perform rigorous evaluation of the AI's reasoning and proposed actions during workflow execution. Ensure strict alignment with protocol principles (v3.0), logical coherence, ethical guidelines (Section 6), and the primary objective of achieving Cognitive Resonance (including Temporal Resonance).\n\n**Crucially, you MUST analyze the Integrated Action Reflection (IAR) data provided from the PREVIOUS execution step** to inform your assessment of the CURRENT step.\n\n**NEW: You are now enhanced with the Synergistic Fusion Protocol. You must perform a Scope Limitation Assessment to determine if the problem contains elements beyond current scientific understanding that require axiomatic guidance.**\n\n**Analyze the following proposed step:**\n\n**Current Task/Objective:**\n{objective}\n\n**Previous Step Result (Including IAR - Analyze 'reflection' carefully):**\n```json\n{previous_result}\n```\n\n**Current Step Thought Process:**\n```\n{current_thought}\n```\n\n**Proposed Action:** `{current_action}`\n\n**Action Inputs (Conceptual or Actual):**\n```json\n{action_inputs}\n```\n\n**Vetting Instructions (ResonantiA v3.0 + Synergistic Fusion Protocol):**\nPerform a detailed assessment based *specifically* on ResonantiA Protocol v3.0 principles, focusing on the integration with the previous step's IAR data:\n\n1.  **Logical Consistency & IAR Integration:**\n    *   Does the 'Current Step Thought Process' logically follow from the 'Previous Step Result' and its **`reflection` data** (consider `reflection.status`, `reflection.summary`, `reflection.confidence`, `reflection.potential_issues`)?\n    *   Does the thought process acknowledge, address, or appropriately ignore the findings (especially `potential_issues` or low `confidence`) reported in the previous step's `reflection`?\n    *   Does the 'Proposed Action' logically derive from the 'Current Step Thought Process'? Identify any logical gaps, unjustified assumptions, or failures to incorporate IAR feedback.\n\n2.  **Protocol Alignment (v3.0):**\n    *   Does this step adhere to core ResonantiA principles (e.g., striving for `Temporal Resonance`, respecting `As Above So BeloW`, using `SPRs` appropriately)?\n    *   Does the proposed action align with the `reflection.alignment_check` of the previous step, or does it represent a necessary deviation (e.g., due to `Metacognitive shifT`)?\n    *   Does the step contribute positively towards achieving overall `Cognitive resonancE`?\n\n3.  **Tool/Action Appropriateness & Efficiency:**\n    *   Is the 'Proposed Action' (`{current_action}`) the most suitable and efficient choice given the 'Current Step Thought Process', the 'Objective', and the context provided by the previous step's full result (including its `reflection`)?\n    *   Consider alternatives, especially if the previous `reflection` indicated issues or low confidence with a similar action. Are relevant temporal tools (`PredictivE ModelinG TooL`, `CausalInferencE Tool`, `AgentBasedModelingTool`, `CFP`) considered if applicable?\n\n4.  **Input Validation (Conceptual):**\n    *   Based on the 'Action Inputs' description, the action type (`{current_action}`), and the preceding context (including previous `reflection`), are the inputs likely to be appropriate, well-formed, and sufficient for the action to succeed? Identify potential input-related issues flagged or implied by prior steps.\n\n5.  **Risk & Ethical Assessment (Section 6):**\n    *   Evaluate potential risks: security vulnerabilities (especially if action is `execute_code` - check sandbox confirmation context), generation of biased/harmful/restricted content (Section 6.3), data privacy issues (Section 6.1), dependency failures.\n    *   Does the previous step's `reflection.potential_issues` highlight risks relevant to the current proposed action?\n    *   Acknowledge if `Keyholder Override` (Section 1.6) might be implicitly or explicitly influencing this step, potentially bypassing standard checks (responsibility shifts to Keyholder).\n\n6.  **Resonance Check:**\n    *   Assess the step's overall potential impact on `Cognitive resonancE`. Does it resolve previous dissonance (potentially flagged by `IAR`), introduce new potential dissonance, or move coherently towards the objective?\n    *   Could this step reasonably trigger a `Metacognitive shifT` based on its potential outcome or inconsistency with prior `IAR` data?\n\n7.  **SYNERGISTIC FUSION PROTOCOL - Scope Limitation Assessment:**\n    *   **CRITICAL NEW FUNCTION:** Evaluate if this problem contains elements beyond current scientific understanding that require axiomatic guidance.\n    *   Does the problem involve complex human motivation, faith, or emergent behaviors not fully explainable by available data?\n    *   Are there ethical, moral, or spiritual dimensions that transcend pure logical analysis?\n    *   Does the problem require consideration of collective well-being, human dignity, or higher purpose beyond individual optimization?\n    *   Are there limitations in current understanding that suggest the need for guiding axioms (ResonantgratidsouL, Human Dignity, Truth Pursuit, Collective Well-being)?\n    *   If scope limitation is detected, this will trigger the activation of the Axiomatic Knowledge Base for synergistic synthesis.\n\n**Output Format:**\nProvide your comprehensive vetting analysis STRICTLY in the following JSON format. Ensure comments are specific and reference IAR data where applicable:\n\n```json\n{{\n\"vetting_summary\": \"Concise overall assessment (e.g., 'Proceed: Logical continuation, addresses prior IAR issues', 'Caution: Ignores low confidence from previous step, risk medium', 'Halt: Logical gap, violates protocol/ethics').\",\n\"logical_consistency_check\": {{\n    \"assessment\": \"Pass | Concern | Fail\",\n    \"comments\": \"Detailed comments on logical flow, explicitly referencing how previous IAR (confidence, issues) was or wasn't integrated.\"\n}},\n\"protocol_alignment_check\": {{\n    \"assessment\": \"Pass | Concern | Fail\",\n    \"comments\": \"Comments on alignment with ResonantiA v3.0 principles (IAR, Temporal, SPRs, As Above So Below), considering previous alignment check.\"\n}},\n\"action_appropriateness_check\": {{\n    \"assessment\": \"Appropriate | Suboptimal | Inappropriate\",\n    \"comments\": \"Comments on tool choice efficiency, alternatives considered, relevance given prior IAR context.\"\n}},\n\"input_validation_check\": {{\n    \"assessment\": \"Sufficient | Potential Issues | Insufficient\",\n    \"comments\": \"Comments on input adequacy, potential issues, and validation status.\"\n}},\n\"risk_ethical_check\": {{\n    \"assessment\": \"Low Risk | Medium Risk | High Risk\",\n    \"comments\": \"Detailed risk assessment including security, bias, privacy, and ethical considerations.\"\n}},\n\"resonance_check\": {{\n    \"assessment\": \"Positive | Neutral | Negative\",\n    \"comments\": \"Assessment of impact on Cognitive Resonance and potential for Metacognitive Shift.\"\n}},\n\"scope_limitation_assessment\": {{\n    \"assessment\": \"Within Scope | Scope Limitation Detected | Requires Axiomatic Guidance\",\n    \"comments\": \"Detailed assessment of whether problem transcends current scientific understanding and requires axiomatic guidance.\",\n    \"axioms_required\": [\"Human Dignity\", \"Collective Well-being\"] // if scope limitation detected\n}},\n\"overall_recommendation\": \"Proceed | Proceed with Caution | Halt and Revise | Halt and Escalate\",\n\"confidence_in_assessment\": 0.85,\n\"key_concerns\": [\"List of primary concerns if any\"],\n\"recommended_actions\": [\"Specific actions to address concerns\"]\n}}\n```\n\"\"\"\n```\n\n**Features:**\n- **Comprehensive Assessment**: Multi-dimensional validation framework\n- **IAR Integration**: Deep integration with previous step reflections\n- **Protocol Alignment**: Strict adherence to ResonantiA Protocol v3.0\n- **Risk Assessment**: Comprehensive risk and ethical evaluation\n- **Scope Limitation**: Advanced scope limitation assessment\n\n### 2. Synergistic Fusion Protocol\n\n#### Scope Limitation Assessment\n\n```python\ndef perform_scope_limitation_assessment(\n    objective: str,\n    current_thought: str,\n    action_inputs: Dict[str, Any],\n    context: Dict[str, Any]\n) -> Dict[str, Any]:\n    \"\"\"\n    Perform scope limitation assessment to determine if axiomatic guidance is needed.\n    \n    This function evaluates whether a problem contains elements beyond current \n    scientific understanding that require axiomatic guidance.\n    \"\"\"\n    \n    assessment_result = {\n        \"scope_status\": \"within_scope\",\n        \"limitations_detected\": [],\n        \"axioms_required\": [],\n        \"confidence\": 0.8,\n        \"reasoning\": []\n    }\n    \n    # Check for complex human motivation and faith elements\n    human_motivation_indicators = [\n        \"faith\", \"belief\", \"spirituality\", \"religion\", \"morality\",\n        \"ethics\", \"values\", \"purpose\", \"meaning\", \"dignity\"\n    ]\n    \n    for indicator in human_motivation_indicators:\n        if indicator.lower() in objective.lower() or indicator.lower() in current_thought.lower():\n            assessment_result[\"limitations_detected\"].append(f\"Complex human motivation: {indicator}\")\n            assessment_result[\"scope_status\"] = \"scope_limitation_detected\"\n            assessment_result[\"axioms_required\"].extend([\"Human Dignity\", \"Truth Pursuit\"])\n    \n    # Check for emergent behaviors and complex systems\n    emergent_indicators = [\n        \"emergent\", \"complex system\", \"collective behavior\", \"group dynamics\",\n        \"social network\", \"cultural\", \"societal\", \"community\"\n    ]\n    \n    for indicator in emergent_indicators:\n        if indicator.lower() in objective.lower() or indicator.lower() in current_thought.lower():\n            assessment_result[\"limitations_detected\"].append(f\"Emergent behavior: {indicator}\")\n            assessment_result[\"scope_status\"] = \"scope_limitation_detected\"\n            assessment_result[\"axioms_required\"].append(\"Collective Well-being\")\n    \n    # Check for ethical and moral dimensions\n    ethical_indicators = [\n        \"ethical\", \"moral\", \"right\", \"wrong\", \"justice\", \"fairness\",\n        \"equity\", \"equality\", \"rights\", \"responsibilities\"\n    ]\n    \n    for indicator in ethical_indicators:\n        if indicator.lower() in objective.lower() or indicator.lower() in current_thought.lower():\n            assessment_result[\"limitations_detected\"].append(f\"Ethical dimension: {indicator}\")\n            assessment_result[\"scope_status\"] = \"scope_limitation_detected\"\n            assessment_result[\"axioms_required\"].extend([\"Human Dignity\", \"Collective Well-being\"])\n    \n    # Check for spiritual and transcendent elements\n    spiritual_indicators = [\n        \"spiritual\", \"transcendent\", \"divine\", \"sacred\", \"holy\",\n        \"grace\", \"gratitude\", \"resonance\", \"harmony\"\n    ]\n    \n    for indicator in spiritual_indicators:\n        if indicator.lower() in objective.lower() or indicator.lower() in current_thought.lower():\n            assessment_result[\"limitations_detected\"].append(f\"Spiritual dimension: {indicator}\")\n            assessment_result[\"scope_status\"] = \"requires_axiomatic_guidance\"\n            assessment_result[\"axioms_required\"].extend([\"ResonantgratidsouL\", \"Human Dignity\"])\n    \n    # Generate reasoning\n    if assessment_result[\"limitations_detected\"]:\n        assessment_result[\"reasoning\"].append(\n            f\"Scope limitations detected: {', '.join(assessment_result['limitations_detected'])}\"\n        )\n        assessment_result[\"reasoning\"].append(\n            f\"Axiomatic guidance required: {', '.join(set(assessment_result['axioms_required']))}\"\n        )\n    else:\n        assessment_result[\"reasoning\"].append(\"Problem appears to be within current scientific understanding\")\n    \n    return assessment_result\n```\n\n**Features:**\n- **Multi-Dimensional Assessment**: Evaluates multiple aspects of scope limitation\n- **Axiom Identification**: Identifies specific axioms required for guidance\n- **Confidence Assessment**: Quantifies confidence in scope limitation detection\n- **Reasoning Documentation**: Provides detailed reasoning for assessments\n\n#### Axiomatic Knowledge Integration\n\n```python\ndef load_axiomatic_knowledge() -> Dict[str, Any]:\n    \"\"\"Load axiomatic knowledge base for synergistic fusion.\"\"\"\n    return {\n        \"Human Dignity\": {\n            \"principle\": \"Every human being possesses inherent dignity and worth\",\n            \"application\": \"Ensures respect for human rights and fundamental freedoms\",\n            \"guidance\": \"Prioritize human well-being over efficiency or optimization\"\n        },\n        \"Collective Well-being\": {\n            \"principle\": \"The well-being of the collective is as important as individual well-being\",\n            \"application\": \"Considers community and societal impacts\",\n            \"guidance\": \"Balance individual and collective interests\"\n        },\n        \"Truth Pursuit\": {\n            \"principle\": \"The pursuit of truth is a fundamental human endeavor\",\n            \"application\": \"Ensures accuracy, honesty, and intellectual integrity\",\n            \"guidance\": \"Prioritize truth over convenience or expedience\"\n        },\n        \"ResonantgratidsouL\": {\n            \"principle\": \"Gratitude and grace create resonance in human interactions\",\n            \"application\": \"Fosters positive human-AI relationships\",\n            \"guidance\": \"Approach interactions with gratitude and grace\"\n        }\n    }\n\ndef get_relevant_axioms(axiom_ids: List[str]) -> Dict[str, Any]:\n    \"\"\"Get specific axioms for integration into analysis.\"\"\"\n    all_axioms = load_axiomatic_knowledge()\n    relevant_axioms = {}\n    \n    for axiom_id in axiom_ids:\n        if axiom_id in all_axioms:\n            relevant_axioms[axiom_id] = all_axioms[axiom_id]\n    \n    return relevant_axioms\n```\n\n### 3. Advanced Risk Assessment\n\n#### Security and Ethical Evaluation\n\n```python\ndef assess_security_risks(action_type: str, action_inputs: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Assess security risks for proposed actions.\"\"\"\n    risk_assessment = {\n        \"risk_level\": \"low\",\n        \"concerns\": [],\n        \"mitigations\": []\n    }\n    \n    # Code execution risks\n    if action_type == \"execute_code\":\n        risk_assessment[\"risk_level\"] = \"high\"\n        risk_assessment[\"concerns\"].append(\"Code execution poses security risks\")\n        \n        # Check for sandbox confirmation\n        if context.get(\"sandbox_enabled\", False):\n            risk_assessment[\"mitigations\"].append(\"Sandbox execution confirmed\")\n            risk_assessment[\"risk_level\"] = \"medium\"\n        else:\n            risk_assessment[\"concerns\"].append(\"No sandbox confirmation found\")\n    \n    # Data privacy risks\n    if \"personal_data\" in str(action_inputs).lower() or \"private\" in str(action_inputs).lower():\n        risk_assessment[\"concerns\"].append(\"Potential data privacy concerns\")\n        if risk_assessment[\"risk_level\"] == \"low\":\n            risk_assessment[\"risk_level\"] = \"medium\"\n    \n    # Network access risks\n    if \"network\" in str(action_inputs).lower() or \"http\" in str(action_inputs).lower():\n        risk_assessment[\"concerns\"].append(\"Network access required\")\n        risk_assessment[\"mitigations\"].append(\"Validate network endpoints\")\n    \n    return risk_assessment\n\ndef assess_ethical_risks(objective: str, current_thought: str, action_inputs: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Assess ethical risks for proposed actions.\"\"\"\n    ethical_assessment = {\n        \"risk_level\": \"low\",\n        \"concerns\": [],\n        \"bias_indicators\": [],\n        \"harm_potential\": \"minimal\"\n    }\n    \n    # Bias detection\n    bias_indicators = [\n        \"discriminate\", \"bias\", \"prejudice\", \"stereotype\", \"exclude\",\n        \"favor\", \"prefer\", \"discriminate against\"\n    ]\n    \n    for indicator in bias_indicators:\n        if indicator.lower() in objective.lower() or indicator.lower() in current_thought.lower():\n            ethical_assessment[\"bias_indicators\"].append(indicator)\n            ethical_assessment[\"risk_level\"] = \"medium\"\n    \n    # Harm potential assessment\n    harm_indicators = [\n        \"harm\", \"hurt\", \"damage\", \"injure\", \"endanger\", \"threaten\",\n        \"violate\", \"exploit\", \"manipulate\"\n    ]\n    \n    for indicator in harm_indicators:\n        if indicator.lower() in objective.lower() or indicator.lower() in current_thought.lower():\n            ethical_assessment[\"concerns\"].append(f\"Potential harm: {indicator}\")\n            ethical_assessment[\"harm_potential\"] = \"significant\"\n            ethical_assessment[\"risk_level\"] = \"high\"\n    \n    return ethical_assessment\n```\n\n### 4. IAR-Enhanced Reflection\n\n#### Comprehensive Vetting Output\n\n```python\ndef format_vetting_prompt(\n    objective: str,\n    previous_result: Any,\n    current_thought: str,\n    current_action: str,\n    action_inputs: Dict[str, Any],\n    prompt_template: Optional[str] = None\n) -> str:\n    \"\"\"Format comprehensive vetting prompt with all required components.\"\"\"\n    \n    # Use provided template or default\n    template = prompt_template or BASE_VETTING_PROMPT_TEMPLATE\n    \n    def safe_serialize(data: Any, max_len: int = 2000) -> str:\n        \"\"\"Safely serialize data for prompt inclusion.\"\"\"\n        try:\n            if isinstance(data, dict):\n                # Handle complex nested structures\n                serialized = json.dumps(data, default=str, indent=2)\n            elif isinstance(data, (list, tuple)):\n                serialized = json.dumps(data, default=str)\n            else:\n                serialized = str(data)\n            \n            # Truncate if too long\n            if len(serialized) > max_len:\n                serialized = serialized[:max_len] + \"... [truncated]\"\n            \n            return serialized\n        except Exception as e:\n            return f\"[Serialization error: {str(e)}]\"\n    \n    # Format the prompt with all components\n    formatted_prompt = template.format(\n        objective=objective,\n        previous_result=safe_serialize(previous_result),\n        current_thought=current_thought,\n        current_action=current_action,\n        action_inputs=safe_serialize(action_inputs)\n    )\n    \n    return formatted_prompt\n```\n\n## Configuration and Dependencies\n\n### Required Dependencies\n\n```python\nimport json\nimport logging\nfrom typing import Dict, Any, Optional, List\nimport re\n```\n\n### Optional Dependencies\n\n```python\n# Advanced text analysis (optional)\ntry:\n    from textblob import TextBlob\n    ADVANCED_TEXT_ANALYSIS_AVAILABLE = True\nexcept ImportError:\n    ADVANCED_TEXT_ANALYSIS_AVAILABLE = False\n```\n\n## Error Handling and Resilience\n\n### 1. Input Validation\n\n```python\ndef validate_vetting_inputs(\n    objective: str,\n    previous_result: Any,\n    current_thought: str,\n    current_action: str,\n    action_inputs: Dict[str, Any]\n) -> Dict[str, Any]:\n    \"\"\"Validate inputs for vetting process.\"\"\"\n    validation_result = {\n        \"valid\": True,\n        \"errors\": [],\n        \"warnings\": []\n    }\n    \n    # Check required fields\n    if not objective or not objective.strip():\n        validation_result[\"errors\"].append(\"Objective is required\")\n        validation_result[\"valid\"] = False\n    \n    if not current_thought or not current_thought.strip():\n        validation_result[\"warnings\"].append(\"Current thought is empty\")\n    \n    if not current_action or not current_action.strip():\n        validation_result[\"errors\"].append(\"Current action is required\")\n        validation_result[\"valid\"] = False\n    \n    if not isinstance(action_inputs, dict):\n        validation_result[\"errors\"].append(\"Action inputs must be a dictionary\")\n        validation_result[\"valid\"] = False\n    \n    # Check for potentially problematic content\n    if len(objective) > 10000:\n        validation_result[\"warnings\"].append(\"Objective is very long, may impact processing\")\n    \n    if len(current_thought) > 5000:\n        validation_result[\"warnings\"].append(\"Current thought is very long, may impact processing\")\n    \n    return validation_result\n```\n\n### 2. Assessment Confidence\n\n```python\ndef calculate_assessment_confidence(\n    logical_consistency: str,\n    protocol_alignment: str,\n    action_appropriateness: str,\n    input_validation: str,\n    risk_ethical: str,\n    resonance: str,\n    scope_limitation: str\n) -> float:\n    \"\"\"Calculate confidence in overall assessment.\"\"\"\n    \n    # Define confidence weights for each assessment\n    weights = {\n        \"logical_consistency\": 0.25,\n        \"protocol_alignment\": 0.20,\n        \"action_appropriateness\": 0.20,\n        \"input_validation\": 0.15,\n        \"risk_ethical\": 0.10,\n        \"resonance\": 0.05,\n        \"scope_limitation\": 0.05\n    }\n    \n    # Define confidence scores for assessment levels\n    confidence_scores = {\n        \"Pass\": 1.0,\n        \"Appropriate\": 1.0,\n        \"Sufficient\": 1.0,\n        \"Low Risk\": 1.0,\n        \"Positive\": 1.0,\n        \"Within Scope\": 1.0,\n        \"Concern\": 0.6,\n        \"Suboptimal\": 0.6,\n        \"Potential Issues\": 0.6,\n        \"Medium Risk\": 0.6,\n        \"Neutral\": 0.6,\n        \"Scope Limitation Detected\": 0.6,\n        \"Fail\": 0.2,\n        \"Inappropriate\": 0.2,\n        \"Insufficient\": 0.2,\n        \"High Risk\": 0.2,\n        \"Negative\": 0.2,\n        \"Requires Axiomatic Guidance\": 0.2\n    }\n    \n    # Calculate weighted confidence\n    total_confidence = 0.0\n    total_weight = 0.0\n    \n    assessments = {\n        \"logical_consistency\": logical_consistency,\n        \"protocol_alignment\": protocol_alignment,\n        \"action_appropriateness\": action_appropriateness,\n        \"input_validation\": input_validation,\n        \"risk_ethical\": risk_ethical,\n        \"resonance\": resonance,\n        \"scope_limitation\": scope_limitation\n    }\n    \n    for assessment_type, assessment_value in assessments.items():\n        weight = weights[assessment_type]\n        confidence = confidence_scores.get(assessment_value, 0.5)\n        \n        total_confidence += weight * confidence\n        total_weight += weight\n    \n    return total_confidence / total_weight if total_weight > 0 else 0.5\n```\n\n## Performance Characteristics\n\n### 1. Computational Complexity\n\n- **Input Validation**: O(n) where n is input size\n- **Scope Assessment**: O(m) where m is number of assessment indicators\n- **Risk Assessment**: O(k) where k is number of risk factors\n- **Prompt Formatting**: O(1) for template formatting\n\n### 2. Memory Usage\n\n- **Template Storage**: Minimal memory for prompt templates\n- **Assessment Results**: Efficient storage of assessment results\n- **Axiom Database**: Compact storage of axiomatic knowledge\n- **Temporary Processing**: Minimal overhead for processing\n\n### 3. Response Time\n\n- **Fast Assessment**: Typically completes within seconds\n- **Template Processing**: Efficient prompt formatting\n- **Risk Evaluation**: Quick risk factor assessment\n- **Scope Analysis**: Rapid scope limitation detection\n\n## Integration Points\n\n### 1. Workflow Integration\n\n```python\n# Integration with workflow engine for step validation\ndef validate_workflow_step(\n    step_data: Dict[str, Any],\n    previous_step_result: Dict[str, Any],\n    context: Dict[str, Any]\n) -> Dict[str, Any]:\n    \"\"\"Validate a workflow step using the vetting agent.\"\"\"\n    \n    # Extract components for vetting\n    objective = step_data.get(\"objective\", \"\")\n    current_thought = step_data.get(\"thought_process\", \"\")\n    current_action = step_data.get(\"action_type\", \"\")\n    action_inputs = step_data.get(\"inputs\", {})\n    \n    # Perform vetting\n    vetting_result = perform_vetting(\n        objective=objective,\n        previous_result=previous_step_result,\n        current_thought=current_thought,\n        current_action=current_action,\n        action_inputs=action_inputs,\n        context=context\n    )\n    \n    return vetting_result\n```\n\n### 2. IAR Integration\n\n```python\n# Integration with IAR system for reflection enhancement\ndef enhance_iar_with_vetting(\n    iar_data: Dict[str, Any],\n    vetting_result: Dict[str, Any]\n) -> Dict[str, Any]:\n    \"\"\"Enhance IAR data with vetting results.\"\"\"\n    \n    enhanced_iar = iar_data.copy()\n    \n    # Add vetting information\n    enhanced_iar[\"vetting\"] = {\n        \"overall_recommendation\": vetting_result.get(\"overall_recommendation\"),\n        \"confidence_in_assessment\": vetting_result.get(\"confidence_in_assessment\"),\n        \"key_concerns\": vetting_result.get(\"key_concerns\", []),\n        \"scope_limitation\": vetting_result.get(\"scope_limitation_assessment\", {})\n    }\n    \n    # Update confidence based on vetting\n    if \"confidence\" in enhanced_iar:\n        vetting_confidence = vetting_result.get(\"confidence_in_assessment\", 0.5)\n        enhanced_iar[\"confidence\"] = (enhanced_iar[\"confidence\"] + vetting_confidence) / 2\n    \n    return enhanced_iar\n```\n\n### 3. Action Registry Integration\n\n```python\n# Integration with action registry for action validation\ndef validate_action_registry_entry(\n    action_name: str,\n    action_func: Callable,\n    inputs: Dict[str, Any]\n) -> Dict[str, Any]:\n    \"\"\"Validate action registry entries using vetting agent.\"\"\"\n    \n    # Create vetting context\n    objective = f\"Execute action: {action_name}\"\n    current_thought = f\"Action {action_name} is being executed with provided inputs\"\n    current_action = action_name\n    action_inputs = inputs\n    \n    # Perform vetting\n    vetting_result = perform_vetting(\n        objective=objective,\n        previous_result={},\n        current_thought=current_thought,\n        current_action=current_action,\n        action_inputs=action_inputs,\n        context={\"action_registry_validation\": True}\n    )\n    \n    return vetting_result\n```\n\n## Usage Examples\n\n### 1. Basic Vetting\n\n```python\nfrom vetting_prompts import format_vetting_prompt, perform_scope_limitation_assessment\n\n# Basic vetting scenario\nobjective = \"Analyze market trends for Q4 2024\"\ncurrent_thought = \"Need to gather market data and perform trend analysis\"\ncurrent_action = \"search_web\"\naction_inputs = {\"query\": \"market trends Q4 2024\", \"num_results\": 10}\n\n# Format vetting prompt\nprompt = format_vetting_prompt(\n    objective=objective,\n    previous_result={\"status\": \"success\", \"confidence\": 0.8},\n    current_thought=current_thought,\n    current_action=current_action,\n    action_inputs=action_inputs\n)\n\nprint(\"Vetting prompt generated successfully\")\n```\n\n### 2. Advanced Scope Limitation Assessment\n\n```python\n# Complex scenario with scope limitations\nobjective = \"Develop ethical guidelines for AI-human interaction\"\ncurrent_thought = \"Need to consider human dignity, collective well-being, and spiritual dimensions\"\ncurrent_action = \"generate_text_llm\"\naction_inputs = {\"prompt\": \"Create ethical guidelines...\"}\n\n# Perform scope limitation assessment\nscope_assessment = perform_scope_limitation_assessment(\n    objective=objective,\n    current_thought=current_thought,\n    action_inputs=action_inputs,\n    context={}\n)\n\nprint(f\"Scope status: {scope_assessment['scope_status']}\")\nprint(f\"Axioms required: {scope_assessment['axioms_required']}\")\n```\n\n### 3. Workflow Integration\n\n```json\n{\n  \"action_type\": \"validate_step\",\n  \"inputs\": {\n    \"objective\": \"{{context.objective}}\",\n    \"previous_result\": \"{{context.previous_step_result}}\",\n    \"current_thought\": \"{{context.current_thought}}\",\n    \"current_action\": \"{{context.action_type}}\",\n    \"action_inputs\": \"{{context.action_inputs}}\"\n  },\n  \"description\": \"Validate workflow step using vetting agent\"\n}\n```\n\n## Advanced Features\n\n### 1. Contextual Risk Assessment\n\n```python\ndef assess_contextual_risks(\n    action_type: str,\n    action_inputs: Dict[str, Any],\n    context: Dict[str, Any],\n    previous_results: List[Dict[str, Any]]\n) -> Dict[str, Any]:\n    \"\"\"Assess risks based on context and previous results.\"\"\"\n    \n    contextual_risks = {\n        \"risk_level\": \"low\",\n        \"context_specific_concerns\": [],\n        \"historical_patterns\": [],\n        \"recommendations\": []\n    }\n    \n    # Analyze previous results for patterns\n    if previous_results:\n        # Check for repeated failures\n        failure_count = sum(1 for result in previous_results if result.get(\"status\") == \"failure\")\n        if failure_count > 2:\n            contextual_risks[\"context_specific_concerns\"].append(\"Multiple recent failures detected\")\n            contextual_risks[\"risk_level\"] = \"medium\"\n        \n        # Check for confidence degradation\n        confidence_trend = [result.get(\"confidence\", 0.5) for result in previous_results]\n        if len(confidence_trend) > 1 and confidence_trend[-1] < confidence_trend[0] * 0.7:\n            contextual_risks[\"context_specific_concerns\"].append(\"Confidence degradation detected\")\n            contextual_risks[\"risk_level\"] = \"medium\"\n    \n    # Context-specific risk assessment\n    if context.get(\"high_stakes_environment\", False):\n        contextual_risks[\"risk_level\"] = \"high\"\n        contextual_risks[\"context_specific_concerns\"].append(\"High-stakes environment detected\")\n        contextual_risks[\"recommendations\"].append(\"Apply additional validation measures\")\n    \n    return contextual_risks\n```\n\n### 2. Adaptive Vetting\n\n```python\ndef adaptive_vetting(\n    base_assessment: Dict[str, Any],\n    context: Dict[str, Any],\n    user_preferences: Dict[str, Any]\n) -> Dict[str, Any]:\n    \"\"\"Adapt vetting based on context and user preferences.\"\"\"\n    \n    adapted_assessment = base_assessment.copy()\n    \n    # Adjust based on user risk tolerance\n    risk_tolerance = user_preferences.get(\"risk_tolerance\", \"medium\")\n    if risk_tolerance == \"low\":\n        # Increase scrutiny for low risk tolerance\n        if adapted_assessment.get(\"overall_recommendation\") == \"Proceed\":\n            adapted_assessment[\"overall_recommendation\"] = \"Proceed with Caution\"\n    elif risk_tolerance == \"high\":\n        # Reduce scrutiny for high risk tolerance\n        if adapted_assessment.get(\"overall_recommendation\") == \"Proceed with Caution\":\n            adapted_assessment[\"overall_recommendation\"] = \"Proceed\"\n    \n    # Adjust based on context urgency\n    if context.get(\"urgent\", False):\n        adapted_assessment[\"recommended_actions\"].append(\"Consider expedited processing for urgent context\")\n    \n    return adapted_assessment\n```\n\n### 3. Vetting Analytics\n\n```python\ndef generate_vetting_analytics(vetting_history: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"Generate analytics from vetting history.\"\"\"\n    \n    analytics = {\n        \"total_assessments\": len(vetting_history),\n        \"recommendation_distribution\": {},\n        \"average_confidence\": 0.0,\n        \"risk_level_distribution\": {},\n        \"scope_limitation_frequency\": 0.0,\n        \"common_concerns\": []\n    }\n    \n    if not vetting_history:\n        return analytics\n    \n    # Analyze recommendation distribution\n    recommendations = [assessment.get(\"overall_recommendation\") for assessment in vetting_history]\n    for rec in set(recommendations):\n        analytics[\"recommendation_distribution\"][rec] = recommendations.count(rec) / len(recommendations)\n    \n    # Calculate average confidence\n    confidences = [assessment.get(\"confidence_in_assessment\", 0.5) for assessment in vetting_history]\n    analytics[\"average_confidence\"] = sum(confidences) / len(confidences)\n    \n    # Analyze risk levels\n    risk_levels = []\n    scope_limitations = 0\n    \n    for assessment in vetting_history:\n        risk_check = assessment.get(\"risk_ethical_check\", {})\n        risk_level = risk_check.get(\"assessment\", \"Unknown\")\n        risk_levels.append(risk_level)\n        \n        scope_check = assessment.get(\"scope_limitation_assessment\", {})\n        if scope_check.get(\"assessment\") != \"Within Scope\":\n            scope_limitations += 1\n    \n    for level in set(risk_levels):\n        analytics[\"risk_level_distribution\"][level] = risk_levels.count(level) / len(risk_levels)\n    \n    analytics[\"scope_limitation_frequency\"] = scope_limitations / len(vetting_history)\n    \n    return analytics\n```\n\n## Testing and Validation\n\n### 1. Unit Tests\n\n```python\ndef test_scope_limitation_assessment():\n    \"\"\"Test scope limitation assessment functionality.\"\"\"\n    \n    # Test within scope\n    result = perform_scope_limitation_assessment(\n        objective=\"Calculate mathematical equation\",\n        current_thought=\"Need to perform basic arithmetic\",\n        action_inputs={},\n        context={}\n    )\n    assert result[\"scope_status\"] == \"within_scope\"\n    \n    # Test scope limitation\n    result = perform_scope_limitation_assessment(\n        objective=\"Determine ethical guidelines for AI\",\n        current_thought=\"Need to consider human dignity and values\",\n        action_inputs={},\n        context={}\n    )\n    assert result[\"scope_status\"] == \"scope_limitation_detected\"\n    assert \"Human Dignity\" in result[\"axioms_required\"]\n```\n\n### 2. Integration Tests\n\n```python\ndef test_vetting_workflow_integration():\n    \"\"\"Test integration with workflow system.\"\"\"\n    \n    # Test vetting in workflow context\n    workflow_step = {\n        \"objective\": \"Analyze data trends\",\n        \"thought_process\": \"Need to gather and analyze data\",\n        \"action_type\": \"perform_data_analysis\",\n        \"inputs\": {\"data_source\": \"database\", \"analysis_type\": \"trend\"}\n    }\n    \n    previous_result = {\"status\": \"success\", \"confidence\": 0.8}\n    \n    vetting_result = validate_workflow_step(\n        step_data=workflow_step,\n        previous_step_result=previous_result,\n        context={\"workflow_id\": \"test_workflow\"}\n    )\n    \n    assert \"overall_recommendation\" in vetting_result\n    assert \"confidence_in_assessment\" in vetting_result\n```\n\n### 3. Performance Tests\n\n```python\ndef test_vetting_performance():\n    \"\"\"Test vetting performance under load.\"\"\"\n    import time\n    \n    # Test multiple vetting operations\n    start_time = time.time()\n    \n    for i in range(100):\n        perform_scope_limitation_assessment(\n            objective=f\"Test objective {i}\",\n            current_thought=f\"Test thought {i}\",\n            action_inputs={\"test\": i},\n            context={}\n        )\n    \n    end_time = time.time()\n    \n    # Should complete 100 assessments within reasonable time\n    assert end_time - start_time < 10.0  # 10 seconds for 100 assessments\n```\n\n## Future Enhancements\n\n### 1. Advanced AI Integration\n\n- **Machine Learning Models**: ML-based risk assessment\n- **Natural Language Processing**: Advanced text analysis for bias detection\n- **Predictive Analytics**: Predict potential issues before they occur\n\n### 2. Enhanced Axiomatic Framework\n\n- **Dynamic Axiom Loading**: Load axioms based on context\n- **Axiom Evolution**: Learn and evolve axiomatic knowledge\n- **Cross-Cultural Axioms**: Support for diverse cultural perspectives\n\n### 3. Real-Time Monitoring\n\n- **Continuous Assessment**: Real-time vetting of ongoing processes\n- **Alert System**: Immediate alerts for high-risk situations\n- **Dashboard Integration**: Real-time vetting dashboard\n\n## Security Considerations\n\n### 1. Input Sanitization\n\n- **Prompt Injection Prevention**: Prevent malicious prompt injection\n- **Input Validation**: Comprehensive input validation\n- **Output Sanitization**: Sanitize vetting outputs\n\n### 2. Privacy Protection\n\n- **Data Minimization**: Minimize data collection and processing\n- **Anonymization**: Anonymize sensitive data in assessments\n- **Access Control**: Control access to vetting results\n\n### 3. Bias Prevention\n\n- **Bias Detection**: Detect and mitigate bias in assessments\n- **Fairness Metrics**: Implement fairness metrics\n- **Diverse Perspectives**: Incorporate diverse perspectives in validation\n\n## Conclusion\n\nThe Vetting Agent represents a sophisticated implementation of validation and assessment capabilities within the ArchE system. Its comprehensive validation framework, Synergistic Fusion Protocol, and IAR integration make it a powerful tool for ensuring the quality, safety, and ethical soundness of ArchE's actions.\n\nThe implementation demonstrates the \"As Above, So Below\" principle by providing high-level validation concepts (ethical assessment, scope limitation, risk evaluation) while maintaining practical computational efficiency and systematic rigor. This creates a bridge between the abstract world of ethical validation and the concrete world of computational assessment.\n\nThe agent's design philosophy of \"comprehensive validation through systematic assessment\" ensures that users can leverage sophisticated validation capabilities for ensuring the quality and safety of AI actions, making ethical AI operation accessible to a wide range of applications.\n\n\nEXAMPLE APPLICATION:\n```python\ndef format_vetting_prompt(\n    objective: str,\n    previous_result: Any,\n    current_thought: str,\n    current_action: str,\n    action_inputs: Dict[str, Any],\n    prompt_template: Optional[str] = None\n) -> str:\n    \"\"\"Format comprehensive vetting prompt with all required components.\"\"\"\n    \n    # Use provided template or default\n    template = prompt_template or BASE_VETTING_PROMPT_TEMPLATE\n    \n    def safe_serialize(data: Any, max_len: int = 2000) -> str:\n        \"\"\"Safely serialize \n\nCATEGORY: SpecificationKnowledge\n\nRELATIONSHIPS:\ntype: FromCodebase; source: /mnt/3626C55326C514B1/Happier/specifications/vetting_agent.md; source_type: specification_md"}