{"content": "TERM: Class: MastermindServer\n\nDEFINITION:\nThe unified ArchE server, integrating the full cognitive core with a WebSocket interface.\n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/mastermind_server.py, type: python_class\n\nFULL IMPLEMENTATION CODE (mastermind_server.py):\n```python\n#!/usr/bin/env python3\n\"\"\"\nArchE Mastermind Server - Unified Cognitive Core\nResonantiA Protocol v3.1-CA\n\nThis script merges the advanced cognitive core from mastermind/interact.py\nwith a robust WebSocket server architecture. It serves as the single,\nauthoritative entry point for the ArchE backend.\n\"\"\"\n\nimport sys\nimport os\nimport logging\nimport json\nfrom pathlib import Path\nimport asyncio\nimport websockets\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import Dict, Any, List\nfrom apscheduler.schedulers.asyncio import AsyncIOScheduler\n\n# Add the project root to the path to allow direct imports\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\ntry:\n    from .workflow_engine import IARCompliantWorkflowEngine\n    from .proactive_truth_system import ProactiveTruthSystem\n    from .tools.enhanced_search_tool import EnhancedSearchTool\n    from .spr_manager import SPRManager\n    from .adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator\n    from .rise_orchestrator import RISE_Orchestrator\n    from .autopoietic_governor import AutopoieticGovernor\n    from .thought_trail import ThoughtTrail # Assuming thought_trail is a singleton or class\n    from .nexus_interface import nexus_interface\nexcept ImportError:\n    # Fallback to absolute imports if relative imports fail\n    from Three_PointO_ArchE.workflow_engine import IARCompliantWorkflowEngine\n    from Three_PointO_ArchE.proactive_truth_system import ProactiveTruthSystem\n    from Three_PointO_ArchE.tools.enhanced_search_tool import EnhancedSearchTool\n    from Three_PointO_ArchE.spr_manager import SPRManager\n    from Three_PointO_ArchE.adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator\n    from Three_PointO_ArchE.rise_orchestrator import RISE_Orchestrator\n    from Three_PointO_ArchE.autopoietic_governor import AutopoieticGovernor\n    from Three_PointO_ArchE.thought_trail import ThoughtTrail\n    from Three_PointO_ArchE.nexus_interface import nexus_interface\nfrom .llm_providers.google import GoogleProvider\n\n# --- Logging Setup ---\ntry:\n    from .logging_config import setup_logging\nexcept ImportError:\n    from Three_PointO_ArchE.logging_config import setup_logging\n\n# Initialize timestamped logging system\nsetup_logging()\nlogger = logging.getLogger(\"ArchE_Mastermind_Server\")\nperf_logger = logging.getLogger(\"ArchE_Performance\")\n# session_logger = logging.getLogger(\"ArchE_Session\") # Example for a session-specific logger\n\nclass MastermindServer:\n    \"\"\"\n    The unified ArchE server, integrating the full cognitive core with a WebSocket interface.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initializes all cognitive components of the ArchE system.\"\"\"\n        logger.info(\"ðŸ§  Initializing ArchE Mastermind Server...\")\n        self.config = self._load_config()\n        self.engine = IARCompliantWorkflowEngine()\n        self._initialize_ptrf()\n        self._initialize_aco()\n        self._initialize_rise()\n        self._initialize_autopoiesis()\n        self.executor = ThreadPoolExecutor()\n        logger.info(\"âœ… ArchE Mastermind Server Initialized Successfully.\")\n\n    def _load_config(self) -> Dict[str, Any]:\n        \"\"\"Loads the enhanced mastermind configuration.\"\"\"\n        try:\n            config_path = project_root / \"mastermind\" / \"enhanced_mastermind_config.json\"\n            with open(config_path, 'r') as f:\n                logger.info(\"Loading enhanced mastermind configuration...\")\n                return json.load(f)\n        except (IOError, json.JSONDecodeError) as e:\n            logger.error(f\"FATAL: Could not load or parse configuration file: {e}. Using empty config.\", exc_info=True)\n            return {}\n\n    def _initialize_ptrf(self):\n        \"\"\"Initializes the Proactive Truth Resonance Framework.\"\"\"\n        try:\n            from dotenv import load_dotenv\n            load_dotenv()\n            api_key = os.environ.get(\"GEMINI_API_KEY\") or os.environ.get(\"GOOGLE_API_KEY\")\n            if not api_key:\n                raise ValueError(\"GEMINI_API_KEY or GOOGLE_API_KEY not set.\")\n            \n            self.llm_provider = GoogleProvider(api_key=api_key) # Store provider as instance variable\n            web_search_tool = EnhancedSearchTool()\n            spr_definitions_path = str(project_root / \"knowledge_graph\" / \"spr_definitions_tv.json\")\n            self.spr_manager = SPRManager(spr_filepath=spr_definitions_path)\n            \n            self.truth_seeker = ProactiveTruthSystem(\n                workflow_engine=self.engine,\n                llm_provider=self.llm_provider,\n                web_search_tool=web_search_tool,\n                spr_manager=self.spr_manager\n            )\n            self.ptrf_enabled = True\n            logger.info(\"âœ… Proactive Truth Resonance Framework is ONLINE.\")\n        except Exception as e:\n            logger.error(f\"âŒ Failed to initialize PTRF: {e}\", exc_info=True)\n            self.ptrf_enabled = False\n            self.truth_seeker = None\n            self.spr_manager = None\n\n    def _initialize_aco(self):\n        \"\"\"Initializes the Adaptive Cognitive Orchestrator.\"\"\"\n        try:\n            protocol_chunks = [\n                'Implementation Resonance refers to the alignment between conceptual understanding and operational implementation.',\n                'The ProportionalResonantControlPatterN eliminates oscillatory errors through resonant gain amplification.',\n                'Adaptive Cognitive Orchestrator enables meta-learning and pattern evolution in cognitive architectures.'\n            ]\n            self.aco = AdaptiveCognitiveOrchestrator(\n                protocol_chunks=protocol_chunks,\n                llm_provider=getattr(self, 'llm_provider', None) # Pass the stored LLM provider\n            )\n            self.autonomous_evolution_enabled = True\n            logger.info(\"âœ… Adaptive Cognitive Orchestrator is ONLINE.\")\n        except Exception as e:\n            logger.error(f\"âŒ Failed to initialize ACO: {e}\", exc_info=True)\n            self.autonomous_evolution_enabled = False\n            self.aco = None\n            \n    def _initialize_rise(self):\n        \"\"\"Initializes the RISE Orchestrator.\"\"\"\n        try:\n            workflows_dir = project_root / \"workflows\"\n            self.rise_orchestrator = RISE_Orchestrator(workflows_dir=str(workflows_dir))\n\n            # Attach event callback to forward SIRC events to websockets clients\n            def _event_sink(event_obj: Dict[str, Any]):\n                try:\n                    # Buffer events on the server instance for retrieval by websocket handler\n                    if not hasattr(self, '_event_queue'):\n                        self._event_queue = []\n                    self._event_queue.append(event_obj)\n                except Exception:\n                    pass\n\n            try:\n                self.rise_orchestrator.event_callback = _event_sink  # type: ignore\n            except Exception:\n                pass\n            self.rise_v2_enabled = True\n            logger.info(\"âœ… RISE v2.0 Genesis Protocol is ONLINE.\")\n        except Exception as e:\n            logger.error(f\"âŒ Failed to initialize RISE v2.0: {e}\", exc_info=True)\n            self.rise_v2_enabled = False\n            self.rise_orchestrator = None\n\n    def _initialize_autopoiesis(self):\n        \"\"\"Initializes the Autopoietic Governor and its dependencies.\"\"\"\n        try:\n            # Connect the global thought_trail instance to the global nexus_interface\n            self.thought_trail = ThoughtTrail() \n            nexus_interface.inject_thoughttrail(self.thought_trail)\n\n            # The insight engine needs to be defined/initialized, mocking for now\n            class MockInsightEngine: pass\n            self.insight_engine = MockInsightEngine()\n\n            self.governor = AutopoieticGovernor(\n                config=self.config, # Pass the loaded server config\n                thought_trail=self.thought_trail,\n                insight_engine=self.insight_engine,\n                spr_manager=self.spr_manager\n            )\n            \n            # Schedule the governor's self-audit\n            self.scheduler = AsyncIOScheduler()\n            audit_interval = self.governor.config.get(\"AUDIT_INTERVAL_MINUTES\", 60)\n            self.scheduler.add_job(\n                self.governor.perform_self_audit, \n                'interval', \n                minutes=audit_interval\n            )\n            self.scheduler.start()\n            \n            self.autopoiesis_enabled = self.governor.config.get(\"AUTOPOIESIS_ENABLED\", False)\n            if self.autopoiesis_enabled:\n                logger.info(f\"âœ… Autopoietic Governor is ONLINE and scheduled for audit every {audit_interval} minutes.\")\n            else:\n                logger.warning(\"Autopoiesis is DISABLED by configuration. The Governor is idle.\")\n        except Exception as e:\n            logger.error(f\"âŒ Failed to initialize Autopoietic Governor: {e}\", exc_info=True)\n            self.autopoiesis_enabled = False\n            self.governor = None\n\n\n    def _handle_cognitive_query_sync(self, query: str) -> Dict[str, Any]:\n        \"\"\"\n        Synchronous wrapper for handling queries through the ACO/RISE cognitive core.\n        This is the method that will be run in a separate thread.\n        \"\"\"\n        # This is a simplified version of the logic from mastermind/interact.py\n        # It determines the cognitive path and executes it.\n        try:\n            query_lower = query.lower()\n            \n            strategic_indicators = [\n                \"crisis\", \"conflicting\", \"ground truth\", \"predictive forecast\", \"geopolitical\",\n                \"strategic\", \"complex\", \"high-stakes\", \"Execution paradoX\"\n            ]\n            \n            is_strategic_query = any(indicator in query_lower for indicator in strategic_indicators)\n            \n            if is_strategic_query and self.rise_v2_enabled:\n                logger.info(f\"ðŸŽ¯ Strategic query detected. Routing to RISE engine.\")\n                rise_result = self.rise_orchestrator.run_rise_workflow(query)\n                return self._wrap_response_with_iar(rise_result, \"RISE\", query)\n            elif self.ptrf_enabled and any(k in query_lower for k in [\"truth\", \"fact\", \"verify\"]):\n                 logger.info(f\"ðŸŽ¯ Truth-seeking query detected. Routing to PTRF engine.\")\n                 ptrf_result = self.truth_seeker.seek_truth(query)\n                 return self._wrap_response_with_iar(ptrf_result, \"PTRF\", query)\n            elif self.autonomous_evolution_enabled:\n                logger.info(f\"ðŸŽ¯ Standard query. Routing to ACO for enhancement.\")\n                context, _ = self.aco.process_query_with_evolution(query)\n                return self._wrap_response_with_iar({\"response\": context}, \"ACO\", query)\n            else:\n                logger.warning(\"No cognitive core available for query.\")\n                return self._wrap_response_with_iar({\"error\": \"No cognitive core available.\"}, \"ERROR\", query)\n\n        except Exception as e:\n            logger.error(f\"Error in cognitive query handler: {e}\", exc_info=True)\n            return self._wrap_response_with_iar({\"error\": str(e)}, \"ERROR\", query)\n    \n    def _wrap_response_with_iar(self, response: Dict[str, Any], engine: str, query: str) -> Dict[str, Any]:\n        \"\"\"\n        Wrap the response with enhanced IAR (Integrated Action Reflection) data for VCD display.\n        Incorporates frontend's sophisticated cognitive assessment structure.\n        \"\"\"\n        import time\n        from datetime import datetime\n\n# ============================================================================\n# TEMPORAL CORE INTEGRATION (CANONICAL DATETIME SYSTEM)\n# ============================================================================\nfrom .temporal_core import now_iso, format_filename, format_log, Timer\n        \n        # Determine confidence based on response content\n        confidence = 0.9\n        status = \"Success\"\n        potential_issues = []\n        \n        if \"error\" in response:\n            confidence = 0.1\n            status = \"Failure\"\n            potential_issues.append(f\"Engine {engine} returned error\")\n        elif \"execution_status\" in response and response[\"execution_status\"] == \"failed\":\n            confidence = 0.3\n            status = \"Warning\"\n            potential_issues.append(\"Workflow execution failed\")\n        \n        # Enhanced cognitive resonance analysis\n        cognitive_resonance = self._analyze_cognitive_resonance(query, response)\n        temporal_resonance = self._analyze_temporal_resonance(query, response)\n        \n        # SPR detection using enhanced backend capabilities\n        spr_activations = self._detect_sprs_in_content(query, response)\n        \n        # Create enhanced IAR data with frontend sophistication\n        iar_data = {\n            \"status\": status,\n            \"confidence\": confidence,\n            \"summary\": f\"Query processed by {engine} engine\",\n            \"potential_issues\": potential_issues,\n            \"alignment_check\": {\n                \"query_relevance\": 0.9,\n                \"engine_selection\": 0.8,\n                \"response_quality\": confidence\n            },\n            \"engine_used\": engine,\n            \"timestamp\": now_iso() + \"Z\",\n            # Enhanced cognitive assessment\n            \"cognitive_resonance\": cognitive_resonance,\n            \"temporal_resonance\": temporal_resonance,\n            \"recommendations\": self._generate_iar_recommendations(cognitive_resonance, temporal_resonance),\n            \"confidence_boost\": max(0, 0.9 - confidence),\n            \"resonance_enhancement\": max(0, 0.8 - ((cognitive_resonance[\"clarity\"] + temporal_resonance[\"causal_understanding\"]) / 2))\n        }\n        \n        # Create enhanced message structure\n        enhanced_response = {\n            \"id\": f\"arche_{int(time.time() * 1000)}\",\n            \"content\": self._format_response_content(response),\n            \"timestamp\": now_iso() + \"Z\",\n            \"sender\": \"arche\",\n            \"message_type\": \"chat\",\n            \"protocol_compliance\": True,\n            \"protocol_version\": \"ResonantiA v3.1-CA\",\n            \"iar\": iar_data,\n            \"spr_activations\": spr_activations,  # Include SPR detection results\n            \"raw_response\": response  # Include original response for debugging\n        }\n        \n        return enhanced_response\n    \n    def _analyze_cognitive_resonance(self, query: str, response: Dict[str, Any]) -> Dict[str, float]:\n        \"\"\"Analyze cognitive resonance using frontend's sophisticated approach.\"\"\"\n        content = self._format_response_content(response)\n        \n        # Analyze clarity indicators\n        clarity_indicators = ['clear', 'specific', 'example', 'detail', 'explain']\n        clarity_score = self._calculate_indicator_score(content, clarity_indicators)\n        \n        # Analyze coherence indicators\n        coherence_indicators = ['logical', 'structure', 'flow', 'organized', 'systematic']\n        coherence_score = self._calculate_indicator_score(content, coherence_indicators)\n        \n        # Analyze completeness indicators\n        completeness_indicators = ['complete', 'comprehensive', 'thorough', 'detailed', 'full']\n        completeness_score = self._calculate_indicator_score(content, completeness_indicators)\n        \n        # Analyze contextual relevance\n        contextual_relevance = 0.9 if len(content) > 100 else 0.6\n        \n        return {\n            \"clarity\": min(0.9, 0.3 + clarity_score),\n            \"coherence\": min(0.9, 0.3 + coherence_score),\n            \"completeness\": min(0.9, 0.3 + completeness_score),\n            \"contextual_relevance\": contextual_relevance\n        }\n    \n    def _analyze_temporal_resonance(self, query: str, response: Dict[str, Any]) -> Dict[str, float]:\n        \"\"\"Analyze temporal resonance using frontend's 4D approach.\"\"\"\n        content = self._format_response_content(response)\n        \n        # Past context analysis\n        past_indicators = ['history', 'past', 'previous', 'before', 'earlier', 'tradition']\n        past_score = self._calculate_indicator_score(content, past_indicators)\n        \n        # Present accuracy analysis\n        present_indicators = ['current', 'now', 'present', 'today', 'contemporary', 'modern']\n        present_score = self._calculate_indicator_score(content, present_indicators)\n        \n        # Future projection analysis\n        future_indicators = ['future', 'will', 'shall', 'project', 'predict', 'forecast']\n        future_score = self._calculate_indicator_score(content, future_indicators)\n        \n        # Causal understanding analysis\n        causal_indicators = ['cause', 'effect', 'because', 'therefore', 'result', 'consequence']\n        causal_score = self._calculate_indicator_score(content, causal_indicators)\n        \n        return {\n            \"past_context\": min(0.9, 0.2 + past_score),\n            \"present_accuracy\": min(0.9, 0.3 + present_score),\n            \"future_projection\": min(0.9, 0.2 + future_score),\n            \"causal_understanding\": min(0.9, 0.3 + causal_score)\n        }\n    \n    def _calculate_indicator_score(self, content: str, indicators: List[str]) -> float:\n        \"\"\"Calculate score based on indicator presence in content.\"\"\"\n        if not content:\n            return 0.0\n        \n        lower_content = content.lower()\n        matches = sum(1 for indicator in indicators if indicator in lower_content)\n        return min(0.6, matches * 0.1)\n    \n    def _generate_iar_recommendations(self, cognitive_resonance: Dict[str, float], temporal_resonance: Dict[str, float]) -> List[str]:\n        \"\"\"Generate recommendations based on IAR analysis.\"\"\"\n        recommendations = []\n        \n        if cognitive_resonance[\"clarity\"] < 0.7:\n            recommendations.append(\"Enhance clarity by providing more specific examples\")\n        if cognitive_resonance[\"coherence\"] < 0.7:\n            recommendations.append(\"Improve logical flow and structure\")\n        if temporal_resonance[\"causal_understanding\"] < 0.6:\n            recommendations.append(\"Strengthen causal relationships and temporal context\")\n        if cognitive_resonance[\"completeness\"] < 0.7:\n            recommendations.append(\"Provide more comprehensive coverage of the topic\")\n        \n        return recommendations\n    \n    def _detect_sprs_in_content(self, query: str, response: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Detect SPRs in query and response content using enhanced backend capabilities.\"\"\"\n        try:\n            # Combine query and response content for SPR detection\n            content = f\"{query} {self._format_response_content(response)}\"\n            \n            # Use the enhanced SPR manager if available\n            if hasattr(self, 'spr_manager') and self.spr_manager:\n                return self.spr_manager.detect_sprs_with_confidence(content)\n            else:\n                # Fallback to basic detection\n                return self._basic_spr_detection(content)\n        except Exception as e:\n            logger.warning(f\"SPR detection failed: {e}\")\n            return []\n    \n    def _basic_spr_detection(self, content: str) -> List[Dict[str, Any]]:\n        \"\"\"Basic SPR detection fallback.\"\"\"\n        # Simple keyword-based detection\n        spr_keywords = {\n            'IntegratedActionReflection': ['IAR', 'action reflection', 'integrated reflection'],\n            'SparsePrimingRepresentations': ['SPR', 'sparse priming', 'priming representations'],\n            'VisualCognitiveDebugger': ['VCD', 'visual debugger', 'cognitive debugger'],\n            'ResonantInsightStrategyEngine': ['RISE', 'resonant insight', 'strategy engine'],\n            'SynergisticIntentResonanceCycle': ['SIRC', 'synergistic intent', 'resonance cycle']\n        }\n        \n        detected_sprs = []\n        lower_content = content.lower()\n        \n        for spr_id, keywords in spr_keywords.items():\n            for keyword in keywords:\n                if keyword.lower() in lower_content:\n                    detected_sprs.append({\n                        'spr_id': spr_id,\n                        'activation_level': 0.8,\n                        'confidence_score': 0.7,\n                        'guardian_point': spr_id,\n                        'matched_keyword': keyword\n                    })\n                    break  # Only add once per SPR\n        \n        return detected_sprs\n    \n    def _format_response_content(self, response: Dict[str, Any]) -> str:\n        \"\"\"\n        Format the response content for display in the VCD.\n        \"\"\"\n        if \"error\" in response:\n            return f\"Error: {response['error']}\"\n        elif \"final_strategy\" in response:\n            return f\"RISE Strategy Generated:\\n\\n{response.get('final_strategy', 'No strategy available')}\"\n        elif \"response\" in response:\n            return str(response[\"response\"])\n        else:\n            return json.dumps(response, indent=2, default=str)\n\n    async def websocket_handler(self, websocket):\n        \"\"\"Handles incoming WebSocket connections and messages.\"\"\"\n        logger.info(f\"ðŸ”— Client connected from {websocket.remote_address}\")\n        try:\n            async for message in websocket:\n                if message == \"ping\":\n                    await websocket.send(\"pong\")\n                    continue\n\n                query = str(message)\n                logger.info(f\"ðŸ“¥ Received query: {query[:150]}...\")\n                \n                try:\n                    loop = asyncio.get_running_loop()\n                    response = await loop.run_in_executor(\n                        self.executor, self._handle_cognitive_query_sync, query\n                    )\n                    \n                    # First stream any buffered SIRC events\n                    try:\n                        if hasattr(self, '_event_queue') and self._event_queue:\n                            for evt in self._event_queue:\n                                await websocket.send(json.dumps(evt))\n                            self._event_queue.clear()\n                    except Exception:\n                        pass\n                    \n                    response_json = json.dumps(response, default=str)\n                    logger.info(f\"ðŸ“¤ Sending response: {response_json[:200]}...\")\n                    await websocket.send(response_json)\n                    \n                except Exception as e:\n                    logger.error(f\"Error processing query: {e}\", exc_info=True)\n                    error_response = {\"error\": \"An error occurred while processing your request.\"}\n                    await websocket.send(json.dumps(error_response))\n        except websockets.exceptions.ConnectionClosed:\n            logger.info(f\"Connection closed for client {websocket.remote_address}\")\n        finally:\n            logger.info(\"WebSocket handler cleanup complete.\")\n\nasync def main():\n    \"\"\"Main function to start the Mastermind WebSocket server.\"\"\"\n    port_str = os.environ.get('ARCHE_PORT')\n    if not port_str or not port_str.isdigit():\n        logger.critical(f\"FATAL: ARCHE_PORT environment variable is not set or invalid. Got: '{port_str}'.\")\n        return\n\n    websocket_port = int(port_str)\n    host = \"0.0.0.0\"\n\n    server_instance = MastermindServer()\n    \n    # Start the Nexus WebSocket server in a background thread\n    nexus_interface.start_server_in_thread()\n    \n    logger.info(f\"ðŸš€ Attempting to start ArchE Mastermind Server on {host}:{websocket_port}\")\n    \n    try:\n        # We are not running a websocket server from here anymore, just the cognitive loop.\n        # This part of the code could be adapted to run a main cognitive loop\n        # or other primary server task. For now, we will just wait.\n        logger.info(\"âœ… ArchE Mastermind Server core is running.\")\n        logger.info(\"Nexus WebSocket bridge is running in a background thread.\")\n        await asyncio.Future() # Keep the main thread alive\n    except OSError as e:\n        logger.critical(f\"FATAL: Failed to start server components. Error: {e}\")\n    except Exception as e:\n        logger.critical(f\"FATAL: An unexpected error occurred: {e}\", exc_info=True)\n    finally:\n        logger.info(\"Shutting down Nexus server...\")\n        nexus_interface.stop()\n\nif __name__ == \"__main__\":\n    try:\n        asyncio.run(main())\n    except KeyboardInterrupt:\n        logger.info(\"ðŸ›‘ Server shutting down.\")\n    except Exception as e:\n        logger.critical(f\"ðŸ’¥ Server failed to start: {e}\", exc_info=True)\n\n```\n\nEXAMPLE APPLICATION:\nThe unified ArchE server, integrating the full cognitive core with a WebSocket interface.\n\nCATEGORY: CodeKnowledge\n\nRELATIONSHIPS:\ntype: FromCodebase; source: /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/mastermind_server.py; source_type: python_class"}