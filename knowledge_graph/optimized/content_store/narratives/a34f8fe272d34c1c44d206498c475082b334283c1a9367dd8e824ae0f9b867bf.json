{"content": "TERM: Free LLM Model Options for ArchE: Integration with ArchE\n\nDEFINITION:\nAll providers implement the `BaseLLMProvider` interface:\n\n```python\nfrom .llm_providers import get_llm_provider\n\n# Automatically works with any provider\nprovider = get_llm_provider(\"groq\")  # or \"ollama\", \"huggingface\", etc.\nresponse = provider.generate(prompt, model=\"llama-3.1-70b\")\n```\n\nThe ThoughtTrail decorator (`@log_to_thought_trail`) ensures all LLM calls are logged to the database with full IAR entries.\n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/specifications/free_model_options.md, type: specification_md\n\nFULL SPECIFICATION (free_model_options.md):\n# Free LLM Model Options for ArchE\n\n**Date**: 2025-10-28  \n**Purpose**: Document free alternatives to Google API for ArchE\n\n## Overview\n\nArchE currently uses Google's Gemini API, but free alternatives exist for development and cost control.\n\n## Option 1: Groq API (RECOMMENDED)\n\n**Why**: Fast inference, generous free tier, excellent for development\n\n### Setup\n\n```bash\npip install groq\n```\n\n### Configuration\n\nAdd to `Three_PointO_ArchE/llm_providers/groq.py`:\n\n```python\nimport groq\nfrom .base import BaseLLMProvider, LLMProviderError\n\nclass GroqProvider(BaseLLMProvider):\n    def __init__(self):\n        api_key = os.getenv(\"GROQ_API_KEY\")\n        if not api_key:\n            raise LLMProviderError(\"GROQ_API_KEY not set\")\n        self.client = groq.Groq(api_key=api_key)\n    \n    def generate(self, prompt: str, model: str = \"llama-3.1-70b-versatile\", **kwargs) -> str:\n        try:\n            response = self.client.chat.completions.create(\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                model=model,\n                **kwargs\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            raise LLMProviderError(f\"Groq API error: {e}\")\n```\n\n### Free Tier\n- Models: llama-3.1-70b, mixtral-8x7b\n- **Rate**: 14,400 requests/day (with auth)\n- **Speed**: Very fast (inference optimized)\n- **Registration**: https://console.groq.com\n\n### Model Selection\nUpdate `config.py`:\n\n```python\nllm_providers = {\n    \"groq\": {\n        \"api_key\": os.getenv(\"GROQ_API_KEY\"),\n        \"default_model\": \"llama-3.1-70b-versatile\",\n        \"temperature\": 0.7,\n        \"max_tokens\": 8192\n    }\n}\n```\n\n## Option 2: Ollama (Local, Completely Free)\n\n**Why**: Runs entirely on your machine, zero API costs\n\n### Setup\n\n```bash\n# Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Download model\nollama pull llama3\nollama pull mistral\n```\n\n### Configuration\n\nAdd to `Three_PointO_ArchE/llm_providers/ollama.py`:\n\n```python\nimport requests\nfrom .base import BaseLLMProvider, LLMProviderError\n\nclass OllamaProvider(BaseLLMProvider):\n    def __init__(self):\n        self.base_url = os.getenv(\"OLLAMA_BASE_URL\", \"http://localhost:11434\")\n    \n    def generate(self, prompt: str, model: str = \"llama3\", **kwargs) -> str:\n        try:\n            response = requests.post(\n                f\"{self.base_url}/api/generate\",\n                json={\n                    \"model\": model,\n                    \"prompt\": prompt,\n                    \"stream\": False\n                }\n            )\n            response.raise_for_status()\n            return response.json()[\"response\"]\n        except Exception as e:\n            raise LLMProviderError(f\"Ollama API error: {e}\")\n```\n\n### Models Available\n- `llama3` (8B params)\n- `llama3:70b` (70B params)\n- `mistral` (7B params)\n- `codellama` (code-specific)\n\n### Resources Required\n- **8B models**: ~8GB RAM minimum\n- **70B models**: ~40GB RAM or 2x GPU\n\n## Option 3: Hugging Face Inference API\n\n**Why**: Access to thousands of models, moderate free tier\n\n### Setup\n\n```bash\npip install huggingface_hub\n```\n\n### Configuration\n\n```python\nfrom huggingface_hub import InferenceClient\n\nclass HuggingFaceProvider(BaseLLMProvider):\n    def __init__(self):\n        self.client = InferenceClient(\n            token=os.getenv(\"HF_API_KEY\")\n        )\n    \n    def generate(self, prompt: str, model: str = \"meta-llama/Llama-3-8b\", **kwargs) -> str:\n        try:\n            response = self.client.text_generation(\n                prompt,\n                model=model,\n                max_new_tokens=kwargs.get(\"max_tokens\", 512)\n            )\n            return response\n        except Exception as e:\n            raise LLMProviderError(f\"HF API error: {e}\")\n```\n\n### Free Tier\n- **Requests**: Limited by model availability\n- **Models**: Community-hosted (varying performance)\n- **API Key**: https://huggingface.co/settings/tokens\n\n## Option 4: Together AI (Free Credits)\n\n**Why**: Simple API, free startup credits\n\n### Setup\n\n```bash\npip install together\n```\n\n### Configuration\n\n```python\nimport together\n\nclass TogetherProvider(BaseLLMProvider):\n    def __init__(self):\n        api_key = os.getenv(\"TOGETHER_API_KEY\")\n        together.api_key = api_key\n    \n    def generate(self, prompt: str, model: str = \"meta-llama/Llama-3-8b-chat-hf\", **kwargs) -> str:\n        try:\n            response = together.Complete.create(\n                prompt=prompt,\n                model=model,\n                **kwargs\n            )\n            return response[\"output\"][\"choices\"][0][\"text\"]\n        except Exception as e:\n            raise LLMProviderError(f\"Together API error: {e}\")\n```\n\n### Free Credits\n- **New users**: $25 free credits\n- **Registration**: https://together.ai\n\n## Implementation Priority\n\n1. **Groq** - Fastest to implement, best free tier\n2. **Ollama** - If you want local/offline capability\n3. **HuggingFace** - For experimentation with many models\n4. **Together AI** - Simple, good for startups\n\n## Integration with ArchE\n\nAll providers implement the `BaseLLMProvider` interface:\n\n```python\nfrom .llm_providers import get_llm_provider\n\n# Automatically works with any provider\nprovider = get_llm_provider(\"groq\")  # or \"ollama\", \"huggingface\", etc.\nresponse = provider.generate(prompt, model=\"llama-3.1-70b\")\n```\n\nThe ThoughtTrail decorator (`@log_to_thought_trail`) ensures all LLM calls are logged to the database with full IAR entries.\n\n## Next Steps\n\n1. Choose a provider (recommend **Groq**)\n2. Add the provider implementation to `llm_providers/`\n3. Register it in `llm_providers/__init__.py`\n4. Set environment variable (e.g., `GROQ_API_KEY`)\n5. Update `config.py` to add the provider\n6. Test with `generate_text_llm` action\n\n## Cost Comparison\n\n| Provider | Monthly Cost | Speed | Quality |\n|----------|--------------|-------|---------|\n| **Groq** (free tier) | $0 | ⚡⚡⚡ Fast | ⭐⭐⭐ Good |\n| **Ollama** (local) | $0 | ⚡⚡ Medium | ⭐⭐⭐ Good |\n| **HuggingFace** (free) | $0 | ⚡ Slow | ⭐⭐ Varies |\n| **Together AI** (credits) | $0-$10 | ⚡⚡ Medium | ⭐⭐⭐ Good |\n| **Google Gemini** | $0-$20+ | ⚡⚡ Medium | ⭐⭐⭐⭐ Excellent |\n\n---\n\n**Note**: All LLM actions are now logged to ThoughtTrail via `@log_to_thought_trail` decorator on `generate_text_llm`.\n\n\n\nEXAMPLE APPLICATION:\nAll providers implement the `BaseLLMProvider` interface:\n\n```python\nfrom .llm_providers import get_llm_provider\n\n# Automatically works with any provider\nprovider = get_llm_provider(\"groq\")  # or \"ollama\", \"huggingface\", etc.\nresponse = provider.generate(prompt, model=\"llama-3.1-70b\")\n```\n\nThe ThoughtTrail decorator (`@log_to_thought_trail`) ensures all LLM calls are logged to the database with full IAR entries.\n\nCATEGORY: SpecificationKnowledge\n\nRELATIONSHIPS:\ntype: FromCodebase; source: /mnt/3626C55326C514B1/Happier/specifications/free_model_options.md; source_type: specification_md"}