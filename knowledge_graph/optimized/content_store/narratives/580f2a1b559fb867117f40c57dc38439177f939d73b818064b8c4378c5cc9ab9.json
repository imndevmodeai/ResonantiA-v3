{"content": "TERM: DoWhy\n\nDEFINITION:\nA Python library for causal inference that enables the identification of cause-and-effect relationships in data. It provides tools for causal discovery, estimation, and validation beyond mere correlation analysis.\n\nBLUEPRINT DETAILS:\nDoWhy library integration in Three_PointO_ArchE/causal_inference_tool.py with perform_causal_inference() function using DoWhy for causal discovery and estimation.\n\nIMPLEMENTATION CODE (causal_inference_tool.py) - First 30KB:\n```python\n# --- START OF FILE 3.0ArchE/causal_inference_tool.py ---\n# ResonantiA Protocol v3.0 - causal_inference_tool.py\n# Implements Causal Inference capabilities with Temporal focus (Conceptual/Simulated).\n# Requires integration with libraries like DoWhy, statsmodels, Tigramite, causal-learn.\n# Returns results including mandatory Integrated Action Reflection (IAR).\n\nimport json\nimport logging\nimport pandas as pd\nimport numpy as np\nimport time\nimport networkx as nx # For graph representation if needed\nfrom typing import Dict, Any, Optional, List, Union, Tuple # Expanded type hints\nimport re\n# Use relative imports for configuration\ntry:\n    from . import config\n    from .thought_trail import log_to_thought_trail\nexcept ImportError:\n    # Fallback config if running standalone or package structure differs\n    class FallbackConfig: CAUSAL_DEFAULT_DISCOVERY_METHOD=\"PC\"; CAUSAL_DEFAULT_ESTIMATION_METHOD=\"backdoor.linear_regression\"; CAUSAL_DEFAULT_TEMPORAL_METHOD=\"Granger\"\n    config = FallbackConfig(); logging.warning(\"config.py not found for causal tool, using fallback configuration.\")\n\n# --- Import Causal Libraries (Set flag based on success) ---\nCAUSAL_LIBS_AVAILABLE = False\nDOWHY_AVAILABLE = False\nSTATSMODELS_AVAILABLE = False\n# Add flags for causal-learn, tigramite if implementing those discovery methods\ntry:\n    import dowhy\n    from dowhy import CausalModel\n    DOWHY_AVAILABLE = True\n    import statsmodels.api as sm # For Granger, VAR models\n    from statsmodels.tsa.stattools import grangercausalitytests\n    from statsmodels.tsa.api import VAR # For lagged effects estimation\n    STATSMODELS_AVAILABLE = True\n\n    CAUSAL_LIBS_AVAILABLE = DOWHY_AVAILABLE and STATSMODELS_AVAILABLE # Set based on core libs needed for implemented features\n    log_msg = \"Actual causal inference libraries loaded: \"\n    if DOWHY_AVAILABLE: log_msg += \"DoWhy, \"\n    if STATSMODELS_AVAILABLE: log_msg += \"statsmodels\"\n    logging.getLogger(__name__).info(log_msg.strip(', '))\n\nexcept ImportError as e_imp:\n    logging.getLogger(__name__).warning(f\"Causal libraries import failed: {e_imp}. Causal Inference Tool functionality will be limited or simulated.\")\nexcept Exception as e_imp_other:\n    logging.getLogger(__name__).error(f\"Unexpected error importing causal libraries: {e_imp_other}. Tool simulating.\")\n\nlogger = logging.getLogger(__name__)\n\n# --- IAR Helper Function ---\n# (Reused for consistency)\ndef _create_reflection(status: str, summary: str, confidence: Optional[float], alignment: Optional[str], issues: Optional[List[str]], preview: Any) -> Dict[str, Any]:\n    \"\"\"Helper function to create the standardized IAR reflection dictionary.\"\"\"\n    if confidence is not None: confidence = max(0.0, min(1.0, confidence))\n    issues_list = issues if issues else None\n    try:\n        preview_str = json.dumps(preview, default=str) if isinstance(preview, (dict, list)) else str(preview)\n        if preview_str and len(preview_str) > 150: preview_str = preview_str[:150] + \"...\"\n    except Exception:\n        try: preview_str = str(preview); preview_str = preview_str[:150] + \"...\" if len(preview_str) > 150 else preview_str\n        except Exception: preview_str = \"[Preview Error]\"\n    return {\"status\": status, \"summary\": summary, \"confidence\": confidence, \"alignment_check\": alignment if alignment else \"N/A\", \"potential_issues\": issues_list, \"raw_output_preview\": preview_str}\n\n# --- Data Preparation Helper ---\n# (Similar to predictive tool, but might need different handling)\ndef _prepare_causal_data(data: Union[Dict, pd.DataFrame]) -> Tuple[Optional[pd.DataFrame], Optional[str]]:\n    \"\"\"Converts input data to DataFrame and performs basic validation.\"\"\"\n    df: Optional[pd.DataFrame] = None\n    error_msg: Optional[str] = None\n    try:\n        if isinstance(data, dict):\n            df = pd.DataFrame(data)\n        elif isinstance(data, pd.DataFrame):\n            df = data.copy()\n        else:\n            error_msg = f\"Invalid 'data' type: {type(data)}. Expected dict or DataFrame.\"\n            return None, error_msg\n\n        if df.empty:\n            error_msg = \"Input data is empty.\"\n            return None, error_msg\n\n        # Basic check for non-numeric types that might cause issues\n        if df.select_dtypes(include=[object]).shape[1] > 0: # Corrected to check number of object columns\n            logger.warning(\"Input data contains object columns. Ensure categorical variables are properly encoded for the chosen causal method.\")\n\n        return df, None # Return DataFrame and no error\n    except Exception as e_prep:\n        error_msg = f\"Causal data preparation failed: {e_prep}\"\n        logger.error(error_msg, exc_info=True)\n        return None, error_msg\n\n# --- Main Tool Function ---\n@log_to_thought_trail\ndef perform_causal_inference(operation: str, **kwargs) -> Dict[str, Any]:\n    \"\"\"\n    [IAR Enabled] Main wrapper for causal inference operations (Static & Temporal).\n    Dispatches to specific implementation or simulation based on 'operation'.\n    Implements DoWhy estimation and Granger causality.\n\n    Args:\n        operation (str): The causal operation to perform (e.g., 'discover_graph',\n                        'estimate_effect', 'run_granger_causality',\n                        'discover_temporal_graph', 'estimate_lagged_effects',\n                        'convert_to_state'). Required.\n        **kwargs: Arguments specific to the operation (e.g., data, treatment, outcome,\n                  confounders, target_column, max_lag, method, causal_result).\n\n    Returns:\n        Dict[str, Any]: Dictionary containing results and IAR reflection.\n    \"\"\"\n    # --- Initialize Results & Reflection ---\n    primary_result = {\"operation_performed\": operation, \"error\": None, \"libs_available\": CAUSAL_LIBS_AVAILABLE, \"note\": \"\"}\n    reflection_status = \"Failure\"; reflection_summary = f\"Causal op '{operation}' init failed.\"; confidence = 0.0; alignment = \"N/A\"; issues = [\"Initialization error.\"]; preview = None\n\n    logger.info(f\"Performing causal inference operation: '{operation}'\")\n\n    # --- Simulation Mode Check (If core libs needed for operation are missing) ---\n    needs_dowhy = operation in ['estimate_effect']\n    needs_statsmodels = operation in ['run_granger_causality', 'estimate_lagged_effects']\n    libs_needed_for_operation = (needs_dowhy and not DOWHY_AVAILABLE) or (needs_statsmodels and not STATSMODELS_AVAILABLE)\n\n    # Graph discovery is always simulated for now, or if libs are missing for it\n    is_simulated_op = operation in ['discover_graph', 'discover_temporal_graph'] or libs_needed_for_operation\n\n    if is_simulated_op:\n        missing_libs_names = []\n        if needs_dowhy and not DOWHY_AVAILABLE: missing_libs_names.append(\"DoWhy\")\n        if needs_statsmodels and not STATSMODELS_AVAILABLE: missing_libs_names.append(\"statsmodels\")\n        libs_str = \", \".join(missing_libs_names) if missing_libs_names else \"N/A (operation simulated by design)\"\n        sim_reason = f\"Missing libs: {libs_str}\" if libs_needed_for_operation else \"Operation simulated by design\"\n        logger.warning(f\"Simulating causal inference operation '{operation}'. Reason: {sim_reason}.\")\n        primary_result[\"note\"] = f\"SIMULATED result ({sim_reason})\"\n        sim_result = _simulate_causal_inference(operation, **kwargs)\n        primary_result.update(sim_result)\n        primary_result[\"error\"] = sim_result.get(\"error\", primary_result.get(\"error\"))\n        if primary_result[\"error\"]:\n            reflection_status = \"Failure\"; reflection_summary = f\"Simulated causal op '{operation}' failed: {primary_result['error']}\"; confidence = 0.1; issues = [primary_result['error']]\n        else:\n            reflection_status = \"Success\"; reflection_summary = f\"Simulated causal op '{operation}' completed.\"; confidence = 0.6; alignment = \"Aligned with causal analysis goal (simulated).\"; issues = [\"Result is simulated.\"]; preview = {k:v for k,v in primary_result.items() if k not in ['operation_performed','error','libs_available','note']}\n        return {**primary_result, \"reflection\": _create_reflection(reflection_status, reflection_summary, confidence, alignment, issues, preview)}\n\n    # --- Actual Implementation Dispatch ---\n    try:\n        op_result: Dict[str, Any] = {} # Store result from the specific operation function\n\n        # --- Operation Specific Logic ---\n        # Note: discover_graph and discover_temporal_graph fall through to simulation above\n        if operation == 'estimate_effect':\n            op_result = _estimate_effect(**kwargs)\n        elif operation == 'run_granger_causality':\n            op_result = _run_granger_causality(**kwargs)\n        elif operation == 'estimate_lagged_effects':\n            op_result = _estimate_lagged_effects(**kwargs)\n        elif operation == 'convert_to_state': # Added for consistency with predictive tool\n            op_result = _convert_to_state_vector(**kwargs)\n        else:\n            # This case should ideally be caught by simulation check, but as a safeguard:\n            primary_result[\"error\"] = f\"Unsupported or unsimulated causal operation: {operation}\"\n\n        # --- Update primary_result and IAR from operation function --- \n        primary_result.update(op_result) # Merge results from the specific function\n        # The specific operation function is responsible for setting its own IAR fields\n        # So we extract them from op_result if present\n        if \"reflection\" in op_result:\n            reflection_data = op_result.pop(\"reflection\") # Remove it from op_result to avoid nesting\n            reflection_status = reflection_data.get(\"status\", reflection_status)\n            reflection_summary = reflection_data.get(\"summary\", reflection_summary)\n            confidence = reflection_data.get(\"confidence\", confidence)\n            alignment = reflection_data.get(\"alignment_check\", alignment)\n            issues = reflection_data.get(\"potential_issues\", issues)\n            preview = reflection_data.get(\"raw_output_preview\", preview)\n        else: # If no reflection provided by sub-function, create a basic one\n            if primary_result.get(\"error\"):\n                reflection_status = \"Failure\"\n                reflection_summary = f\"Causal op '{operation}' failed: {primary_result['error']}\"\n                confidence = 0.1\n                issues = [primary_result['error']]\n            else:\n                reflection_status = \"Success\"\n                reflection_summary = f\"Causal op '{operation}' completed.\"\n                confidence = 0.7 # Default confidence if not specified by op_result\n                alignment = \"Aligned with causal analysis goal.\"\n                preview = {k:v for k,v in primary_result.items() if k not in ['operation_performed','error','libs_available','note']}\n\n    except Exception as e_main:\n        logger.error(f\"Error executing causal operation '{operation}': {e_main}\", exc_info=True)\n        primary_result[\"error\"] = f\"Causal operation execution error: {e_main}\"\n        reflection_status = \"Failure\"; reflection_summary = f\"Causal op '{operation}' failed: {e_main}\"; confidence = 0.0; alignment = \"Failed due to system error.\"; issues = [f\"System Error: {e_main}\"]\n\n    # --- Final Return --- \n    return {**primary_result, \"reflection\": _create_reflection(reflection_status, reflection_summary, confidence, alignment, issues, preview)}\n\n\n# --- Specific Causal Operation Implementations ---\n\ndef _estimate_effect(**kwargs) -> Dict[str, Any]:\n    \"\"\"[IAR Enabled] Estimates causal effect using DoWhy.\"\"\"\n    result: Dict[str, Any] = {\"error\": None}\n    status = \"Failure\"; summary = \"DoWhy estimation init failed.\"; confidence = 0.0; alignment = \"N/A\"; issues = [\"Initialization error.\"]; preview = None\n\n    if not DOWHY_AVAILABLE:\n        result[\"error\"] = \"DoWhy library not available for effect estimation.\"\n        issues = [result[\"error\"]]; summary = result[\"error\"]\n        return {**result, \"reflection\": _create_reflection(status, summary, confidence, alignment, issues, preview)}\n\n    # --- Input Extraction & Validation ---\n    data = kwargs.get(\"data\")\n    treatment_name = kwargs.get(\"treatment\")\n    outcome_name = kwargs.get(\"outcome\")\n    graph_dot_str = kwargs.get(\"graph\") # DOT string format\n    confounder_names = kwargs.get(\"confounders\", []) # List of confounder column names\n    estimation_method = kwargs.get(\"method\", getattr(config, 'CAUSAL_DEFAULT_ESTIMATION_METHOD', \"backdoor.linear_regression\"))\n\n    df, prep_error = _prepare_causal_data(data)\n    if prep_error or df is None:\n        result[\"error\"] = f\"Data preparation error: {prep_error}\"\n        issues = [result[\"error\"]]; summary = result[\"error\"]\n        return {**result, \"reflection\": _create_reflection(status, summary, confidence, alignment, issues, preview)}\n\n    if not treatment_name or treatment_name not in df.columns:\n        result[\"error\"] = f\"Treatment '{treatment_name}' not found in data columns.\"\n        issues = [result[\"error\"]]; summary = result[\"error\"]\n        return {**result, \"reflection\": _create_reflection(status, summary, confidence, alignment, issues, preview)}\n    if not outcome_name or outcome_name not in df.columns:\n        result[\"error\"] = f\"Outcome '{outcome_name}' not found in data columns.\"\n        issues = [result[\"error\"]]; summary = result[\"error\"]\n        return {**result, \"reflection\": _create_reflection(status, summary, confidence, alignment, issues, preview)}\n    missing_confounders = [c for c in confounder_names if c not in df.columns]\n    if missing_confounders:\n        result[\"error\"] = f\"Confounder(s) not found in data: {missing_confounders}\"\n        issues = [result[\"error\"]]; summary = result[\"error\"]\n        return {**result, \"reflection\": _create_reflection(status, summary, confidence, alignment, issues, preview)}\n\n    # --- DoWhy Estimation --- \n    try:\n        model_args = {\"data\": df, \"treatment\": treatment_name, \"outcome\": outcome_name}\n        if graph_dot_str:\n            model_args[\"graph\"] = graph_dot_str\n            logger.info(f\"Using provided causal graph for DoWhy model.\")\n        elif confounder_names:\n            model_args[\"common_causes\"] = confounder_names\n            logger.info(f\"Using provided common_causes (confounders) for DoWhy model as no graph was given.\")\n        else:\n            logger.warning(\"No causal graph or explicit confounders provided for DoWhy. Estimation might be biased if unobserved confounders exist.\")\n            issues.append(\"Warning: No graph or confounders specified; results may be biased.\")\n\n        model = CausalModel(**model_args)\n        \n        identified_estimand = model.identify_effect(proceed_when_unidentifiable=True)\n        logger.info(f\"DoWhy identified estimand object: {identified_estimand}\")\n        logger.info(f\"DoWhy identified_estimand attributes: {dir(identified_estimand)}\")\n\n        estimate = model.estimate_effect(\n            identified_estimand,\n            method_name=estimation_method,\n            test_significance=True,\n            confidence_intervals=True\n        )\n        logger.debug(f\"DoWhy estimate object: {estimate}\")\n        logger.debug(f\"dir(estimate): {dir(estimate)}\") # Log attributes of estimate object\n        \n        # Attempt to get p-value from test_stat_significance\n        p_val = None\n        if hasattr(estimate, \"test_stat_significance\"):\n            sig_results = estimate.test_stat_significance\n            logger.debug(f\"estimate.test_stat_significance: {sig_results} (type: {type(sig_results)})\")\n            if isinstance(sig_results, dict) and \"p_value\" in sig_results:\n                try:\n                    p_val_raw = sig_results[\"p_value\"]\n                    # p_value might be an array/list or a scalar\n                    if isinstance(p_val_raw, (list, np.ndarray)) and len(p_val_raw) > 0:\n                        p_val = float(p_val_raw[0])\n                    elif isinstance(p_val_raw, (float, int)):\n                        p_val = float(p_val_raw)\n                    logger.debug(f\"P-value from test_stat_significance: {p_val}\")\n                except (TypeError, ValueError, IndexError) as e:\n                    logger.warning(f\"Could not extract/convert p-value from test_stat_significance: {e}\")\n        else:\n            logger.debug(\"estimate has no attribute 'test_stat_significance'\")\n\n        # Fallback: Parse from string representation if p_val is still None\n        if p_val is None:\n            logger.debug(\"P-value not found via test_stat_significance, trying to parse from str(estimate).\")\n            try:\n                estimate_str = str(estimate)\n                match = re.search(r\"p-value: \\s*\\[?([0-9\\.eE\\-]+)\\s*\\]?\", estimate_str) # handle optional brackets\n                if match:\n                    p_val_str = match.group(1)\n                    p_val = float(p_val_str)\n                    logger.info(f\"Successfully parsed p-value ({p_val}) from string representation of estimate object.\")\n                else:\n                    logger.warning(f\"Could not find p-value pattern in str(estimate): {estimate_str[:500]}\") # Log part of string if pattern fails\n            except Exception as e_parse:\n                logger.warning(f\"Failed to parse p-value from string representation: {e_parse}\")\n\n        result[\"p_value\"] = p_val\n        logger.debug(f\"Final p_value set in result: {result['p_value']}\")\n        \n        result[\"estimated_effect\"] = estimate.value\n        result[\"estimand_type\"] = identified_estimand.estimand_type.name\n        \n        estimand_str = \"N/A\"\n        if hasattr(identified_estimand, 'estimands') and 'backdoor' in identified_estimand.estimands and identified_estimand.estimands['backdoor']:\n            if isinstance(identified_estimand.estimands['backdoor'], dict) and 'estimand' in identified_estimand.estimands['backdoor']:\n                estimand_str = str(identified_estimand.estimands['backdoor']['estimand'])\n            elif hasattr(identified_estimand.estimands['backdoor'], 'estimand_expression'):\n                 estimand_str = str(identified_estimand.estimands['backdoor'].estimand_expression)\n            else:\n                estimand_str = str(identified_estimand.estimands['backdoor'])\n        elif hasattr(identified_estimand, 'estimand_expression'):\n             estimand_str = str(identified_estimand.estimand_expression)\n        else:\n            estimand_str = str(identified_estimand)\n            if \"Estimand expression:\" in estimand_str:\n                try:\n                    estimand_str = estimand_str.split(\"Estimand expression:\")[1].split(\"\\\\n\\\\n\")[0].strip()\n                except IndexError:\n                    pass \n        result[\"estimand_expression\"] = estimand_str\n\n        result[\"method_used\"] = estimation_method\n        if hasattr(estimate, 'params'):\n            try: result[\"estimator_params\"] = json.loads(json.dumps(estimate.params, default=str))\n            except: result[\"estimator_params\"] = str(estimate.params)\n        if hasattr(estimate, 'control_value'): result[\"control_value\"] = estimate.control_value\n        if hasattr(estimate, 'treatment_value'): result[\"treatment_value\"] = estimate.treatment_value\n        \n        # Corrected confounder logic\n        confounders_used_set = set()\n        if hasattr(identified_estimand, 'backdoor_variables') and identified_estimand.backdoor_variables:\n            bv = identified_estimand.backdoor_variables\n            logger.debug(f\"Raw identified_estimand.backdoor_variables: {bv} (type: {type(bv)})\")\n            if isinstance(bv, dict):\n                for val_list in bv.values():\n                    if isinstance(val_list, list):\n                        for item in val_list:\n                            confounders_used_set.add(str(item))\n                    else:\n                        confounders_used_set.add(str(val_list))\n            elif isinstance(bv, list):\n                for item in bv:\n                    if isinstance(item, (list, tuple)):\n                        confounders_used_set.update([str(i) for i in item])\n                    else:\n                        confounders_used_set.add(str(item))\n            else:\n                 confounders_used_set.add(str(bv))\n        elif hasattr(identified_estimand, 'common_causes') and identified_estimand.common_causes: # Fallback\n            cc_vars = identified_estimand.common_causes\n            logger.debug(f\"Raw identified_estimand.common_causes: {cc_vars} (type: {type(cc_vars)})\")\n            confounders_used_set.update([str(cc) for cc in cc_vars])\n        result[\"confounders_identified_used\"] = sorted(list(confounders_used_set))\n        logger.debug(f\"Processed confounders_identified_used: {result['confounders_identified_used']}\")\n\n        result[\"assumptions\"] = str(identified_estimand.assumptions) if hasattr(identified_estimand, 'assumptions') else \"N/A\"\n        \n        result[\"confidence_intervals\"] = estimate.confidence_interval if hasattr(estimate, \"confidence_interval\") else None\n        \n        status = \"Success\"\n        summary = f\"DoWhy causal effect estimation completed for {treatment_name} -> {outcome_name}.\"\n        confidence = 0.75 \n        if p_val is not None and p_val > 0.05: confidence = max(0.3, confidence - 0.2)\n        alignment = \"Aligned with causal effect estimation goal.\"\n        preview = {\"estimated_effect\": result[\"estimated_effect\"], \"p_value\": result.get(\"p_value\")} \n\n    except Exception as e_dowhy:\n        logger.error(f\"DoWhy effect estimation failed: {e_dowhy}\", exc_info=True)\n        result[\"error\"] = f\"DoWhy estimation error: {e_dowhy}\"\n        status = \"Failure\"; summary = result[\"error\"]; confidence = 0.1; issues.append(result[\"error\"])\n\n    return {**result, \"reflection\": _create_reflection(status, summary, confidence, alignment, issues, preview)}\n\n\ndef _run_granger_causality(**kwargs) -> Dict[str, Any]:\n    \"\"\"[IAR Enabled] Performs Granger causality tests between time series columns.\"\"\"\n    result: Dict[str, Any] = {\"error\": None, \"granger_results\": {}}\n    status = \"Failure\"; summary = \"Granger causality init failed.\"; confidence = 0.0; alignment = \"N/A\"; issues = [\"Initialization error.\"]; preview = None\n\n    if not STATSMODELS_AVAILABLE:\n        result[\"error\"] = \"Statsmodels library not available for Granger causality.\"\n        issues = [result[\"error\"]]; summary = result[\"error\"]\n        return {**result, \"reflection\": _create_reflection(status, summary, confidence, alignment, issues, preview)}\n\n    # --- Input Extraction & Validation ---\n    data = kwargs.get(\"data\")\n    max_lag = kwargs.get(\"max_lag\", 5) # Default max lag\n    # target_column: The variable whose causes are being investigated (Y in X -> Y)\n    target_column = kwargs.get(\"target_column\")\n    # predictor_columns: List of variables to test as potential causes of target_column (X_i in X_i -> Y)\n    # If None, test all other numeric columns against target_column.\n    predictor_columns = kwargs.get(\"predictor_columns\")\n    significance_level = kwargs.get(\"significance_level\", 0.05) # Default alpha\n    # Test types: 'ssr_chi2test', 'ssr_ftest', 'lrtest', 'params_ftest'\n    # Default to F-test which is common\n    granger_test_type = kwargs.get(\"test_type\", \"ssr_ftest\")\n\n    df, prep_error = _prepare_causal_data(data)\n    if prep_error or df is None:\n        result[\"error\"] = f\"Data preparation error: {prep_error}\"\n        issues = [result[\"error\"]]; summary = result[\"error\"]\n        return {**result, \"reflection\": _create_reflection(status, summary, confidence, alignment, issues, preview)}\n\n    try: max_lag = int(max_lag); assert max_lag > 0\n    except: max_lag = 5; logger.warning(f\"Invalid max_lag, defaulting to {max_lag}.\")\n\n    if not target_column or target_column not in df.columns:\n        result[\"error\"] = f\"Target column '{target_column}' not found in data.\"\n        issues = [result[\"error\"]]; summary = result[\"error\"]\n        return {**result, \"reflection\": _create_reflection(status, summary, confidence, alignment, issues, preview)}\n\n    # Ensure target column is numeric\n    if not pd.api.types.is_numeric_dtype(df[target_column]):\n        result[\"error\"] = f\"Target column '{target_column}' must be numeric for Granger causality.\"\n        issues = [result[\"error\"]]; summary = result[\"error\"]\n        return {**result, \"reflection\": _create_reflection(status, summary, confidence, alignment, issues, preview)}\n\n    # Determine predictor columns\n    if predictor_columns is None:\n        # Use all other numeric columns as potential predictors\n        predictor_columns = [col for col in df.columns if col != target_column and pd.api.types.is_numeric_dtype(df[col])]\n        if not predictor_columns:\n            result[\"error\"] = \"No suitable numeric predictor columns found in data (excluding target).\"\n            issues = [result[\"error\"]]; summary = result[\"error\"]\n            return {**result, \"reflection\": _create_reflection(status, summary, confidence, alignment, issues, preview)}\n    else:\n        # Validate provided predictor columns\n        missing_preds = [p for p in predictor_columns if p not in df.columns]\n        if missing_preds: result[\"error\"] = f\"Predictor column(s) not found: {missing_preds}\"; issues = [result[\"error\"]]; summary = result[\"error\"]\n        non_numeric_preds = [p for p in predictor_columns if p in df.columns and not pd.api.types.is_numeric_dtype(df[p])]\n        if non_numeric_preds: result[\"error\"] = f\"Predictor column(s) must be numeric: {non_numeric_preds}\"; issues = [result[\"error\"]]; summary = result[\"error\"]\n        if result[\"error\"]: return {**result, \"reflection\": _create_reflection(status, summary, confidence, alignment, issues, preview)}\n\n    # --- Perform Granger Causality Tests --- \n    test_results_dict: Dict[str, Any] = {}\n    any_significant = False\n    # Ensure the specific exception is importable or defined\n    try:\n        from statsmodels.tools.sm_exceptions import InfeasibleTestError\n    except ImportError:\n        # Define a placeholder if not importable, so `except` block doesn't fail\n        class InfeasibleTestError(Exception):\n            pass\n\n    try:\n        for pred_col in predictor_columns:\n            if pred_col == target_column: continue # Skip testing column against itself\n\n            logger.info(f\"Running Granger causality: '{pred_col}' -> '{target_column}' (max_lag={max_lag}) Session ID: {getattr(config, 'SESSION_ID', 'N/A')[:8]}\")\n            pair_df = df[[target_column, pred_col]].dropna()\n            if len(pair_df) < max_lag + 5: # Heuristic: Need enough data points relative to lag\n                logger.warning(f\"Skipping Granger test for {pred_col} -> {target_column} due to insufficient data after dropna (len={len(pair_df)}, max_lag={max_lag}).\")\n                test_results_dict[f\"{pred_col}_to_{target_column}\"] = {\"skipped\": True, \"reason\": \"Insufficient data after dropna for max_lag.\", \"p_value\": None, \"is_significant\": False, \"error\": None}\n                continue\n\n            p_value = None\n            current_pair_error = None\n            lag_results_dict = {} # Initialize here\n            try:\n                granger_test_output = grangercausalitytests(pair_df[[target_column, pred_col]], maxlag=[max_lag], verbose=False)\n                if max_lag in granger_test_output and len(granger_test_output[max_lag]) > 0:\n                    lag_results_dict = granger_test_output[max_lag][0]\n                    if granger_test_type in lag_results_dict:\n                        p_value = lag_results_dict[granger_test_type][1]\n                    else:\n                        logger.warning(f\"Chosen Granger test_type '{granger_test_type}' not found in results for {pred_col} -> {target_column} at lag {max_lag}. Trying F-test.\")\n                        if \"ssr_ftest\" in lag_results_dict:\n                            p_value = lag_results_dict[\"ssr_ftest\"][1]\n            except InfeasibleTestError as e_infeasible:\n                logger.warning(f\"Granger test infeasible for {pred_col} -> {target_column}: {e_infeasible}\")\n                current_pair_error = f\"InfeasibleTestError: {e_infeasible}\"\n            except Exception as e_pair_test:\n                logger.error(f\"Error during Granger test for {pred_col} -> {target_column}: {e_pair_test}\", exc_info=True)\n                current_pair_error = str(e_pair_test)\n\n            is_significant = p_value is not None and p_value < significance_level\n            if is_significant: any_significant = True\n\n            test_results_dict[f\"{pred_col}_to_{target_column}\"] = {\n                \"p_value\": p_value,\n                \"is_significant_at_{significance_level}\": is_significant,\n                \"max_lag_tested\": max_lag,\n                \"test_type_used\": granger_test_type if p_value is not None else (\"ssr_ftest\" if \"ssr_ftest\" in lag_results_dict and not current_pair_error else \"N/A\"),\n                \"error\": current_pair_error\n            }\n\n        result[\"granger_results\"] = test_results_dict\n        status = \"Success\"\n        summary = f\"Granger causality tests completed for target '{target_column}'.\"\n        confidence = 0.7 # Granger implies correlation, not deep causation. Requires assumptions.\n        alignment = \"Aligned with temporal causal influence detection goal.\"\n        issues.append(\"Granger causality indicates temporal precedence, not true causal effect without strong assumptions (e.g., no unobserved confounders, correct lag). Stationarity of series is assumed.\")\n        if not any_significant: summary += \" No significant Granger causes found at this lag/alpha.\"\n        else: summary += \" Significant Granger cause(s) identified.\"\n        preview = test_results_dict\n\n    except Exception as e_granger:\n        logger.error(f\"Granger causality test failed: {e_granger}\", exc_info=True)\n        result[\"error\"] = f\"Granger causality error: {e_granger}\"\n        status = \"Failure\"; summary = result[\"error\"]; confidence = 0.1; issues.append(result[\"error\"])\n\n    return {**result, \"reflection\": _create_reflection(status, summary, confidence, alignment...\n```\n\nEXAMPLE APPLICATION:\nDoWhy enables the CausalinferencE tool to identify that a 15% increase in customer support response time causes a 23% increase in churn rate, not just correlates with it, by controlling for confounding variables.\n\nCATEGORY: ExternalLibrary\n\nRELATIONSHIPS:\ntype: CausalInferenceLibrary; enables: Causal Inference, Causal Discovery, Causal Estimation; used_by: CausalinferencE, Temporal Reasoning, Advanced Analysis; provides: PC Algorithm, Backdoor Adjustment, Causal Validation; confidence: high"}