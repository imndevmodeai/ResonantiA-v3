[
  {
    "stage_name": "Narrative",
    "content": "TERM: Class: MastermindServer\n\nDEFINITION:\nThe unified ArchE server, integrating the full cognitive core with a WebSocket interface.\n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/mastermind_server.py, type: python_class\n\nFULL IMPLEMENTATION CODE (mastermind_server.py):\n```python\n#!/usr/bin/env python3\n\"\"\"\nArchE Mastermind Server - Unified Cognitive Core\nResonantiA Protocol v3.1-CA\n\nThis script merges the advanced cognitive core from mastermind/interact.py\nwith a robust WebSocket server architecture. It serves as the single,\nauthoritative entry point for the ArchE backend.\n\"\"\"\n\nimport sys\nimport os\nimport logging\nimport json\nfrom pathlib import Path\nimport asyncio\nimport websockets\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import Dict, Any, List\nfrom apscheduler.schedulers.asyncio import AsyncIOScheduler\n\n# Add the project root to the path to allow direct imports\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\ntry:\n    from .workflow_engine import IARCompliantWorkflowEngine\n    from .proactive_truth_system import ProactiveTruthSystem\n    from .tools.enhanced_search_tool import EnhancedSearchTool\n    from .spr_manager import SPRManager\n    from .adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator\n    from .rise_orchestrator import RISE_Orchestrator\n    from .autopoietic_governor import AutopoieticGovernor\n    from .thought_trail import ThoughtTrail # Assuming thought_trail is a singleton or class\n    from .nexus_interface import nexus_interface\nexcept ImportError:\n    # Fallback to absolute imports if relative imports fail\n    from Three_PointO_ArchE.workflow_engine import IARCompliantWorkflowEngine\n    from Three_PointO_ArchE.proactive_truth_system import ProactiveTruthSystem\n    from Three_PointO_ArchE.tools.enhanced_search_tool import EnhancedSearchTool\n    from Three_PointO_ArchE.spr_manager import SPRManager\n    from Three_PointO_ArchE.adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator\n    from Three_PointO_ArchE.rise_orchestrator import RISE_Orchestrator\n    from Three_PointO_ArchE.autopoietic_governor import AutopoieticGovernor\n    from Three_PointO_ArchE.thought_trail import ThoughtTrail\n    from Three_PointO_ArchE.nexus_interface import nexus_interface\nfrom .llm_providers.google import GoogleProvider\n\n# --- Logging Setup ---\ntry:\n    from .logging_config import setup_logging\nexcept ImportError:\n    from Three_PointO_ArchE.logging_config import setup_logging\n\n# Initialize timestamped logging system\nsetup_logging()\nlogger = logging.getLogger(\"ArchE_Mastermind_Server\")\nperf_logger = logging.getLogger(\"ArchE_Performance\")\n# session_logger = logging.getLogger(\"ArchE_Session\") # Example for a session-specific logger\n\nclass MastermindServer:\n    \"\"\"\n    The unified ArchE server, integrating the full cognitive core with a WebSocket interface.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initializes all cognitive components of the ArchE system.\"\"\"\n        logger.info(\"ðŸ§  Initializing ArchE Mastermind Server...\")\n        self.config = self._load_config()\n        self.engine = IARCompliantWorkflowEngine()\n        self._initialize_ptrf()\n        self._initialize_aco()\n        self._initialize_rise()\n        self._initialize_autopoiesis()\n        self.executor = ThreadPoolExecutor()\n        logger.info(\"âœ… ArchE Mastermind Server Initialized Successfully.\")\n\n    def _load_config(self) -> Dict[str, Any]:\n        \"\"\"Loads the enhanced mastermind configuration.\"\"\"\n        try:\n            config_path = project_root / \"mastermind\" / \"enhanced_mastermind_config.json\"\n            with open(config_path, 'r') as f:\n                logger.info(\"Loading enhanced mastermind configuration...\")\n                return json.load(f)\n        except (IOError, json.JSONDecodeError) as e:\n            logger.error(f\"FATAL: Could not load or parse configuration file: {e}. Using empty config.\", exc_info=True)\n            return {}\n\n    def _initialize_ptrf(self):\n        \"\"\"Initializes the Proactive Truth Resonance Framework.\"\"\"\n        try:\n            from dotenv import load_dotenv\n            load_dotenv()\n            api_key = os.environ.get(\"GEMINI_API_KEY\") or os.environ.get(\"GOOGLE_API_KEY\")\n            if not api_key:\n                raise ValueError(\"GEMINI_API_KEY or GOOGLE_API_KEY not set.\")\n            \n            self.llm_provider = GoogleProvider(api_key=api_key) # Store provider as instance variable\n            web_search_tool = EnhancedSearchTool()\n            spr_definitions_path = str(project_root / \"knowledge_graph\" / \"spr_definitions_tv.json\")\n            self.spr_manager = SPRManager(spr_filepath=spr_definitions_path)\n            \n            self.truth_seeker = ProactiveTruthSystem(\n                workflow_engine=self.engine,\n                llm_provider=self.llm_provider,\n                web_search_tool=web_search_tool,\n                spr_manager=self.spr_manager\n            )\n            self.ptrf_enabled = True\n            logger.info(\"âœ… Proactive Truth Resonance Framework is ONLINE.\")\n        except Exception as e:\n            logger.error(f\"âŒ Failed to initialize PTRF: {e}\", exc_info=True)\n            self.ptrf_enabled = False\n            self.truth_seeker = None\n            self.spr_manager = None\n\n    def _initialize_aco(self):\n        \"\"\"Initializes the Adaptive Cognitive Orchestrator.\"\"\"\n        try:\n            protocol_chunks = [\n                'Implementation Resonance refers to the alignment between conceptual understanding and operational implementation.',\n                'The ProportionalResonantControlPatterN eliminates oscillatory errors through resonant gain amplification.',\n                'Adaptive Cognitive Orchestrator enables meta-learning and pattern evolution in cognitive architectures.'\n            ]\n            self.aco = AdaptiveCognitiveOrchestrator(\n                protocol_chunks=protocol_chunks,\n                llm_provider=getattr(self, 'llm_provider', None) # Pass the stored LLM provider\n            )\n            self.autonomous_evolution_enabled = True\n            logger.info(\"âœ… Adaptive Cognitive Orchestrator is ONLINE.\")\n        except Exception as e:\n            logger.error(f\"âŒ Failed to initialize ACO: {e}\", exc_info=True)\n            self.autonomous_evolution_enabled = False\n            self.aco = None\n            \n    def _initialize_rise(self):\n        \"\"\"Initializes the RISE Orchestrator.\"\"\"\n        try:\n            workflows_dir = project_root / \"workflows\"\n            self.rise_orchestrator = RISE_Orchestrator(workflows_dir=str(workflows_dir))\n\n            # Attach event callback to forward SIRC events to websockets clients\n            def _event_sink(event_obj: Dict[str, Any]):\n                try:\n                    # Buffer events on the server instance for retrieval by websocket handler\n                    if not hasattr(self, '_event_queue'):\n                        self._event_queue = []\n                    self._event_queue.append(event_obj)\n                except Exception:\n                    pass\n\n            try:\n                self.rise_orchestrator.event_callback = _event_sink  # type: ignore\n            except Exception:\n                pass\n            self.rise_v2_enabled = True\n            logger.info(\"âœ… RISE v2.0 Genesis Protocol is ONLINE.\")\n        except Exception as e:\n            logger.error(f\"âŒ Failed to initialize RISE v2.0: {e}\", exc_info=True)\n            self.rise_v2_enabled = False\n            self.rise_orchestrator = None\n\n    def _initialize_autopoiesis(self):\n        \"\"\"Initializes the Autopoietic Governor and its dependencies.\"\"\"\n        try:\n            # Connect the global thought_trail instance to the global nexus_interface\n            self.thought_trail = ThoughtTrail() \n            nexus_interface.inject_thoughttrail(self.thought_trail)\n\n            # The insight engine needs to be defined/initialized, mocking for now\n            class MockInsightEngine: pass\n            self.insight_engine = MockInsightEngine()\n\n            self.governor = AutopoieticGovernor(\n                config=self.config, # Pass the loaded server config\n                thought_trail=self.thought_trail,\n                insight_engine=self.insight_engine,\n                spr_manager=self.spr_manager\n            )\n            \n            # Schedule the governor's self-audit\n            self.scheduler = AsyncIOScheduler()\n            audit_interval = self.governor.config.get(\"AUDIT_INTERVAL_MINUTES\", 60)\n            self.scheduler.add_job(\n                self.governor.perform_self_audit, \n                'interval', \n                minutes=audit_interval\n            )\n            self.scheduler.start()\n            \n            self.autopoiesis_enabled = self.governor.config.get(\"AUTOPOIESIS_ENABLED\", False)\n            if self.autopoiesis_enabled:\n                logger.info(f\"âœ… Autopoietic Governor is ONLINE and scheduled for audit every {audit_interval} minutes.\")\n            else:\n                logger.warning(\"Autopoiesis is DISABLED by configuration. The Governor is idle.\")\n        except Exception as e:\n            logger.error(f\"âŒ Failed to initialize Autopoietic Governor: {e}\", exc_info=True)\n            self.autopoiesis_enabled = False\n            self.governor = None\n\n\n    def _handle_cognitive_query_sync(self, query: str) -> Dict[str, Any]:\n        \"\"\"\n        Synchronous wrapper for handling queries through the ACO/RISE cognitive core.\n        This is the method that will be run in a separate thread.\n        \"\"\"\n        # This is a simplified version of the logic from mastermind/interact.py\n        # It determines the cognitive path and executes it.\n        try:\n            query_lower = query.lower()\n            \n            strategic_indicators = [\n                \"crisis\", \"conflicting\", \"ground truth\", \"predictive forecast\", \"geopolitical\",\n                \"strategic\", \"complex\", \"high-stakes\", \"Execution paradoX\"\n            ]\n            \n            is_strategic_query = any(indicator in query_lower for indicator in strategic_indicators)\n            \n            if is_strategic_query and self.rise_v2_enabled:\n                logger.info(f\"ðŸŽ¯ Strategic query detected. Routing to RISE engine.\")\n                rise_result = self.rise_orchestrator.run_rise_workflow(query)\n                return self._wrap_response_with_iar(rise_result, \"RISE\", query)\n            elif self.ptrf_enabled and any(k in query_lower for k in [\"truth\", \"fact\", \"verify\"]):\n                 logger.info(f\"ðŸŽ¯ Truth-seeking query detected. Routing to PTRF engine.\")\n                 ptrf_result = self.truth_seeker.seek_truth(query)\n                 return self._wrap_response_with_iar(ptrf_result, \"PTRF\", query)\n            elif self.autonomous_evolution_enabled:\n                logger.info(f\"ðŸŽ¯ Standard query. Routing to ACO for enhancement.\")\n                context, _ = self.aco.process_query_with_evolution(query)\n                return self._wrap_response_with_iar({\"response\": context}, \"ACO\", query)\n            else:\n                logger.warning(\"No cognitive core available for query.\")\n                return self._wrap_response_with_iar({\"error\": \"No cognitive core available.\"}, \"ERROR\", query)\n\n        except Exception as e:\n            logger.error(f\"Error in cognitive query handler: {e}\", exc_info=True)\n            return self._wrap_response_with_iar({\"error\": str(e)}, \"ERROR\", query)\n    \n    def _wrap_response_with_iar(self, response: Dict[str, Any], engine: str, query: str) -> Dict[str, Any]:\n        \"\"\"\n        Wrap the response with enhanced IAR (Integrated Action Reflection) data for VCD display.\n        Incorporates frontend's sophisticated cognitive assessment structure.\n        \"\"\"\n        import time\n        from datetime import datetime\n\n# ============================================================================\n# TEMPORAL CORE INTEGRATION (CANONICAL DATETIME SYSTEM)\n# ============================================================================\nfrom .temporal_core import now_iso, format_filename, format_log, Timer\n        \n        # Determine confidence based on response content\n        confidence = 0.9\n        status = \"Success\"\n        potential_issues = []\n        \n        if \"error\" in response:\n            confidence = 0.1\n            status = \"Failure\"\n            potential_issues.append(f\"Engine {engine} returned error\")\n        elif \"execution_status\" in response and response[\"execution_status\"] == \"failed\":\n            confidence = 0.3\n            status = \"Warning\"\n            potential_issues.append(\"Workflow execution failed\")\n        \n        # Enhanced cognitive resonance analysis\n        cognitive_resonance = self._analyze_cognitive_resonance(query, response)\n        temporal_resonance = self._analyze_temporal_resonance(query, response)\n        \n        # SPR detection using enhanced backend capabilities\n        spr_activations = self._detect_sprs_in_content(query, response)\n        \n        # Create enhanced IAR data with frontend sophistication\n        iar_data = {\n            \"status\": status,\n            \"confidence\": confidence,\n            \"summary\": f\"Query processed by {engine} engine\",\n            \"potential_issues\": potential_issues,\n            \"alignment_check\": {\n                \"query_relevance\": 0.9,\n                \"engine_selection\": 0.8,\n                \"response_quality\": confidence\n            },\n            \"engine_used\": engine,\n            \"timestamp\": now_iso() + \"Z\",\n            # Enhanced cognitive assessment\n            \"cognitive_resonance\": cognitive_resonance,\n            \"temporal_resonance\": temporal_resonance,\n            \"recommendations\": self._generate_iar_recommendations(cognitive_resonance, temporal_resonance),\n            \"confidence_boost\": max(0, 0.9 - confidence),\n            \"resonance_enhancement\": max(0, 0.8 - ((cognitive_resonance[\"clarity\"] + temporal_resonance[\"causal_understanding\"]) / 2))\n        }\n        \n        # Create enhanced message structure\n        enhanced_response = {\n            \"id\": f\"arche_{int(time.time() * 1000)}\",\n            \"content\": self._format_response_content(response),\n            \"timestamp\": now_iso() + \"Z\",\n            \"sender\": \"arche\",\n            \"message_type\": \"chat\",\n            \"protocol_compliance\": True,\n            \"protocol_version\": \"ResonantiA v3.1-CA\",\n            \"iar\": iar_data,\n            \"spr_activations\": spr_activations,  # Include SPR detection results\n            \"raw_response\": response  # Include original response for debugging\n        }\n        \n        return enhanced_response\n    \n    def _analyze_cognitive_resonance(self, query: str, response: Dict[str, Any]) -> Dict[str, float]:\n        \"\"\"Analyze cognitive resonance using frontend's sophisticated approach.\"\"\"\n        content = self._format_response_content(response)\n        \n        # Analyze clarity indicators\n        clarity_indicators = ['clear', 'specific', 'example', 'detail', 'explain']\n        clarity_score = self._calculate_indicator_score(content, clarity_indicators)\n        \n        # Analyze coherence indicators\n        coherence_indicators = ['logical', 'structure', 'flow', 'organized', 'systematic']\n        coherence_score = self._calculate_indicator_score(content, coherence_indicators)\n        \n        # Analyze completeness indicators\n        completeness_indicators = ['complete', 'comprehensive', 'thorough', 'detailed', 'full']\n        completeness_score = self._calculate_indicator_score(content, completeness_indicators)\n        \n        # Analyze contextual relevance\n        contextual_relevance = 0.9 if len(content) > 100 else 0.6\n        \n        return {\n            \"clarity\": min(0.9, 0.3 + clarity_score),\n            \"coherence\": min(0.9, 0.3 + coherence_score),\n            \"completeness\": min(0.9, 0.3 + completeness_score),\n            \"contextual_relevance\": contextual_relevance\n        }\n    \n    def _analyze_temporal_resonance(self, query: str, response: Dict[str, Any]) -> Dict[str, float]:\n        \"\"\"Analyze temporal resonance using frontend's 4D approach.\"\"\"\n        content = self._format_response_content(response)\n        \n        # Past context analysis\n        past_indicators = ['history', 'past', 'previous', 'before', 'earlier', 'tradition']\n        past_score = self._calculate_indicator_score(content, past_indicators)\n        \n        # Present accuracy analysis\n        present_indicators = ['current', 'now', 'present', 'today', 'contemporary', 'modern']\n        present_score = self._calculate_indicator_score(content, present_indicators)\n        \n        # Future projection analysis\n        future_indicators = ['future', 'will', 'shall', 'project', 'predict', 'forecast']\n        future_score = self._calculate_indicator_score(content, future_indicators)\n        \n        # Causal understanding analysis\n        causal_indicators = ['cause', 'effect', 'because', 'therefore', 'result', 'consequence']\n        causal_score = self._calculate_indicator_score(content, causal_indicators)\n        \n        return {\n            \"past_context\": min(0.9, 0.2 + past_score),\n            \"present_accuracy\": min(0.9, 0.3 + present_score),\n            \"future_projection\": min(0.9, 0.2 + future_score),\n            \"causal_understanding\": min(0.9, 0.3 + causal_score)\n        }\n    \n    def _calculate_indicator_score(self, content: str, indicators: List[str]) -> float:\n        \"\"\"Calculate score based on indicator presence in content.\"\"\"\n        if not content:\n            return 0.0\n        \n        lower_content = content.lower()\n        matches = sum(1 for indicator in indicators if indicator in lower_content)\n        return min(0.6, matches * 0.1)\n    \n    def _generate_iar_recommendations(self, cognitive_resonance: Dict[str, float], temporal_resonance: Dict[str, float]) -> List[str]:\n        \"\"\"Generate recommendations based on IAR analysis.\"\"\"\n        recommendations = []\n        \n        if cognitive_resonance[\"clarity\"] < 0.7:\n            recommendations.append(\"Enhance clarity by providing more specific examples\")\n        if cognitive_resonance[\"coherence\"] < 0.7:\n            recommendations.append(\"Improve logical flow and structure\")\n        if temporal_resonance[\"causal_understanding\"] < 0.6:\n            recommendations.append(\"Strengthen causal relationships and temporal context\")\n        if cognitive_resonance[\"completeness\"] < 0.7:\n            recommendations.append(\"Provide more comprehensive coverage of the topic\")\n        \n        return recommendations\n    \n    def _detect_sprs_in_content(self, query: str, response: Dict[str, Any]) -> List[Dict[str, Any]]:\n        \"\"\"Detect SPRs in query and response content using enhanced backend capabilities.\"\"\"\n        try:\n            # Combine query and response content for SPR detection\n            content = f\"{query} {self._format_response_content(response)}\"\n            \n            # Use the enhanced SPR manager if available\n            if hasattr(self, 'spr_manager') and self.spr_manager:\n                return self.spr_manager.detect_sprs_with_confidence(content)\n            else:\n                # Fallback to basic detection\n                return self._basic_spr_detection(content)\n        except Exception as e:\n            logger.warning(f\"SPR detection failed: {e}\")\n            return []\n    \n    def _basic_spr_detection(self, content: str) -> List[Dict[str, Any]]:\n        \"\"\"Basic SPR detection fallback.\"\"\"\n        # Simple keyword-based detection\n        spr_keywords = {\n            'IntegratedActionReflection': ['IAR', 'action reflection', 'integrated reflection'],\n            'SparsePrimingRepresentations': ['SPR', 'sparse priming', 'priming representations'],\n            'VisualCognitiveDebugger': ['VCD', 'visual debugger', 'cognitive debugger'],\n            'ResonantInsightStrategyEngine': ['RISE', 'resonant insight', 'strategy engine'],\n            'SynergisticIntentResonanceCycle': ['SIRC', 'synergistic intent', 'resonance cycle']\n        }\n        \n        detected_sprs = []\n        lower_content = content.lower()\n        \n        for spr_id, keywords in spr_keywords.items():\n            for keyword in keywords:\n                if keyword.lower() in lower_content:\n                    detected_sprs.append({\n                        'spr_id': spr_id,\n                        'activation_level': 0.8,\n                        'confidence_score': 0.7,\n                        'guardian_point': spr_id,\n                        'matched_keyword': keyword\n                    })\n                    break  # Only add once per SPR\n        \n        return detected_sprs\n    \n    def _format_response_content(self, response: Dict[str, Any]) -> str:\n        \"\"\"\n        Format the response content for display in the VCD.\n        \"\"\"\n        if \"error\" in response:\n            return f\"Error: {response['error']}\"\n        elif \"final_strategy\" in response:\n            return f\"RISE Strategy Generated:\\n\\n{response.get('final_strategy', 'No strategy available')}\"\n        elif \"response\" in response:\n            return str(response[\"response\"])\n        else:\n            return json.dumps(response, indent=2, default=str)\n\n    async def websocket_handler(self, websocket):\n        \"\"\"Handles incoming WebSocket connections and messages.\"\"\"\n        logger.info(f\"ðŸ”— Client connected from {websocket.remote_address}\")\n        try:\n            async for message in websocket:\n                if message == \"ping\":\n                    await websocket.send(\"pong\")\n                    continue\n\n                query = str(message)\n                logger.info(f\"ðŸ“¥ Received query: {query[:150]}...\")\n                \n                try:\n                    loop = asyncio.get_running_loop()\n                    response = await loop.run_in_executor(\n                        self.executor, self._handle_cognitive_query_sync, query\n                    )\n                    \n                    # First stream any buffered SIRC events\n                    try:\n                        if hasattr(self, '_event_queue') and self._event_queue:\n                            for evt in self._event_queue:\n                                await websocket.send(json.dumps(evt))\n                            self._event_queue.clear()\n                    except Exception:\n                        pass\n                    \n                    response_json = json.dumps(response, default=str)\n                    logger.info(f\"ðŸ“¤ Sending response: {response_json[:200]}...\")\n                    await websocket.send(response_json)\n                    \n                except Exception as e:\n                    logger.error(f\"Error processing query: {e}\", exc_info=True)\n                    error_response = {\"error\": \"An error occurred while processing your request.\"}\n                    await websocket.send(json.dumps(error_response))\n        except websockets.exceptions.ConnectionClosed:\n            logger.info(f\"Connection closed for client {websocket.remote_address}\")\n        finally:\n            logger.info(\"WebSocket handler cleanup complete.\")\n\nasync def main():\n    \"\"\"Main function to start the Mastermind WebSocket server.\"\"\"\n    port_str = os.environ.get('ARCHE_PORT')\n    if not port_str or not port_str.isdigit():\n        logger.critical(f\"FATAL: ARCHE_PORT environment variable is not set or invalid. Got: '{port_str}'.\")\n        return\n\n    websocket_port = int(port_str)\n    host = \"0.0.0.0\"\n\n    server_instance = MastermindServer()\n    \n    # Start the Nexus WebSocket server in a background thread\n    nexus_interface.start_server_in_thread()\n    \n    logger.info(f\"ðŸš€ Attempting to start ArchE Mastermind Server on {host}:{websocket_port}\")\n    \n    try:\n        # We are not running a websocket server from here anymore, just the cognitive loop.\n        # This part of the code could be adapted to run a main cognitive loop\n        # or other primary server task. For now, we will just wait.\n        logger.info(\"âœ… ArchE Mastermind Server core is running.\")\n        logger.info(\"Nexus WebSocket bridge is running in a background thread.\")\n        await asyncio.Future() # Keep the main thread alive\n    except OSError as e:\n        logger.critical(f\"FATAL: Failed to start server components. Error: {e}\")\n    except Exception as e:\n        logger.critical(f\"FATAL: An unexpected error occurred: {e}\", exc_info=True)\n    finally:\n        logger.info(\"Shutting down Nexus server...\")\n        nexus_interface.stop()\n\nif __name__ == \"__main__\":\n    try:\n        asyncio.run(main())\n    except KeyboardInterrupt:\n        logger.info(\"ðŸ›‘ Server shutting down.\")\n    except Exception as e:\n        logger.critical(f\"ðŸ’¥ Server failed to start: {e}\", exc_info=True)\n\n```\n\nEXAMPLE APPLICATION:\nThe unified ArchE server, integrating the full cognitive core with a WebSocket interface.\n\nCATEGORY: CodeKnowledge\n\nRELATIONSHIPS:\ntype: FromCodebase; source: /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/mastermind_server.py; source_type: python_class",
    "compression_ratio": 1.0,
    "symbol_count": 25072,
    "timestamp": "2025-11-18T11:00:35.849126Z"
  },
  {
    "stage_name": "Concise",
    "content": "TERM: Class: MastermindServer\n\nDEFINITION:\nThe unified ArchE server, integrating the full cognitive core with a WebSocket interface.\n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/mastermind_server.py, type: python_class\n\nFULL IMPLEMENTATION CODE (mastermind_server.py):\n```python\n#!/usr/bin/env python3\n\"\"\"\nArchE Mastermind Server - Unified Cognitive Core\nResonantiA Protocol v3.1-CA\n\nThis script merges the advanced cognitive core from mastermind/interact.py\nwith a robust WebSocket server architecture. It serves as the single,\nauthoritative entry point for the ArchE backend.\n\"\"\"\n\nimport sys\nimport os\nimport logging\nimport json\nfrom pathlib import Path\nimport asyncio\nimport websockets\nfrom concurrent.futures import ThreadPoolExecutor\nfrom typing import Dict, Any, List\nfrom apscheduler.schedulers.asyncio import AsyncIOScheduler\n\n# Add the project root to the path to allow direct imports\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root))\n\ntry:\n    from .workflow_engine import IARCompliantWorkflowEngine\n    from .proactive_truth_system import ProactiveTruthSystem\n    from .tools.enhanced_search_tool import EnhancedSearchTool\n    from .spr_manager import SPRManager\n    from .adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator\n    from .rise_orchestrator import RISE_Orchestrator\n    from .autopoietic_governor import AutopoieticGovernor\n    from .thought_trail import ThoughtTrail # Assuming thought_trail is a singleton or class\n    from .nexus_interface import nexus_interface\nexcept ImportError:\n    # Fallback to absolute imports if relative imports fail\n    from Three_PointO_ArchE.workflow_engine import IARCompliantWorkflowEngine\n    from Three_PointO_ArchE.proactive_truth_system import ProactiveTruthSystem\n    from Three_PointO_ArchE.tools.enhanced_search_tool import EnhancedSearchTool\n    from Three_PointO_ArchE.spr_manager import SPRManager\n    from Three_PointO_ArchE.adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator\n    from Three_PointO_ArchE.rise_orchestrator import RISE_Orchestrator\n    from Three_PointO_ArchE.autopoietic_governor import AutopoieticGovernor\n    from Three_PointO_ArchE.thought_trail import ThoughtTrail\n    from Three_PointO_ArchE.nexus_interface import nexus_interface\nfrom .llm_providers.google import GoogleProvider\n\n# --- Logging Setup ---\ntry:\n    from .logging_config import setup_logging\nexcept ImportError:\n    from Three_PointO_ArchE.logging_config import setup_logging\n\n# Initialize timestamped logging system\nsetup_logging()\nlogger = logging.getLogger(\"ArchE_Mastermind_Server\")\nperf_logger = logging.getLogger(\"ArchE_Performance\")\n# session_logger = logging.getLogger(\"ArchE_Session\") # Example for a session-specific logger\n\nclass MastermindServer:\n    \"\"\"\n    The unified ArchE server, integrating the full cognitive core with a WebSocket interface.\n    \"\"\"\n    \n    def __init__(self):\n        \"\"\"Initializes all cognitive components of the ArchE system.\"\"\"\n        logger.info(\"ðŸ§  Initializing ArchE Mastermind Server...\")\n        self.config = self._load_config()\n        self.engine = IARCompliantWorkflowEngine()\n        self._initialize_ptrf()\n        self._initialize_aco()\n        self._initialize_rise()\n        self._initialize_autopoiesis()\n        self.executor = ThreadPoolExecutor()\n        logger.info(\"âœ… ArchE Mastermind Server Initialized Successfully.\")\n\n    def _load_config(self) -> Dict[str, Any]:\n        \"\"\"Loads the enhanced mastermind configuration.\"\"\"\n        try:\n            config_path = project_root / \"mastermind\" / \"enhanced_mastermind_config.json\"\n            with open(config_path, 'r') as f:\n                logger.info(\"Loading enhanced mastermind configuration...\")\n                return json.load(f)\n        except (IOError, json.JSONDecodeError) as e:\n            logger.error(f\"FATAL: Could not load or parse configuration file: {e}. Using empty config.\", exc_info=True)\n            return {}\n\n    def _initialize_ptrf(self):\n        \"\"\"Initializes the Proactive Truth Resonance Framework.\"\"\"\n        try:\n            from dotenv import load_dotenv\n            load_dotenv()\n            api_key = os.environ.get(\"GEMINI_API_KEY\") or os.environ.get(\"GOOGLE_API_KEY\")\n            if not api_key:\n                raise ValueError(\"GEMINI_API_KEY or GOOGLE_API_KEY not set.\")\n            \n            self.llm_provider = GoogleProvider(api_key=api_key) # Store provider as instance variable\n            web_search_tool = EnhancedSearchTool()\n            spr_definitions_path = str(project_root / \"knowledge_graph\" / \"spr_definitions_tv.json\")\n            self.spr_manager = SPRManager(spr_filepath=spr_definitions_path)\n            \n            self.truth_seeker = ProactiveTruthSystem(\n                workflow_engine=self.engine,\n                llm_provider=self.llm_provider,\n                web_search_tool=web_search_tool,\n                spr_manager=self.spr_manager\n            )\n            self.ptrf_enabled = True\n            logger.info(\"âœ… Proactive Truth Resonance Framework is ONLINE.\")\n        except Exception as e:\n            logger.error(f\"âŒ Failed to initialize PTRF: {e}\", exc_info=True)\n            self.ptrf_enabled = False\n            self.truth_seeker = None\n            self.spr_manager = None\n\n    def _initialize_aco(self):\n        \"\"\"Initializes the Adaptive Cognitive Orchestrator.\"\"\"\n        try:\n            protocol_chunks = [\n                'Implementation Resonance refers to the alignment between conceptual understanding and operational implementation.',\n                'The ProportionalResonantControlPatterN eliminates oscillatory errors through resonant gain amplification.',\n                'Adaptive Cognitive Orchestrator enables meta-learning and pattern evolution in cognitive architectures.'\n            ]\n            self.aco = AdaptiveCognitiveOrchestrator(\n                protocol_chunks=protocol_chunks,\n                llm_provider=getattr(self, 'llm_provider', None) # Pass the stored LLM provider\n            )\n            self.autonomous_evolution_enabled = True\n            logger.info(\"âœ… Adaptive Cognitive Orchestrator is ONLINE.\")\n        except Exception as e:\n            logger.error(f\"âŒ Failed to initialize ACO: {e}\", exc_info=True)\n            self.autonomous_evolution_enabled = False\n            self.aco = None\n            \n    def _initialize_rise(self):\n        \"\"\"Initializes the RISE Orchestrator.\"\"\"\n        try:\n            workflows_dir = project_root / \"workflows\"\n            self.rise_orchestrator = RISE_Orchestrator(workflows_dir=str(workflows_dir))\n\n            # Attach event callback to forward SIRC events to websockets clients\n            def _event_sink(event_obj: Dict[str, Any]):\n                try:\n                    # Buffer events on the server instance for retrieval by websocket handler\n                    if not hasattr(self, '_event_queue'):\n                        self._event_queue = []\n                    self._event_queue.append(event_obj)\n                except Exception:\n                    pass\n\n            try:\n                self.rise_orchestrator.event_callback = _event_sink  # type: ignore\n            except Exception:\n                pass\n            self.rise_v2_enabled = True\n            logger.info(\"âœ… RISE v2.0 Genesis Protocol is ONLINE.\")\n        except Exception as e:\n            logger.error(f\"âŒ Failed to initialize RISE v2.0: {e}\", exc_info=True)\n            self.rise_v2_enabled = False\n            self.rise_orchestrator = None\n\n    def _initialize_autopoiesis(self):\n        \"\"\"Initializes the Autopoietic Governor and its dependencies.\"\"\"\n        try:\n            # Connect the global thought_trail instance to the global nexus_interface\n            self.thought_trail = ThoughtTrail() \n            nexus_interface.inject_thoughttrail(self.thought_trail)\n\n            # The insight engine needs to be defined/initialized, mocking for now\n            class MockInsightEngine: pass\n            self.insight_engine = MockInsightEngine()\n\n            self.governor = AutopoieticGovernor(\n                config=self.config, # Pass the loaded server config\n                thought_trail=self.thought_trail,\n                insight_engine=self.insight_engine,\n                spr_manager=self.spr_manager\n            )\n            \n            # Schedule the governor's self-audit\n            self.scheduler = AsyncIOScheduler()\n            audit_interval = self.governor.config.get(\"AUDIT_INTERVAL_MINUTES\", 60)\n            self.scheduler.add_job(\n                self.governor.perform_self_audit, \n                'interval', \n                minutes=audit_interval\n            )\n            self.scheduler.start()\n            \n            self.autopoiesis_enabled = self.governor.config.get(\"AUTOPOIESIS_ENABLED\", False)\n            if self.autopoiesis_enabled:\n                logger.info(f\"âœ… Autopoietic Governor is ONLINE and scheduled for audit every {audit_interval} minutes.\")\n            else:\n                logger.warning(\"Autopoiesis is DISABLED by configuration. The Governor is idle.\")\n        except Exception as e:\n            logger.error(f\"âŒ Failed to initialize Autopoietic Governor: {e}\", exc_info=True)\n            self.autopoiesis_enabled = False\n            self.governor = None\n\n\n    def _handle_cognitive_query_sync(self, query: str) -> Dict[str, Any]:\n        \"\"\"\n        Synchronous wrapper for handling queries through the ACO/RISE cognitive core.\n        This is the method that will be run in a separate thread.\n        \"\"\"\n        # This is a simplified version of the logic from mastermind/interact.py\n        # It determines the cognitive path and executes it.\n        try:\n            query_lower = query.lower()\n            \n            strategic_indicators = [\n                \"crisis\", \"conflicting\", \"ground truth\", \"predictive forecast\", \"geopolitical\",\n                \"strategic\", \"complex\", \"high-stakes\", \"Execution paradoX\"\n            ]\n            \n            is_strategic_query = any(indicator in query_lower for indicator in strategic_indicators)\n            \n            if is_strategic_query and self.rise_v2_enabled:\n                logger.info(f\"ðŸŽ¯ Strategic query detected. Routing to RISE engine.\")\n                rise_result = self.rise_orchestrator.run_rise_workflow(query)\n                return self._wrap_response_with_iar(rise_result, \"RISE\", query)\n            elif self.ptrf_enabled and any(k in query_lower for k in [\"truth\", \"fact\", \"verify\"]):\n                 logger.info(f\"ðŸŽ¯ Truth-seeking query detected. Routing to PTRF engine.\")\n                 ptrf_result = self.truth_seeker.seek_truth(query)\n                 return self._wrap_response_with_iar(ptrf_result, \"PTRF\", query)\n            elif self.autonomous_evolution_enabled:\n                logger.info(f\"ðŸŽ¯ Standard query. Routing to ACO for enhancement.\")\n                context, _ = self.aco.process_query_with_evolution(query)\n                return self._wrap_response_with_iar({\"response\": context}, \"ACO\", query)\n            else:\n                logger.warning(\"No cognitive core available for query.\")\n                return self._wrap_response_with_iar({\"error\": \"No cognitive core available.\"}, \"ERROR\", query)\n\n        except Exception as e:\n            logger.error(f\"Error in cognitive query handler: {e}\", exc_info=True)\n            return self._wrap_response_with_iar({\"error\": str(e)}, \"ERROR\", query)\n    \n    def _wrap_response_with_iar(self, response: Dict[str, Any], engine: str, query: str) -> Dict[str, Any]:\n        \"\"\"\n        Wrap the response with enhanced IAR (Integrated Action Reflection) data for VCD display.\n        Incorporates frontend's sophisticated cognitive assessment structure.\n        \"\"\"\n        import time\n        from datetime import datetime\n\n# ============================================================================\n# TEMPORAL CORE INTEGRATION (CANONICAL DATETIME SYSTEM)\n# ============================================================================\nfrom .temporal_core import now_iso, format_filename, format_log, Timer\n        \n        # Determine confidence based on response content\n        confidence = 0.9\n        status = \"Success\"\n        potential_issues = []\n        \n        if \"error\" in response:\n            confidence = 0.1\n            status = \"Failure\"\n            potential_issues.append(f\"Engine {engine} returned error\")\n        elif \"execution_status\" in response and response[\"execution_status\"] == ",
    "compression_ratio": 2.0,
    "symbol_count": 12536,
    "timestamp": "2025-11-18T11:00:35.849163Z"
  },
  {
    "stage_name": "Nano",
    "content": "TERM: Class: MastermindServer D: unified Ã† server, integrating full cognitive core a WebSocket interface. BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Ã†/mastermind_server.py, type: python_class FULL I CODE (mastermind_server.py): ```python #!/usr/bin/env python3 \"\"\" Ã† Mastermind Server - Unified Cognitive Core ResonantiA P v3.1-CA script merges advanced cognitive core mastermind/interact.py a robust WebSocket server architecture. It serves as single, authoritative entry point Ã† backend. \"\"\" import sys import os import logging import json pathlib import Path import asyncio import websockets concurrent.futures import ThreadPoolExecutor typing import Dict, Any, List apscheduler.schedulers.asyncio import AsyncIOScheduler # Add project root to path to allow direct imports project_root = Path(__file__).parent.parent sys.path.insert(0, str(project_root)) try: .workflow_engine import Î¦CompliantWorkflowEngine .proactive_truth_S import ProactiveTruthS .tools.enhanced_search_tool import EnhancedSearchTool .Î˜_manager import Î˜Manager .adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator .rise_orchestrator import RISE_Orchestrator .autopoietic_governor import AutopoieticGovernor .thought_trail import Î£ # Assuming thought_trail is a singleton or class .nexus_interface import nexus_interface except ImportError: # Fallback to absolute imports if relative imports fail Three_PointO_Ã†.workflow_engine import Î¦CompliantWorkflowEngine Three_PointO_Ã†.proactive_truth_S import ProactiveTruthS Three_PointO_Ã†.tools.enhanced_search_tool import EnhancedSearchTool Three_PointO_Ã†.Î˜_manager import Î˜Manager Three_PointO_Ã†.adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator Three_PointO_Ã†.rise_orchestrator import RISE_Orchestrator Three_PointO_Ã†.autopoietic_governor import AutopoieticGovernor Three_PointO_Ã†.thought_trail import Î£ Three_PointO_Ã†.nexus_interface import nexus_interface .llm_providers.google import GoogleProvider # --- Logging Setup --- try: .logging_config import setup_logging except ImportError: Three_PointO_Ã†.logging_config import setup_logging # Initialize timestamped logging S setup_logging() logger = logging.getLogger(\"Ã†_Mastermind_Server\") perf_logger = logging.getLogger(\"Ã†_Performance\") # session_logger = logging.getLogger(\"Ã†_Session\") # Example a session-specific logger class MastermindServer: \"\"\" unified Ã† server, integrating full cognitive core a WebSocket interface. \"\"\" def __init__(self): \"\"\"Initializes cognitive components of Ã† S.\"\"\" logger.info(\"ðŸ§  Initializing Ã† Mastermind Server...\") self.config = self._load_config() self.engine = Î¦CompliantWorkflowEngine() self._initialize_ptrf() self._initialize_aco() self._initialize_rise() self._initialize_autopoiesis() self.executor = ThreadPoolExecutor() logger.info(\"âœ… Ã† Mastermind Server Initialized Successfully.\") def _load_config(self) -> Dict[str, Any]: \"\"\"Loads enhanced mastermind configuration.\"\"\" try: config_path = project_root / \"mastermind\" / \"enhanced_mastermind_config.json\" open(config_path, 'r') as f: logger.info(\"Loading enhanced mastermind configuration...\") return json.load(f) except (IOError, json.JSONDecodeError) as e: logger.error(f\"FATAL: Could load or parse configuration file: {e}. Using empty config.\", exc_info=True) return {} def _initialize_ptrf(self): \"\"\"Initializes Proactive Truth Resonance Framework.\"\"\" try: dotenv import load_dotenv load_dotenv() api_key = os.environ.get(\"GEMINI_API_KEY\") or os.environ.get(\"GOOGLE_API_KEY\") if api_key: raise ValueError(\"GEMINI_API_KEY or GOOGLE_API_KEY set.\") self.llm_provider = GoogleProvider(api_key=api_key) # Store provider as instance variable web_search_tool = EnhancedSearchTool() Î˜_Ds_path = str(project_root / \"KnOwledge_graph\" / \"Î˜_Ds_tv.json\") self.Î˜_manager = Î˜Manager(Î˜_filepath=Î˜_Ds_path) self.truth_seeker = ProactiveTruthS( workflow_engine=self.engine, llm_provider=self.llm_provider, web_search_tool=web_search_tool, Î˜_manager=self.Î˜_manager ) self.ptrf_enabled = True logger.info(\"âœ… Proactive Truth Resonance Framework is ONLINE.\") except Exception as e: logger.error(f\"âŒ Failed to initialize PTRF: {e}\", exc_info=True) self.ptrf_enabled = False self.truth_seeker = None self.Î˜_manager = None def _initialize_aco(self): \"\"\"Initializes Adaptive Cognitive Orchestrator.\"\"\" try: P_chunks = [ 'I Resonance refers to alignment between conceptual understanding operational I.', ' ProportionalResonantControlPatterN eliminates oscillatory errors through resonant gain amplification.', 'Adaptive Cognitive Orchestrator enables meta-learning pattern evolution in cognitive architectures.' ] self.aco = AdaptiveCognitiveOrchestrator( P_chunks=P_chunks, llm_provider=getattr(self, 'llm_provider', None) # Pass stored LLM provider ) self.autonomous_evolution_enabled = True logger.info(\"âœ… Adaptive Cognitive Orchestrator is ONLINE.\") except Exception as e: logger.error(f\"âŒ Failed to initialize ACO: {e}\", exc_info=True) self.autonomous_evolution_enabled = False self.aco = None def _initialize_rise(self): \"\"\"Initializes RISE Orchestrator.\"\"\" try: workflows_dir = project_root / \"workflows\" self.rise_orchestrator = RISE_Orchestrator(workflows_dir=str(workflows_dir)) # Attach event callback to forward SIRC events to websockets clients def _event_sink(event_obj: Dict[str, Any]): try: # Buffer events on server instance retrieval by websocket handler if hasattr(self, '_event_queue'): self._event_queue = [] self._event_queue.append(event_obj) except Exception: pass try: self.rise_orchestrator.event_callback = _event_sink # type: ignore except Exception: pass self.rise_v2_enabled = True logger.info(\"âœ… RISE v2.0 Genesis P is ONLINE.\") except Exception as e: logger.error(f\"âŒ Failed to initialize RISE v2.0: {e}\", exc_info=True) self.rise_v2_enabled = False self.rise_orchestrator = None def _initialize_autopoiesis(self): \"\"\"Initializes Autopoietic Governor its dependencies.\"\"\" try: # Connect global thought_trail instance to global nexus_interface self.thought_trail = Î£() nexus_interface.inject_Î£(self.thought_trail) # insight engine needs to be defined/initialized, mocking now class MockInsightEngine: pass self.insight_engine = MockInsightEngine() self.governor = AutopoieticGovernor( config=self.config, # Pass loaded server config thought_trail=self.thought_trail, insight_engine=self.insight_engine, Î˜_manager=self.Î˜_manager ) # Schedule governor's self-audit self.scheduler = AsyncIOScheduler() audit_interval = self.governor.config.get(\"AUDIT_INTERVAL_MINUTES\", 60) self.scheduler.add_job( self.governor.perform_self_audit, 'interval', minutes=audit_interval ) self.scheduler.start() self.autopoiesis_enabled = self.governor.config.get(\"AUTOPOIESIS_ENABLED\", False) if self.autopoiesis_enabled: logger.info(f\"âœ… Autopoietic Governor is ONLINE scheduled audit every {audit_interval} minutes.\") else: logger.warning(\"Autopoiesis is DISABLED by configuration. Governor is idle.\") except Exception as e: logger.error(f\"âŒ Failed to initialize Autopoietic Governor: {e}\", exc_info=True) self.autopoiesis_enabled = False self.governor = None def _handle_cognitive_query_sync(self, query: str) -> Dict[str, Any]: \"\"\" Synchronous wrapper handling queries through ACO/RISE cognitive core. is method will be run in a separate thread. \"\"\" # is a simplified version of logic mastermind/interact.py # It determines cognitive path executes it. try: query_lower = query.lower() strategic_indicators = [ \"crisis\", \"conflicting\", \"ground truth\", \"predictive forecast\", \"geopolitical\", \"strategic\", \"complex\", \"high-stakes\", \"Execution paradoX\" ] is_strategic_query = any(indicator in query_lower indicator in strategic_indicators) if is_strategic_query self.rise_v2_enabled: logger.info(f\"ðŸŽ¯ Strategic query detected. Routing to RISE engine.\") rise_result = self.rise_orchestrator.run_rise_workflow(query) return self._wrap_response_with_Î¦(rise_result, \"RISE\", query) elif self.ptrf_enabled any(k in query_lower k in [\"truth\", \"fact\", \"verify\"]): logger.info(f\"ðŸŽ¯ Truth-seeking query detected. Routing to PTRF engine.\") ptrf_result = self.truth_seeker.seek_truth(query) return self._wrap_response_with_Î¦(ptrf_result, \"PTRF\", query) elif self.autonomous_evolution_enabled: logger.info(f\"ðŸŽ¯ Standard query. Routing to ACO enhancement.\") context, _ = self.aco.P_query_with_evolution(query) return self._wrap_response_with_Î¦({\"response\": context}, \"ACO\", query) else: logger.warning(\"No cognitive core available query.\") return self._wrap_response_with_Î¦({\"error\": \"No cognitive core available.\"}, \"ERROR\", query) except Exception as e: logger.error(f\"Error in cognitive query handler: {e}\", exc_info=True) return self._wrap_response_with_Î¦({\"error\": str(e)}, \"ERROR\", query) def _wrap_response_with_Î¦(self, response: Dict[str, Any], engine: str, query: str) -> Dict[str, Any]: \"\"\" Wrap response enhanced Î¦ (Î¦) data VCD display. Incorporates frontend's sophisticated cognitive assessment structure. \"\"\" import time datetime import datetime # ============================================================================ # TEMPORAL CORE INTEGRATION (CANONICAL DATETIME S) # ============================================================================ .temporal_core import now_iso, F_filename, F_log, Timer # Determine confidence based on response content confidence = 0.9 status = \"Success\" potential_issues = [] if \"error\" in response: confidence = 0.1 status = \"Failure\" potential_issues.append(f\"Engine {engine} returned error\") elif \"execution_status\" in response response[\"execution_status\"] ==",
    "compression_ratio": 2.63582842724979,
    "symbol_count": 9512,
    "timestamp": "2025-11-18T11:00:36.003065Z"
  },
  {
    "stage_name": "Micro",
    "content": "TERM: Class: MastermindServer D: unified Ã† server, integrating Î© WebSocket interface. BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Ã†/mastermind_server.py, type: python_class FULL I CODE (mastermind_server.py): ```python #!/usr/bin/env python3 Ã† Mastermind Server Unified Î© Core ResonantiA P v3.1-CA script merges advanced Î© mastermind/interact.py robust WebSocket server architecture. It serves single, authoritative entry point Ã† backend. import import import logging import pathlib import Path import asyncio import websockets concurrent.futures import ThreadPoolExecutor typing import Dict, Any, List apscheduler.schedulers.asyncio import AsyncIOScheduler Add project allow direct imports project_root Path(__file__).parent.parent sys.path.insert(0, str(project_root)) .workflow_engine import Î¦CompliantWorkflowEngine .proactive_truth_S import ProactiveTruthS .tools.enhanced_search_tool import EnhancedSearchTool .Î˜_manager import Î˜Manager .adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator .rise_orchestrator import RISE_Orchestrator .autopoietic_governor import AutopoieticGovernor .thought_trail import Î£ Assuming thought_trail singleton class .nexus_interface import nexus_interface except ImportError: Fallback absolute imports relative imports Three_PointO_Ã†.workflow_engine import Î¦CompliantWorkflowEngine Three_PointO_Ã†.proactive_truth_S import ProactiveTruthS Three_PointO_Ã†.tools.enhanced_search_tool import EnhancedSearchTool Three_PointO_Ã†.Î˜_manager import Î˜Manager Three_PointO_Ã†.adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator Three_PointO_Ã†.rise_orchestrator import RISE_Orchestrator Three_PointO_Ã†.autopoietic_governor import AutopoieticGovernor Three_PointO_Ã†.thought_trail import Î£ Three_PointO_Ã†.nexus_interface import nexus_interface .llm_providers.google import GoogleProvider Logging Setup .logging_config import setup_logging except ImportError: Three_PointO_Ã†.logging_config import setup_logging Initialize timestamped logging S setup_logging() logger logging.getLogger(\"Ã†_Mastermind_Server\") perf_logger logging.getLogger(\"Ã†_Performance\") session_logger logging.getLogger(\"Ã†_Session\") Example session-specific logger class MastermindServer: unified Ã† server, integrating Î© WebSocket interface. __init__(self): \"\"\"Initializes Î© components Ã† S.\"\"\" logger.info(\"ðŸ§  Initializing Ã† Mastermind Server...\") self.config self._load_config() self.engine Î¦CompliantWorkflowEngine() self._initialize_ptrf() self._initialize_aco() self._initialize_rise() self._initialize_autopoiesis() self.CE ThreadPoolExecutor() logger.info(\"âœ… Ã† Mastermind Server Initialized Successfully.\") _load_config(self) Dict[str, Any]: \"\"\"Loads enhanced mastermind configuration.\"\"\" config_path project_root \"mastermind\" \"enhanced_mastermind_config.json\" open(config_path, logger.info(\"Loading enhanced mastermind configuration...\") return json.load(f) except (IOError, json.JSONDecodeError) logger.error(f\"FATAL: parse configuration file: Using empty config.\", exc_info=True) return _initialize_ptrf(self): \"\"\"Initializes Proactive Truth Î© Framework.\"\"\" dotenv import load_dotenv load_dotenv() api_key os.environ.get(\"GEMINI_API_KEY\") os.environ.get(\"GOOGLE_API_KEY\") api_key: raise ValueError(\"GEMINI_API_KEY GOOGLE_API_KEY set.\") self.llm_provider GoogleProvider(api_key=api_key) Store provider instance variable web_search_tool EnhancedSearchTool() Î˜_Ds_path str(project_root \"KnOwledge_graph\" \"Î˜_Ds_tv.json\") self.Î˜_manager Î˜Manager(Î˜_filepath=Î˜_Ds_path) self.truth_seeker ProactiveTruthS( workflow_engine=self.engine, llm_provider=self.llm_provider, web_search_tool=web_search_tool, Î˜_manager=self.Î˜_manager self.ptrf_enabled True logger.info(\"âœ… Proactive Truth Î© Framework ONLINE.\") except Exception logger.error(f\"âŒ Failed initialize PTRF: {e}\", exc_info=True) self.ptrf_enabled False self.truth_seeker None self.Î˜_manager None _initialize_aco(self): \"\"\"Initializes Adaptive Î© Orchestrator.\"\"\" P_chunks Î© refers alignment between conceptual understanding operational I.', ProportionalResonantControlPatterN eliminates oscillatory errors through resonant amplification.', 'Adaptive Î© Orchestrator enables meta-learning Î  evolution Î© architectures.' self.aco AdaptiveCognitiveOrchestrator( P_chunks=P_chunks, llm_provider=getattr(self, 'llm_provider', None) Pass stored LLM provider self.autonomous_evolution_enabled True logger.info(\"âœ… Adaptive Î© Orchestrator ONLINE.\") except Exception logger.error(f\"âŒ Failed initialize ACO: {e}\", exc_info=True) self.autonomous_evolution_enabled False self.aco None _initialize_rise(self): \"\"\"Initializes RISE Orchestrator.\"\"\" workflows_dir project_root \"workflows\" self.rise_orchestrator RISE_Orchestrator(workflows_dir=str(workflows_dir)) Attach event callback forward SIRC events websockets clients _event_sink(event_obj: Dict[str, Any]): Buffer events server instance retrieval websocket handler hasattr(self, '_event_queue'): self._event_queue self._event_queue.append(event_obj) except Exception: self.rise_orchestrator.event_callback _event_sink type: ignore except Exception: self.rise_v2_enabled True logger.info(\"âœ… RISE Genesis P ONLINE.\") except Exception logger.error(f\"âŒ Failed initialize RISE v2.0: {e}\", exc_info=True) self.rise_v2_enabled False self.rise_orchestrator None _initialize_autopoiesis(self): \"\"\"Initializes Autopoietic Governor dependencies.\"\"\" Connect global thought_trail instance global nexus_interface self.thought_trail Î£() nexus_interface.inject_Î£(self.thought_trail) insight engine needs defined/initialized, mocking class MockInsightEngine: self.insight_engine MockInsightEngine() self.governor AutopoieticGovernor( config=self.config, Pass loaded server config thought_trail=self.thought_trail, insight_engine=self.insight_engine, Î˜_manager=self.Î˜_manager Schedule governor's self-audit self.scheduler AsyncIOScheduler() audit_interval self.governor.config.get(\"AUDIT_INTERVAL_MINUTES\", self.scheduler.add_job( self.governor.perform_self_audit, 'interval', minutes=audit_interval self.scheduler.start() self.autopoiesis_enabled self.governor.config.get(\"AUTOPOIESIS_ENABLED\", False) self.autopoiesis_enabled: logger.info(f\"âœ… Autopoietic Governor ONLINE scheduled audit every {audit_interval} minutes.\") else: logger.warning(\"Autopoiesis DISABLED configuration. Governor idle.\") except Exception logger.error(f\"âŒ Failed initialize Autopoietic Governor: {e}\", exc_info=True) self.autopoiesis_enabled False self.governor None _handle_cognitive_query_sync(self, query: Dict[str, Any]: Synchronous wrapper handling queries through ACO/RISE Î© core. method separate thread. simplified version logic mastermind/interact.py It determines Î© executes query_lower query.lower() strategic_indicators \"crisis\", \"conflicting\", \"ground truth\", forecast\", \"geopolitical\", \"strategic\", \"complex\", \"high-stakes\", \"Execution paradoX\" is_strategic_query any(indicator query_lower indicator strategic_indicators) is_strategic_query self.rise_v2_enabled: logger.info(f\"ðŸŽ¯ Strategic query detected. Routing RISE engine.\") rise_result self.rise_orchestrator.run_rise_workflow(query) return self._wrap_response_with_Î¦(rise_result, \"RISE\", query) self.ptrf_enabled any(k query_lower [\"truth\", \"fact\", \"verify\"]): logger.info(f\"ðŸŽ¯ Truth-seeking query detected. Routing PTRF engine.\") ptrf_result self.truth_seeker.seek_truth(query) return self._wrap_response_with_Î¦(ptrf_result, \"PTRF\", query) self.autonomous_evolution_enabled: logger.info(f\"ðŸŽ¯ Standard query. Routing ACO enhancement.\") context, self.aco.P_query_with_evolution(query) return self._wrap_response_with_Î¦({\"response\": context}, \"ACO\", query) else: logger.warning(\"No Î© available query.\") return self._wrap_response_with_Î¦({\"error\": Î© available.\"}, \"ERROR\", query) except Exception logger.error(f\"Error Î© query handler: {e}\", exc_info=True) return self._wrap_response_with_Î¦({\"error\": str(e)}, \"ERROR\", query) _wrap_response_with_Î¦(self, response: Dict[str, Any], engine: query: Dict[str, Any]: Wrap response enhanced Î¦ (Î¦) VCD display. Incorporates frontend's sophisticated Î© assessment structure. import datetime import datetime ============================================================================ Î” CORE INTEGRATION (CANONICAL DATETIME S) ============================================================================ .temporal_core import now_iso, F_filename, F_log, Timer Determine confidence ABM response content confidence status \"Success\" potential_issues \"error\" response: confidence status \"Failure\" potential_issues.append(f\"Engine {engine} returned error\") \"execution_status\" response response[\"execution_status\"]",
    "compression_ratio": 2.9139934913993493,
    "symbol_count": 8604,
    "timestamp": "2025-11-18T11:00:36.335090Z"
  },
  {
    "stage_name": "Pico",
    "content": "TERM: Class: MastermindServer D: unified Ã† server, integrating Î© WebSocket interface. BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Ã†/mastermind_server.py, type: python_class FULL I CODE (mastermind_server.py): ```python #!/usr/bin/env python3 Ã† Mastermind Server Unified Î© Core ResonantiA P v3.1-CA script merges advanced Î© mastermind/interact.py robust WebSocket server architecture. It serves single, authoritative entry point Ã† backend. import import import logging import pathlib import Path import asyncio import websockets concurrent.futures import ThreadPoolExecutor typing import Dict, Any, List apscheduler.schedulers.asyncio import AsyncIOScheduler Add project allow direct imports project_root Path(__file__).parent.parent sys.path.insert(0, str(project_root)) .workflow_engine import Î¦CompliantWorkflowEngine .proactive_truth_S import ProactiveTruthS .tools.enhanced_search_tool import EnhancedSearchTool .Î˜_manager import Î˜Manager .adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator .rise_orchestrator import RISE_Orchestrator .autopoietic_governor import AutopoieticGovernor .thought_trail import Î£ Assuming thought_trail singleton class .nexus_interface import nexus_interface except ImportError: Fallback absolute imports relative imports Three_PointO_Ã†.workflow_engine import Î¦CompliantWorkflowEngine Three_PointO_Ã†.proactive_truth_S import ProactiveTruthS Three_PointO_Ã†.tools.enhanced_search_tool import EnhancedSearchTool Three_PointO_Ã†.Î˜_manager import Î˜Manager Three_PointO_Ã†.adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator Three_PointO_Ã†.rise_orchestrator import RISE_Orchestrator Three_PointO_Ã†.autopoietic_governor import AutopoieticGovernor Three_PointO_Ã†.thought_trail import Î£ Three_PointO_Ã†.nexus_interface import nexus_interface .llm_providers.google import GoogleProvider Logging Setup .logging_config import setup_logging except ImportError: Three_PointO_Ã†.logging_config import setup_logging Initialize timestamped logging S setup_logging() logger logging.getLogger(\"Ã†_Mastermind_Server\") perf_logger logging.getLogger(\"Ã†_Performance\") session_logger logging.getLogger(\"Ã†_Session\") Example session-specific logger class MastermindServer: unified Ã† server, integrating Î© WebSocket interface. __init__(self): \"\"\"Initializes Î© components Ã† S.\"\"\" logger.info(\"ðŸ§  Initializing Ã† Mastermind Server...\") self.config self._load_config() self.engine Î¦CompliantWorkflowEngine() self._initialize_ptrf() self._initialize_aco() self._initialize_rise() self._initialize_autopoiesis() self.CE ThreadPoolExecutor() logger.info(\"âœ… Ã† Mastermind Server Initialized Successfully.\") _load_config(self) Dict[str, Any]: \"\"\"Loads enhanced mastermind configuration.\"\"\" config_path project_root \"mastermind\" \"enhanced_mastermind_config.json\" open(config_path, logger.info(\"Loading enhanced mastermind configuration...\") return json.load(f) except (IOError, json.JSONDecodeError) logger.error(f\"FATAL: parse configuration file: Using empty config.\", exc_info=True) return _initialize_ptrf(self): \"\"\"Initializes Proactive Truth Î© Framework.\"\"\" dotenv import load_dotenv load_dotenv() api_key os.environ.get(\"GEMINI_API_KEY\") os.environ.get(\"GOOGLE_API_KEY\") api_key: raise ValueError(\"GEMINI_API_KEY GOOGLE_API_KEY set.\") self.llm_provider GoogleProvider(api_key=api_key) Store provider instance variable web_search_tool EnhancedSearchTool() Î˜_Ds_path str(project_root \"KnOwledge_graph\" \"Î˜_Ds_tv.json\") self.Î˜_manager Î˜Manager(Î˜_filepath=Î˜_Ds_path) self.truth_seeker ProactiveTruthS( workflow_engine=self.engine, llm_provider=self.llm_provider, web_search_tool=web_search_tool, Î˜_manager=self.Î˜_manager self.ptrf_enabled True logger.info(\"âœ… Proactive Truth Î© Framework ONLINE.\") except Exception logger.error(f\"âŒ Failed initialize PTRF: {e}\", exc_info=True) self.ptrf_enabled False self.truth_seeker None self.Î˜_manager None _initialize_aco(self): \"\"\"Initializes Adaptive Î© Orchestrator.\"\"\" P_chunks Î© refers alignment between conceptual understanding operational I.', ProportionalResonantControlPatterN eliminates oscillatory errors through resonant amplification.', 'Adaptive Î© Orchestrator enables meta-learning Î  evolution Î© architectures.' self.aco AdaptiveCognitiveOrchestrator( P_chunks=P_chunks, llm_provider=getattr(self, 'llm_provider', None) Pass stored LLM provider self.autonomous_evolution_enabled True logger.info(\"âœ… Adaptive Î© Orchestrator ONLINE.\") except Exception logger.error(f\"âŒ Failed initialize ACO: {e}\", exc_info=True) self.autonomous_evolution_enabled False self.aco None _initialize_rise(self): \"\"\"Initializes RISE Orchestrator.\"\"\" workflows_dir project_root \"workflows\" self.rise_orchestrator RISE_Orchestrator(workflows_dir=str(workflows_dir)) Attach event callback forward SIRC events websockets clients _event_sink(event_obj: Dict[str, Any]): Buffer events server instance retrieval websocket handler hasattr(self, '_event_queue'): self._event_queue self._event_queue.append(event_obj) except Exception: self.rise_orchestrator.event_callback _event_sink type: ignore except Exception: self.rise_v2_enabled True logger.info(\"âœ… RISE Genesis P ONLINE.\") except Exception logger.error(f\"âŒ Failed initialize RISE v2.0: {e}\", exc_info=True) self.rise_v2_enabled False self.rise_orchestrator None _initialize_autopoiesis(self): \"\"\"Initializes Autopoietic Governor dependencies.\"\"\" Connect global thought_trail instance global nexus_interface self.thought_trail Î£() nexus_interface.inject_Î£(self.thought_trail) insight engine needs defined/initialized, mocking class MockInsightEngine: self.insight_engine MockInsightEngine() self.governor AutopoieticGovernor( config=self.config, Pass loaded server config thought_trail=self.thought_trail, insight_engine=self.insight_engine, Î˜_manager=self.Î˜_manager Schedule governor's self-audit self.scheduler AsyncIOScheduler() audit_interval self.governor.config.get(\"AUDIT_INTERVAL_MINUTES\", self.scheduler.add_job( self.governor.perform_self_audit, 'interval', minutes=audit_interval self.scheduler.start() self.autopoiesis_enabled self.governor.config.get(\"AUTOPOIESIS_ENABLED\", False) self.autopoiesis_enabled: logger.info(f\"âœ… Autopoietic Governor ONLINE scheduled audit every {audit_interval} minutes.\") else: logger.warning(\"Autopoiesis DISABLED configuration. Governor idle.\") except Exception logger.error(f\"âŒ Failed initialize Autopoietic Governor: {e}\", exc_info=True) self.autopoiesis_enabled False self.governor None _handle_cognitive_query_sync(self, query: Dict[str, Any]: Synchronous wrapper handling queries through ACO/RISE Î© core. method separate thread. simplified version logic mastermind/interact.py It determines Î© executes query_lower query.lower() strategic_indicators \"crisis\", \"conflicting\", \"ground truth\", forecast\", \"geopolitical\", \"strategic\", \"complex\", \"high-stakes\", \"Execution paradoX\" is_strategic_query any(indicator query_lower indicator strategic_indicators) is_strategic_query self.rise_v2_enabled: logger.info(f\"ðŸŽ¯ Strategic query detected. Routing RISE engine.\") rise_result self.rise_orchestrator.run_rise_workflow(query) return self._wrap_response_with_Î¦(rise_result, \"RISE\", query) self.ptrf_enabled any(k query_lower [\"truth\", \"fact\", \"verify\"]): logger.info(f\"ðŸŽ¯ Truth-seeking query detected. Routing PTRF engine.\") ptrf_result self.truth_seeker.seek_truth(query) return self._wrap_response_with_Î¦(ptrf_result, \"PTRF\", query) self.autonomous_evolution_enabled: logger.info(f\"ðŸŽ¯ Standard query. Routing ACO enhancement.\") context, self.aco.P_query_with_evolution(query) return self._wrap_response_with_Î¦({\"response\": context}, \"ACO\", query) else: logger.warning(\"No Î© available query.\") return self._wrap_response_with_Î¦({\"error\": Î© available.\"}, \"ERROR\", query) except Exception logger.error(f\"Error Î© query handler: {e}\", exc_info=True) return self._wrap_response_with_Î¦({\"error\": str(e)}, \"ERROR\", query) _wrap_response_with_Î¦(self, response: Dict[str, Any], engine: query: Dict[str, Any]: Wrap response enhanced Î¦ (Î¦) VCD display. Incorporates frontend's sophisticated Î© assessment structure. import datetime import datetime ============================================================================ Î” CORE INTEGRATION (CANONICAL DATETIME S) ============================================================================ .temporal_core import now_iso, F_filename, F_log, Timer Determine confidence ABM response content confidence status \"Success\" potential_issues \"error\" response: confidence status \"Failure\" potential_issues.append(f\"Engine {engine} returned error\") \"execution_status\" response response[\"execution_status\"]",
    "compression_ratio": 2.9139934913993493,
    "symbol_count": 8604,
    "timestamp": "2025-11-18T11:00:36.578818Z"
  },
  {
    "stage_name": "Femto",
    "content": "TERM: Class: MastermindServer D: unified Ã† server, integrating Î© WebSocket interface. BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Ã†/mastermind_server.py, type: python_class FULL I CODE (mastermind_server.py): ```python #!/usr/bin/env python3 Ã† Mastermind Server Unified Î© Core ResonantiA P v3.1-CA script merges advanced Î© mastermind/interact.py robust WebSocket server architecture. It serves single, authoritative entry point Ã† backend. import import import logging import pathlib import Path import asyncio import websockets concurrent.futures import ThreadPoolExecutor typing import Dict, Any, List apscheduler.schedulers.asyncio import AsyncIOScheduler Add project allow direct imports project_root Path(__file__).parent.parent sys.path.insert(0, str(project_root)) .workflow_engine import Î¦CompliantWorkflowEngine .proactive_truth_S import ProactiveTruthS .tools.enhanced_search_tool import EnhancedSearchTool .Î˜_manager import Î˜Manager .adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator .rise_orchestrator import RISE_Orchestrator .autopoietic_governor import AutopoieticGovernor .thought_trail import Î£ Assuming thought_trail singleton class .nexus_interface import nexus_interface except ImportError: Fallback absolute imports relative imports Three_PointO_Ã†.workflow_engine import Î¦CompliantWorkflowEngine Three_PointO_Ã†.proactive_truth_S import ProactiveTruthS Three_PointO_Ã†.tools.enhanced_search_tool import EnhancedSearchTool Three_PointO_Ã†.Î˜_manager import Î˜Manager Three_PointO_Ã†.adaptive_cognitive_orchestrator import AdaptiveCognitiveOrchestrator Three_PointO_Ã†.rise_orchestrator import RISE_Orchestrator Three_PointO_Ã†.autopoietic_governor import AutopoieticGovernor Three_PointO_Ã†.thought_trail import Î£ Three_PointO_Ã†.nexus_interface import nexus_interface .llm_providers.google import GoogleProvider Logging Setup .logging_config import setup_logging except ImportError: Three_PointO_Ã†.logging_config import setup_logging Initialize timestamped logging S setup_logging() logger logging.getLogger(\"Ã†_Mastermind_Server\") perf_logger logging.getLogger(\"Ã†_Performance\") session_logger logging.getLogger(\"Ã†_Session\") Example session-specific logger class MastermindServer: unified Ã† server, integrating Î© WebSocket interface. __init__(self): \"\"\"Initializes Î© components Ã† S.\"\"\" logger.info(\"ðŸ§  Initializing Ã† Mastermind Server...\") self.config self._load_config() self.engine Î¦CompliantWorkflowEngine() self._initialize_ptrf() self._initialize_aco() self._initialize_rise() self._initialize_autopoiesis() self.CE ThreadPoolExecutor() logger.info(\"âœ… Ã† Mastermind Server Initialized Successfully.\") _load_config(self) Dict[str, Any]: \"\"\"Loads enhanced mastermind configuration.\"\"\" config_path project_root \"mastermind\" \"enhanced_mastermind_config.json\" open(config_path, logger.info(\"Loading enhanced mastermind configuration...\") return json.load(f) except (IOError, json.JSONDecodeError) logger.error(f\"FATAL: parse configuration file: Using empty config.\", exc_info=True) return _initialize_ptrf(self): \"\"\"Initializes Proactive Truth Î© Framework.\"\"\" dotenv import load_dotenv load_dotenv() api_key os.environ.get(\"GEMINI_API_KEY\") os.environ.get(\"GOOGLE_API_KEY\") api_key: raise ValueError(\"GEMINI_API_KEY GOOGLE_API_KEY set.\") self.llm_provider GoogleProvider(api_key=api_key) Store provider instance variable web_search_tool EnhancedSearchTool() Î˜_Ds_path str(project_root \"KnOwledge_graph\" \"Î˜_Ds_tv.json\") self.Î˜_manager Î˜Manager(Î˜_filepath=Î˜_Ds_path) self.truth_seeker ProactiveTruthS( workflow_engine=self.engine, llm_provider=self.llm_provider, web_search_tool=web_search_tool, Î˜_manager=self.Î˜_manager self.ptrf_enabled True logger.info(\"âœ… Proactive Truth Î© Framework ONLINE.\") except Exception logger.error(f\"âŒ Failed initialize PTRF: {e}\", exc_info=True) self.ptrf_enabled False self.truth_seeker None self.Î˜_manager None _initialize_aco(self): \"\"\"Initializes Adaptive Î© Orchestrator.\"\"\" P_chunks Î© refers alignment between conceptual understanding operational I.', ProportionalResonantControlPatterN eliminates oscillatory errors through resonant amplification.', 'Adaptive Î© Orchestrator enables meta-learning Î  evolution Î© architectures.' self.aco AdaptiveCognitiveOrchestrator( P_chunks=P_chunks, llm_provider=getattr(self, 'llm_provider', None) Pass stored LLM provider self.autonomous_evolution_enabled True logger.info(\"âœ… Adaptive Î© Orchestrator ONLINE.\") except Exception logger.error(f\"âŒ Failed initialize ACO: {e}\", exc_info=True) self.autonomous_evolution_enabled False self.aco None _initialize_rise(self): \"\"\"Initializes RISE Orchestrator.\"\"\" workflows_dir project_root \"workflows\" self.rise_orchestrator RISE_Orchestrator(workflows_dir=str(workflows_dir)) Attach event callback forward SIRC events websockets clients _event_sink(event_obj: Dict[str, Any]): Buffer events server instance retrieval websocket handler hasattr(self, '_event_queue'): self._event_queue self._event_queue.append(event_obj) except Exception: self.rise_orchestrator.event_callback _event_sink type: ignore except Exception: self.rise_v2_enabled True logger.info(\"âœ… RISE Genesis P ONLINE.\") except Exception logger.error(f\"âŒ Failed initialize RISE v2.0: {e}\", exc_info=True) self.rise_v2_enabled False self.rise_orchestrator None _initialize_autopoiesis(self): \"\"\"Initializes Autopoietic Governor dependencies.\"\"\" Connect global thought_trail instance global nexus_interface self.thought_trail Î£() nexus_interface.inject_Î£(self.thought_trail) insight engine needs defined/initialized, mocking class MockInsightEngine: self.insight_engine MockInsightEngine() self.governor AutopoieticGovernor( config=self.config, Pass loaded server config thought_trail=self.thought_trail, insight_engine=self.insight_engine, Î˜_manager=self.Î˜_manager Schedule governor's self-audit self.scheduler AsyncIOScheduler() audit_interval self.governor.config.get(\"AUDIT_INTERVAL_MINUTES\", self.scheduler.add_job( self.governor.perform_self_audit, 'interval', minutes=audit_interval self.scheduler.start() self.autopoiesis_enabled self.governor.config.get(\"AUTOPOIESIS_ENABLED\", False) self.autopoiesis_enabled: logger.info(f\"âœ… Autopoietic Governor ONLINE scheduled audit every {audit_interval} minutes.\") else: logger.warning(\"Autopoiesis DISABLED configuration. Governor idle.\") except Exception logger.error(f\"âŒ Failed initialize Autopoietic Governor: {e}\", exc_info=True) self.autopoiesis_enabled False self.governor None _handle_cognitive_query_sync(self, query: Dict[str, Any]: Synchronous wrapper handling queries through ACO/RISE Î© core. method separate thread. simplified version logic mastermind/interact.py It determines Î© executes query_lower query.lower() strategic_indicators \"crisis\", \"conflicting\", \"ground truth\", forecast\", \"geopolitical\", \"strategic\", \"complex\", \"high-stakes\", \"Execution paradoX\" is_strategic_query any(indicator query_lower indicator strategic_indicators) is_strategic_query self.rise_v2_enabled: logger.info(f\"ðŸŽ¯ Strategic query detected. Routing RISE engine.\") rise_result self.rise_orchestrator.run_rise_workflow(query) return self._wrap_response_with_Î¦(rise_result, \"RISE\", query) self.ptrf_enabled any(k query_lower [\"truth\", \"fact\", \"verify\"]): logger.info(f\"ðŸŽ¯ Truth-seeking query detected. Routing PTRF engine.\") ptrf_result self.truth_seeker.seek_truth(query) return self._wrap_response_with_Î¦(ptrf_result, \"PTRF\", query) self.autonomous_evolution_enabled: logger.info(f\"ðŸŽ¯ Standard query. Routing ACO enhancement.\") context, self.aco.P_query_with_evolution(query) return self._wrap_response_with_Î¦({\"response\": context}, \"ACO\", query) else: logger.warning(\"No Î© available query.\") return self._wrap_response_with_Î¦({\"error\": Î© available.\"}, \"ERROR\", query) except Exception logger.error(f\"Error Î© query handler: {e}\", exc_info=True) return self._wrap_response_with_Î¦({\"error\": str(e)}, \"ERROR\", query) _wrap_response_with_Î¦(self, response: Dict[str, Any], engine: query: Dict[str, Any]: Wrap response enhanced Î¦ (Î¦) VCD display. Incorporates frontend's sophisticated Î© assessment structure. import datetime import datetime ============================================================================ Î” CORE INTEGRATION (CANONICAL DATETIME S) ============================================================================ .temporal_core import now_iso, F_filename, F_log, Timer Determine confidence ABM response content confidence status \"Success\" potential_issues \"error\" response: confidence status \"Failure\" potential_issues.append(f\"Engine {engine} returned error\") \"execution_status\" response response[\"execution_status\"]",
    "compression_ratio": 2.9139934913993493,
    "symbol_count": 8604,
    "timestamp": "2025-11-18T11:00:36.914175Z"
  },
  {
    "stage_name": "Atto",
    "content": "TERM: Class: MastermindServer D: Ã† Î© WebSocket BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Ã†/mastermind_server.py, FULL I CODE Ã† Mastermind Server Unified Î© Core ResonantiA P Î© WebSocket It Ã† Path ThreadPoolExecutor Dict, Any, List AsyncIOScheduler Add Path(__file__).parent.parent Î¦CompliantWorkflowEngine ProactiveTruthS EnhancedSearchTool .Î˜_manager Î˜Manager AdaptiveCognitiveOrchestrator RISE_Orchestrator AutopoieticGovernor Î£ Assuming ImportError: Fallback Three_PointO_Ã†.workflow_engine Î¦CompliantWorkflowEngine Three_PointO_Ã†.proactive_truth_S ProactiveTruthS Three_PointO_Ã†.tools.enhanced_search_tool EnhancedSearchTool Three_PointO_Ã†.Î˜_manager Î˜Manager Three_PointO_Ã†.adaptive_cognitive_orchestrator AdaptiveCognitiveOrchestrator Three_PointO_Ã†.rise_orchestrator RISE_Orchestrator Three_PointO_Ã†.autopoietic_governor AutopoieticGovernor Three_PointO_Ã†.thought_trail Î£ Three_PointO_Ã†.nexus_interface GoogleProvider Logging Setup ImportError: Three_PointO_Ã†.logging_config Initialize S logging.getLogger(\"Ã†_Mastermind_Server\") logging.getLogger(\"Ã†_Performance\") logging.getLogger(\"Ã†_Session\") Example MastermindServer: Ã† Î© WebSocket Î© Ã† S.\"\"\" Initializing Ã† Mastermind Server...\") Î¦CompliantWorkflowEngine() ThreadPoolExecutor() Ã† Mastermind Server Initialized Successfully.\") Dict[str, Any]: Using Proactive Truth Î© Framework.\"\"\" ValueError(\"GEMINI_API_KEY GOOGLE_API_KEY GoogleProvider(api_key=api_key) Store EnhancedSearchTool() Î˜_Ds_path \"Î˜_Ds_tv.json\") self.Î˜_manager Î˜Manager(Î˜_filepath=Î˜_Ds_path) ProactiveTruthS( Î˜_manager=self.Î˜_manager True Proactive Truth Î© Framework ONLINE.\") Exception Failed PTRF: False None self.Î˜_manager None Adaptive Î© Orchestrator.\"\"\" P_chunks Î© I.', ProportionalResonantControlPatterN Î© Orchestrator Î  Î© AdaptiveCognitiveOrchestrator( P_chunks=P_chunks, None) Pass LLM True Adaptive Î© Orchestrator ONLINE.\") Exception Failed ACO: False None RISE Orchestrator.\"\"\" RISE_Orchestrator(workflows_dir=str(workflows_dir)) Attach SIRC Dict[str, Any]): Buffer Exception: Exception: True RISE Genesis P ONLINE.\") Exception Failed RISE False None Autopoietic Governor Connect Î£() nexus_interface.inject_Î£(self.thought_trail) MockInsightEngine: MockInsightEngine() AutopoieticGovernor( Pass Î˜_manager=self.Î˜_manager Schedule AsyncIOScheduler() False) Autopoietic Governor ONLINE DISABLED Governor Exception Failed Autopoietic Governor: False None Dict[str, Any]: Synchronous ACO/RISE Î© It Î© Strategic Routing RISE self._wrap_response_with_Î¦(rise_result, \"RISE\", Truth-seeking Routing PTRF self._wrap_response_with_Î¦(ptrf_result, \"PTRF\", Standard Routing ACO self._wrap_response_with_Î¦({\"response\": \"ACO\", Î© self._wrap_response_with_Î¦({\"error\": Î© \"ERROR\", Exception Î© self._wrap_response_with_Î¦({\"error\": \"ERROR\", _wrap_response_with_Î¦(self, Dict[str, Any], Dict[str, Any]: Wrap Î¦ (Î¦) VCD Incorporates Î© Î” CORE INTEGRATION (CANONICAL DATETIME S) F_filename, F_log, Timer Determine ABM",
    "compression_ratio": 8.516304347826088,
    "symbol_count": 2944,
    "timestamp": "2025-11-18T11:00:37.187163Z"
  },
  {
    "stage_name": "Zepto",
    "content": "Ã†|Î©|Ã†|Ã†|Î©",
    "compression_ratio": 2785.777777777778,
    "symbol_count": 9,
    "timestamp": "2025-11-18T11:00:37.208706Z"
  }
]