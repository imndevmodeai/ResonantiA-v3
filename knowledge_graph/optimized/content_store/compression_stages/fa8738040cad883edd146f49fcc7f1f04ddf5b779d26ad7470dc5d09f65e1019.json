[
  {
    "stage_name": "Narrative",
    "content": "TERM: Class: IARValidator\n\nDEFINITION:\nValidates IAR structure compliance per crystallized artifacts specification\n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/workflow_engine.py, type: python_class\n\nIMPLEMENTATION CODE (workflow_engine.py) - First 30KB:\n```python\n# ResonantiA Protocol v3.0 - workflow_engine.py\n# Orchestrates the execution of defined workflows (Process Blueprints).\n# Manages context, dependencies, conditions, action execution, and error handling.\n# Critically handles Integrated Action Reflection (IAR) results by storing\n# the complete action output dictionary (including 'reflection') in the\n# context.\n\nimport json\nimport os\nimport logging\nimport copy\nimport time\nimport re\nimport uuid\nimport tempfile\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional, Set, Union, Tuple, Callable\nfrom jinja2 import Environment, meta, exceptions # Import Jinja2\n\n# --- Standardized Imports ---\n# This single block ensures robust relative imports within the package.\ntry:\n    from . import config\n    from .action_registry import execute_action, main_action_registry\n    from .spr_manager import SPRManager\n    from .error_handler import handle_action_error\n    from .action_context import ActionContext\n    from .workflow_recovery import WorkflowRecoveryHandler\n    from .recovery_actions import (\n        analyze_failure,\n        fix_template,\n        fix_action,\n        validate_workflow,\n        validate_action,\n        self_heal_output\n    )\n    from .system_genesis_tool import perform_system_genesis_action\n    from .qa_tools import run_code_linter, run_workflow_suite\n    from .output_handler import print_tagged_execution, print_tagged_results, display_output, display_workflow_start, display_workflow_progress, display_task_result, display_workflow_complete, display_workflow_error\n    from .custom_json import dumps\n    from .thought_trail import log_to_thought_trail\nexcept ImportError as e:\n    # This block should ideally not be hit if run as part of the package,\n    # but serves as a fallback and clear error indicator.\n    import logging\n    _import_error_msg = str(e)  # Capture error message as string\n    logging.getLogger(__name__).critical(f\"A critical relative import failed in workflow_engine: {e}\", exc_info=True)\n    # Set WorkflowOptimizer to None so code can check for it\n    WorkflowOptimizer = None\n    # Define dummy fallbacks to prevent outright crashing where possible\n    def execute_action(*args, **kwargs): return {\"error\": f\"Action Registry not available due to import error: {_import_error_msg}\"}\n    # Add other necessary fallbacks here as needed...\n    class SPRManager: pass\n    # Ensure decorator name is defined to avoid NameError in class annotations\n    def log_to_thought_trail(func): \n        return func\n\n# Optional import for WorkflowOptimizer (requires networkx)\ntry:\n    from .workflow_optimizer import WorkflowOptimizer\nexcept ImportError:\n    WorkflowOptimizer = None\n\nimport ast\nfrom datetime import datetime\n\n# ============================================================================\n# TEMPORAL CORE INTEGRATION (CANONICAL DATETIME SYSTEM)\n# ============================================================================\nfrom Three_PointO_ArchE.temporal_core import now, now_iso, ago, from_now, format_log, format_filename\n\n\n# Attempt to import numpy for numeric type checking in _compare_values,\n# optional\ntry:\n    import numpy as np\nexcept ImportError:\n    np = None  # Set to None if numpy is not available\n    logging.info(\n        \"Numpy not found, some numeric type checks in _compare_values might be limited.\")\n\nlogger = logging.getLogger(__name__)\n\n\n# === IAR COMPLIANCE ENHANCEMENT ===\n# Crystallized Artifacts Implementation - ARTIFACT 4A\n\nclass IARValidator:\n    \"\"\"Validates IAR structure compliance per crystallized artifacts specification\"\"\"\n    \n    def __init__(self):\n        self.required_fields = [\n            'status', 'summary', 'confidence', \n            'alignment_check', 'potential_issues',\n            'raw_output_preview'\n        ]\n        self.enhanced_fields = [\n            'tactical_resonance', 'crystallization_potential'\n        ]\n    \n    def validate_structure(self, iar_data):\n        \"\"\"Validate IAR structure meets all requirements\"\"\"\n        if not isinstance(iar_data, dict):\n            return False, [\"IAR must be a dictionary\"]\n        \n        missing_fields = []\n        for field in self.required_fields:\n            if field not in iar_data:\n                missing_fields.append(field)\n        \n        issues = []\n        if missing_fields:\n            issues.extend(\n                [f\"Missing required field: {field}\" for field in missing_fields])\n        \n        # Validate confidence is float between 0-1\n        confidence = iar_data.get('confidence')\n        if confidence is not None:\n            if not isinstance(\n    confidence, (int, float)) or not (\n        0.0 <= confidence <= 1.0):\n                issues.append(\"Confidence must be float between 0.0 and 1.0\")\n        \n        # Validate status is valid\n        status = iar_data.get('status')\n        if status not in ['Success', 'Partial', 'Failed']:\n            issues.append(\"Status must be 'Success', 'Partial', or 'Failed'\")\n        \n        return len(issues) == 0, issues\n    \n    def validate_enhanced_fields(self, iar_data):\n        \"\"\"Validate enhanced IAR fields for tactical resonance\"\"\"\n        enhanced_issues = []\n        \n        for field in self.enhanced_fields:\n            if field in iar_data:\n                value = iar_data[field]\n                if not isinstance(\n    value, (int, float)) or not (\n        0.0 <= value <= 1.0):\n                    enhanced_issues.append(\n    f\"{field} must be float between 0.0 and 1.0\")\n        \n        return len(enhanced_issues) == 0, enhanced_issues\n\n\nclass ResonanceTracker:\n    \"\"\"Tracks tactical resonance and crystallization metrics\"\"\"\n    \n    def __init__(self):\n        self.execution_history = []\n        self.resonance_metrics = {\n            'avg_tactical_resonance': 0.0,\n            'avg_crystallization_potential': 0.0,\n            'total_executions': 0\n        }\n    \n    def record_execution(self, task_id, iar_data, context):\n        \"\"\"Record task execution for resonance tracking\"\"\"\n        execution_record = {\n            'timestamp': now_iso(),\n            'task_id': task_id,\n            'status': iar_data.get('status'),\n            'confidence': iar_data.get('confidence', 0.0),\n            'tactical_resonance': iar_data.get('tactical_resonance', 0.0),\n            'crystallization_potential': iar_data.get('crystallization_potential', 0.0)\n        }\n        \n        self.execution_history.append(execution_record)\n        self._update_metrics()\n        \n        return execution_record\n    \n    def _update_metrics(self):\n        \"\"\"Update aggregate resonance metrics\"\"\"\n        if not self.execution_history:\n            return\n        \n        # Last 100 executions\n        recent_executions = self.execution_history[-100:]\n\n        tactical_scores = [ex.get('tactical_resonance', 0.0)\n                                  for ex in recent_executions]\n        crystallization_scores = [\n    ex.get(\n        'crystallization_potential',\n         0.0) for ex in recent_executions]\n        \n        self.resonance_metrics = {\n            'avg_tactical_resonance': sum(tactical_scores) / len(tactical_scores),\n            'avg_crystallization_potential': sum(crystallization_scores) / len(crystallization_scores),\n            'total_executions': len(self.execution_history)\n        }\n    \n    def get_resonance_report(self):\n        \"\"\"Get current resonance metrics report\"\"\"\n        return {\n            'current_metrics': self.resonance_metrics,\n            'recent_trend': self._calculate_trend(),\n            'compliance_score': self._calculate_compliance_score()\n        }\n    \n    def _calculate_trend(self):\n        \"\"\"Calculate resonance trend over recent executions\"\"\"\n        if len(self.execution_history) < 10:\n            return \"insufficient_data\"\n        \n        recent_10 = self.execution_history[-10:]\n        older_10 = self.execution_history[-20:-10]\n        \n        recent_avg = sum(ex.get('tactical_resonance', 0.0)\n                         for ex in recent_10) / 10\n        older_avg = sum(ex.get('tactical_resonance', 0.0)\n                        for ex in older_10) / 10\n        \n        if recent_avg > older_avg + 0.05:\n            return \"improving\"\n        elif recent_avg < older_avg - 0.05:\n            return \"declining\"\n        else:\n            return \"stable\"\n    \n    def _calculate_compliance_score(self):\n        \"\"\"Calculate overall IAR compliance score\"\"\"\n        if not self.execution_history:\n            return 0.0\n        \n        recent_executions = self.execution_history[-50:]\n        successful_executions = [\n    ex for ex in recent_executions if ex.get('status') == 'Success']\n        \n        success_rate = len(successful_executions) / len(recent_executions)\n        avg_confidence = sum(ex.get('confidence', 0.0)\n                             for ex in successful_executions) / max(len(successful_executions), 1)\n        avg_resonance = self.resonance_metrics['avg_tactical_resonance']\n        \n        # Weighted compliance score\n        compliance_score = (success_rate * 0.4) + \\\n                            (avg_confidence * 0.3) + (avg_resonance * 0.3)\n        return min(compliance_score, 1.0)\n\n# === Session State Manager ===\ntry:\n    from .session_state_manager import load_session_state, save_session_state, append_fact\n    from .context_superposition import create_context_bundle, merge_bundles\n    from .prefetch_manager import trigger_predictive_prefetch\n    from .sirc_autonomy import maybe_autorun_sirc\n    from .causal_digest import build_flux_annotated_digest\nexcept Exception:\n    # Fallback no-ops if module not available\n    def load_session_state():\n        return {\"facts_ledger\": [], \"updated_at\": now_iso()}\n    def save_session_state(state):\n        return None\n    def append_fact(state, fact):\n        state.setdefault(\"facts_ledger\", []).append({**fact, \"ts\": now_iso()})\n    def create_context_bundle(spr_def, runtime_context, initial_context):\n        return {\"spr_id\": spr_def.get(\"spr_id\"), \"created_at\": now_iso()}\n    def merge_bundles(bundles):\n        return {\"spr_index\": [b.get(\"spr_id\") for b in bundles]}\n\n\ndef _execute_standalone_workflow(workflow_definition: Dict[str, Any], initial_context: Dict[str, Any], parent_run_id: str, action_registry: Dict[str, Callable]) -> Dict[str, Any]:\n    \"\"\"\n    Executes a workflow definition in-memory, for use by meta-actions like for_each.\n    This is a simplified version of the main run_workflow loop, moved outside the class\n    to prevent circular dependencies.\n    \"\"\"\n    run_id = f\"{parent_run_id}_sub_{uuid.uuid4().hex[:8]}\"\n    initial_context[\"workflow_run_id\"] = run_id\n    \n    runtime_context = {\n        \"initial_context\": initial_context,\n        \"workflow_run_id\": run_id,\n        \"workflow_definition\": workflow_definition,\n    }\n    \n    tasks = workflow_definition.get('tasks', {})\n    task_statuses = {key: \"pending\" for key in tasks}\n    \n    sorted_tasks = list(tasks.keys()) \n\n    logger.info(f\"Starting standalone sub-workflow (Run ID: {run_id}).\")\n    start_time = time.time()\n\n    for task_key in sorted_tasks:\n        task_info = tasks[task_key]\n        action_type = task_info.get(\"action_type\")\n        \n        # This is a critical simplification: we can't use the full resolver here\n        # without re-introducing the dependency. We'll manually resolve for now.\n        resolved_inputs = {}\n        for k, v in task_info.get('inputs', {}).items():\n            if isinstance(v, str) and v == \"{{ item }}\":\n                resolved_inputs[k] = initial_context.get('item')\n            else:\n                resolved_inputs[k] = v\n\n        action_func = action_registry.get(action_type)\n        if not action_func:\n            error_result = {\"error\": f\"Sub-workflow failed: Action type '{action_type}' not found.\"}\n            runtime_context[task_key] = self._ensure_iar_compliance(error_result, action_context_obj)\n            task_statuses[task_key] = \"failed\"\n            break\n\n        try:\n            # The arguments must be passed as keyword arguments\n            # to align with the action wrappers in action_registry.py\n            result = action_func(**resolved_inputs)\n            if not isinstance(result, dict):\n                result = {\"output\": result}\n            runtime_context[task_key] = self._ensure_iar_compliance(result, action_context_obj)\n            task_statuses[task_key] = \"completed\"\n        except Exception as e:\n            error_msg = f\"Sub-workflow task '{task_key}' failed: {e}\"\n            logger.error(error_msg, exc_info=True)\n            error_result = {\"error\": error_msg}\n            runtime_context[task_key] = self._ensure_iar_compliance(error_result, action_context_obj)\n            task_statuses[task_key] = \"failed\"\n            break\n\n    end_time = time.time()\n    run_duration = round(end_time - start_time, 2)\n    \n    final_status = \"Completed\" if all(s == \"completed\" for s in task_statuses.values()) else \"Failed\"\n    \n    # Can't call _summarize_run directly, so construct a simplified summary\n    return {\n        \"workflow_name\": workflow_definition.get(\"name\", \"sub-workflow\"),\n        \"run_id\": run_id,\n        \"status\": final_status,\n        \"duration\": run_duration,\n        \"task_statuses\": task_statuses,\n        \"results\": runtime_context\n    }\n\n\nclass IARCompliantWorkflowEngine:\n    \"\"\"Enhanced workflow engine with IAR compliance and recovery support.\"\"\"\n\n    def __init__(self, workflows_dir: str = \"workflows\", spr_manager=None, event_callback: Optional[Callable] = None):\n        self.workflows_dir = workflows_dir\n        self.spr_manager = spr_manager\n        self.event_callback = event_callback  # <-- ADDED\n        self.last_workflow_name = None\n        self.action_registry = main_action_registry.actions.copy()  # Use centralized registry\n        self.recovery_handler = None\n        self.current_run_id = None\n        self.current_workflow = None\n        self.iar_validator = IARValidator()\n        self.resonance_tracker = ResonanceTracker()\n        self.jinja_env = Environment() # Initialize Jinja2 environment\n\n        # Register standard actions\n        self.register_action(\"display_output\", display_output)\n        self.register_action(\"perform_system_genesis_action\",\n     perform_system_genesis_action)\n        self.register_action(\"run_code_linter\", run_code_linter)\n        self.register_action(\"run_workflow_suite\", run_workflow_suite)\n\n        # Register recovery actions\n        self.register_recovery_actions()\n        logger.info(\n            \"IARCompliantWorkflowEngine initialized with full vetting capabilities\")\n\n    @log_to_thought_trail\n    def register_action(self, action_type: str, action_func: Callable) -> None:\n        \"\"\"Register an action function with the engine.\"\"\"\n        main_action_registry.register_action(\n            action_type,\n            action_func,\n        )\n        self.action_registry = main_action_registry.actions.copy()  # Update local copy\n        logger.debug(f\"Registered action: {action_type}\")\n\n    def register_recovery_actions(self) -> None:\n        \"\"\"Register recovery-related actions.\"\"\"\n        self.register_action(\"analyze_failure\", analyze_failure)\n        self.register_action(\"fix_template\", fix_template)\n        self.register_action(\"fix_action\", fix_action)\n        self.register_action(\"validate_workflow\", validate_workflow)\n        self.register_action(\"validate_action\", validate_action)\n        self.register_action(\"self_heal_output\", self_heal_output)\n        # Register the new for_each meta-action\n        self.register_action(\"for_each\", self._execute_for_each_task_wrapper)\n\n    def _execute_for_each_task_wrapper(self, inputs: Dict[str, Any], **kwargs) -> Dict[str, Any]:\n        \"\"\"\n        Wrapper for for_each action that can be called from action registry.\n        Creates a minimal ActionContext for the actual implementation.\n        \"\"\"\n        # Create a minimal ActionContext for the for_each implementation\n        context_for_action = ActionContext(\n            workflow_name=\"for_each_wrapper\",\n            task_name=\"for_each\",\n            runtime_context=kwargs.get('runtime_context', {}),\n            initial_context=kwargs.get('initial_context', {})\n        )\n        return self._execute_for_each_task(inputs, context_for_action)\n\n    def _execute_for_each_task(self, inputs: Dict[str, Any], context_for_action: ActionContext) -> Dict[str, Any]:\n        \"\"\"\n        Executes a sub-workflow for each item in a list.\n        This is a meta-action handled directly by the engine.\n        \"\"\"\n        logger.info(f\"Executing for_each task in workflow '{context_for_action.workflow_name}'\")\n        \n        items_list = inputs.get('items')\n        sub_workflow_def = inputs.get('workflow')\n\n        if not isinstance(items_list, list):\n            return {\"error\": f\"Input 'items' for for_each must be a list, but got {type(items_list)}.\"}\n        if not isinstance(sub_workflow_def, dict) or 'tasks' not in sub_workflow_def:\n            return {\"error\": \"Input 'workflow' for for_each must be a valid workflow definition with a 'tasks' section.\"}\n\n        all_results = []\n        \n        for index, item in enumerate(items_list):\n            logger.info(f\"Executing for_each iteration {index + 1}/{len(items_list)}.\")\n            \n            # The initial context for the sub-workflow includes the main context\n            # plus the current item.\n            sub_initial_context = copy.deepcopy(context_for_action.runtime_context.get('initial_context', {}))\n            sub_initial_context['item'] = item\n            sub_initial_context['loop_index'] = index\n            \n            # Run the sub-workflow directly without creating a new engine instance\n            sub_workflow_result = _execute_standalone_workflow(\n                workflow_definition=sub_workflow_def,\n                initial_context=sub_initial_context,\n                parent_run_id=context_for_action.run_id,\n                action_registry=self.action_registry\n            )\n            all_results.append(sub_workflow_result)\n        \n        logger.info(\"Completed all for_each iterations.\")\n        return {\"results\": all_results}\n    \n    @log_to_thought_trail\n    def _execute_task(self, task: Dict[str, Any],\n                      runtime_context: Dict[str, Any], initial_context: Dict[str, Any] = None, task_key: str = None, workflow_name: str = None, run_id: str = None) -> Dict[str, Any]:\n        \"\"\"Execute a single task with proper action type handling.\"\"\"\n        action_type = task.get(\"action_type\")\n        if not action_type:\n            raise ValueError(\"Task must specify action_type\")\n\n        if action_type not in self.action_registry:\n            raise ValueError(f\"Unknown action type: {action_type}\")\n\n        action_func = self.action_registry[action_type]\n        \n        # Build combined context for template resolution (initial_context + runtime_context)\n        combined_context = {**(initial_context or {}), **runtime_context}\n        \n        # Use the robust resolver that supports embedded placeholders and context paths\n        resolved_inputs = self._resolve_inputs(\n            task.get('inputs'),\n            runtime_context,  # runtime_context\n            initial_context,\n            task_key\n        )\n        \n        # Create ActionContext for actions that need it (like string_template_action)\n        action_context_obj = ActionContext(\n            task_key=task_key or \"unknown\",\n            action_name=task_key or \"unknown\",\n            action_type=action_type,\n            workflow_name=workflow_name or \"unknown\",\n            run_id=run_id or str(uuid.uuid4()),\n            attempt_number=1,\n            max_attempts=1,\n            execution_start_time=now(),\n            runtime_context=combined_context,\n            initial_context=initial_context or {}\n        )\n        \n        # Pass context_for_action to actions that need it\n        if action_type == \"string_template\":\n            resolved_inputs[\"context_for_action\"] = action_context_obj\n\n        try:\n            result = action_func(**resolved_inputs)\n            if not isinstance(result, dict):\n                result = {\"output\": result}\n            return result\n        except Exception as e:\n            error_msg = f\"Task '{task.get('description', 'Unknown')}' failed: {str(e)}\"\n            logger.error(error_msg)\n            \n            # Enhanced error reporting with input validation\n            if \"Missing required input\" in str(e):\n                logger.error(f\"Input validation failed for task. Provided inputs: {list(resolved_inputs.keys())}\")\n            elif \"NoneType\" in str(e):\n                logger.error(f\"Null value error - check if previous task outputs are properly formatted\")\n            \n            raise ValueError(error_msg)\n\n    def _resolve_template_variables(self, inputs: Dict[str, Any], results: Dict[str, Any], initial_context: Dict[str, Any] = None) -> Dict[str, Any]:\n        \"\"\"Resolve template variables in task inputs.\"\"\"\n        resolved = {}\n        initial_context = initial_context or {}\n        for key, value in inputs.items():\n            if isinstance(value, str) and \"{{\" in value and \"}}\" in value:\n                # Extract task and field from template\n                template = value.strip(\"{} \")\n                \n                # NEW: Add support for a default filter\n                default_value = None\n                if \"|\" in template:\n                    parts = template.split(\"|\", 1)\n                    template = parts[0].strip()\n                    filter_part = parts[1].strip()\n                    if filter_part.startswith(\"default(\"):\n                        default_value = filter_part[len(\"default(\"):-1].strip()\n                        # Basic handling for quotes\n                        if default_value.startswith((\"'\", '\"')) and default_value.endswith((\"'\", '\"')):\n                            default_value = default_value[1:-1]\n\n                parts = template.split(\".\", 1)\n                \n                # NEW: Add support for initial_context via `context.` prefix\n                if parts[0] == 'context':\n                    context_key = parts[1] if len(parts) > 1 else None\n                    # Check if key exists directly in initial_context (e.g., model, provider)\n                    if context_key and context_key in initial_context:\n                        resolved[key] = initial_context[context_key]\n                    # Also check if there's a nested 'context' dict\n                    elif context_key and 'context' in initial_context and isinstance(initial_context['context'], dict):\n                        if context_key in initial_context['context']:\n                            resolved[key] = initial_context['context'][context_key]\n                        else:\n                            resolved[key] = default_value if default_value is not None else value\n                    else:\n                        resolved[key] = default_value if default_value is not None else value\n                    continue\n\n                if len(parts) == 2:\n                    task_id, field_path = parts\n                else:\n                    task_id, field_path = parts[0], \"\"\n                \n                # Support nested field resolution (e.g., result.patterns)\n                if task_id in results:\n                    field_value = results[task_id]\n                    if field_path:\n                        for subfield in field_path.split('.'):\n                            if isinstance(field_value, dict) and subfield in field_value:\n                                field_value = field_value[subfield]\n                            else:\n                                field_value = None\n                                break\n                    if field_value is not None:\n                        resolved[key] = field_value\n                    else:\n                        resolved[key] = default_value if default_value is not None else value\n                else:\n                    resolved[key] = default_value if default_value is not None else value\n            else:\n                resolved[key] = value\n        return resolved\n\n    def get_resonance_dashboard(self) -> Dict[str, Any]:\n        \"\"\"Get the current resonance dashboard.\"\"\"\n        return {\n            \"run_id\": self.current_run_id,\n            \"workflow\": self.last_workflow_name,\n            \"resonance_report\": self.resonance_tracker.get_resonance_report(),\n            \"iar_validation\": self.iar_validator.get_validation_status() if hasattr(self.iar_validator, 'get_validation_status') else None\n        }\n\n    @log_to_thought_trail\n    def load_workflow(self, workflow_name: str) -> Dict[str, Any]:\n        \"\"\"Load workflow definition from file.\"\"\"\n        # Ensure the workflow path is absolute by joining with the engine's workflows_dir\n        if not os.path.isabs(workflow_name):\n            # Dynamic path resolution - check multiple possible locations\n            engine_dir = os.path.dirname(os.path.abspath(__file__))\n            possible_paths = [\n                workflow_name,  # Direct path\n                os.path.join(self.workflows_dir, os.path.basename(workflow_name)),  # In workflows dir\n                os.path.join(engine_dir, \"workflows\", os.path.basename(workflow_name)), # Relative to the engine\n                os.path.join(os.path.dirname(engine_dir), os.path.basename(workflow_name)),  # Project root\n                os.path.join(os.getcwd(), os.path.basename(workflow_name))  # Current working directory\n            ]\n            \n            workflow_path = None\n            for path in possible_paths:\n                if os.path.exists(path):\n                    workflow_path = path\n                    break\n            \n            if workflow_path is None:\n                raise FileNotFoundError(f\"Workflow file not found. Searched: {possible_paths}\")\n        else:\n            workflow_path = workflow_name\n\n        try:\n            logger.info(f\"Attempting to load workflow definition from: {workflow_path}\")\n            with open(workflow_path, 'r') as f:\n                workflow = json.load(f)\n\n            # Validate workflow structure\n            if \"tasks\" not in workflow:\n                raise ValueError(\"Workflow must contain 'tasks' section\")\n            \n            # Ensure workflow has a proper name\n            if \"name\" not in workflow and \"workflow_name\" not in workflow:\n                workflow[\"name\"] = f\"Unnamed Workflow ({os.path.basename(workflow_path)})\"\n            elif \"name\" not in workflow and \"workflow_name\" in workflow:\n                workflow[\"name\"] = workflow[\"workflow_name\"]\n\n            # Validate each task\n            for task_id, task in workflow[\"tasks\"].items():\n                if \"action_type\" not in task:\n                    raise ValueError(f\"Task '{task_id}' is missing required 'action_type'\")\n                if \"description\" not in task:\n                    raise ValueError(f\"Task '{task_id}' is missing required 'description'\")\n                if \"inputs\" not in task:\n                    raise ValueError(f\"Task '{task_id}' is missing required 'inputs'\")\n\n                # Verify action is registered\n                action_type = task[\"action_type\"]\n                if action_type not in self.action_registry:\n                    raise ValueError(f\"Action type '{action_type}' is not registered in the engine\")\n\n            self.last_workflow_name = workflow.get(\"name\", \"Unnamed Workflow\")\n            return workflow\n\n        except FileNotFoundError:\n            logger.error(f\"Workflow file not found at the specified path: {workflow_path}\")\n            raise\n        except json.JSONDecodeError as e:\n            logger.error(f\"Failed to decode JSON from workflow file {workflow_path}: {e}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Unexpected error loading workflow file {workflow_path}: {str(e)}\")\n            raise\n\n    def _extract_var_path(self, template_content: str) -> str:\n        \"\"\"Extracts the core variable path from a template string, ignoring filters.\"\"\"\n        # The variable path is the part before the first pipe '|'\n        return template_content.split('|', 1)[0].strip()\n\n    def _parse_filters(self, template_content: str) -> list:\n        \"\"\"Parses filter syntax (e.g., | filter(arg1, 'arg2')) from a template string.\"\"\"\n        parts = template_content.split('|')[1:] # Get all parts after the variable path\n        filters = []\n        filter_regex = re.compile(r\"^\\s*(\\w+)\\s*(?:\\((.*?)\\))?\\s*$\") # Matches 'filter(args)' or 'filter'\n\n        for part in parts:\n            match = filter_regex.match(part)\n            if match:\n                name, args_str = match.groups()\n                args = []\n                if args_str:\n                    # This is a simplified arg parser; it splits by comma and handles basic quotes\n                    # A more robust solution might use shlex or a dedicated parsing library\n                    try:\n                        # Attempt to parse as a JSON list to handle quotes and types\n                        args = json.loads(f'[{args_str}]')\n                    except json.JSONDecodeError:\n                        # Fallback for non-standard JSON arguments\n                        args = [a.strip() for a in args_str.split(',')]\n                filters.append({\"name\": name, \"args\": args})\n        return filters\n\n    def _get_value_from_path(self, path: str, context: Dict[str, Any]) -> Any:\n        \"\"\"Gets a value from a nested dictionary using a dot-separated path.\"\"\"\n        keys = path.split('.')\n        value = context\n        for key in keys:\n            if isinstance(value, dict) and key in value:\n                value = value[key]\n            else:\n                return None # Path does not exist\n        return value\n\n    def _resolve_value(self, value: Any, context: Dict[str, Any]) -> Any:\n        \"\"\"\n        Rec...\n```\n\nEXAMPLE APPLICATION:\nValidates IAR structure compliance per crystallized artifacts specification\n\nCATEGORY: CodeKnowledge\n\nRELATIONSHIPS:\ntype: FromCodebase; source: /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/workflow_engine.py; source_type: python_class",
    "compression_ratio": 1.0,
    "symbol_count": 30573,
    "timestamp": "2025-11-18T10:59:56.794557Z"
  },
  {
    "stage_name": "Concise",
    "content": "TERM: Class: IARValidator\n\nDEFINITION:\nValidates IAR structure compliance per crystallized artifacts specification\n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/workflow_engine.py, type: python_class\n\nIMPLEMENTATION CODE (workflow_engine.py) - First 30KB:\n```python\n# ResonantiA Protocol v3.0 - workflow_engine.py\n# Orchestrates the execution of defined workflows (Process Blueprints).\n# Manages context, dependencies, conditions, action execution, and error handling.\n# Critically handles Integrated Action Reflection (IAR) results by storing\n# the complete action output dictionary (including 'reflection') in the\n# context.\n\nimport json\nimport os\nimport logging\nimport copy\nimport time\nimport re\nimport uuid\nimport tempfile\nfrom datetime import datetime\nfrom typing import Dict, Any, List, Optional, Set, Union, Tuple, Callable\nfrom jinja2 import Environment, meta, exceptions # Import Jinja2\n\n# --- Standardized Imports ---\n# This single block ensures robust relative imports within the package.\ntry:\n    from . import config\n    from .action_registry import execute_action, main_action_registry\n    from .spr_manager import SPRManager\n    from .error_handler import handle_action_error\n    from .action_context import ActionContext\n    from .workflow_recovery import WorkflowRecoveryHandler\n    from .recovery_actions import (\n        analyze_failure,\n        fix_template,\n        fix_action,\n        validate_workflow,\n        validate_action,\n        self_heal_output\n    )\n    from .system_genesis_tool import perform_system_genesis_action\n    from .qa_tools import run_code_linter, run_workflow_suite\n    from .output_handler import print_tagged_execution, print_tagged_results, display_output, display_workflow_start, display_workflow_progress, display_task_result, display_workflow_complete, display_workflow_error\n    from .custom_json import dumps\n    from .thought_trail import log_to_thought_trail\nexcept ImportError as e:\n    # This block should ideally not be hit if run as part of the package,\n    # but serves as a fallback and clear error indicator.\n    import logging\n    _import_error_msg = str(e)  # Capture error message as string\n    logging.getLogger(__name__).critical(f\"A critical relative import failed in workflow_engine: {e}\", exc_info=True)\n    # Set WorkflowOptimizer to None so code can check for it\n    WorkflowOptimizer = None\n    # Define dummy fallbacks to prevent outright crashing where possible\n    def execute_action(*args, **kwargs): return {\"error\": f\"Action Registry not available due to import error: {_import_error_msg}\"}\n    # Add other necessary fallbacks here as needed...\n    class SPRManager: pass\n    # Ensure decorator name is defined to avoid NameError in class annotations\n    def log_to_thought_trail(func): \n        return func\n\n# Optional import for WorkflowOptimizer (requires networkx)\ntry:\n    from .workflow_optimizer import WorkflowOptimizer\nexcept ImportError:\n    WorkflowOptimizer = None\n\nimport ast\nfrom datetime import datetime\n\n# ============================================================================\n# TEMPORAL CORE INTEGRATION (CANONICAL DATETIME SYSTEM)\n# ============================================================================\nfrom Three_PointO_ArchE.temporal_core import now, now_iso, ago, from_now, format_log, format_filename\n\n\n# Attempt to import numpy for numeric type checking in _compare_values,\n# optional\ntry:\n    import numpy as np\nexcept ImportError:\n    np = None  # Set to None if numpy is not available\n    logging.info(\n        \"Numpy not found, some numeric type checks in _compare_values might be limited.\")\n\nlogger = logging.getLogger(__name__)\n\n\n# === IAR COMPLIANCE ENHANCEMENT ===\n# Crystallized Artifacts Implementation - ARTIFACT 4A\n\nclass IARValidator:\n    \"\"\"Validates IAR structure compliance per crystallized artifacts specification\"\"\"\n    \n    def __init__(self):\n        self.required_fields = [\n            'status', 'summary', 'confidence', \n            'alignment_check', 'potential_issues',\n            'raw_output_preview'\n        ]\n        self.enhanced_fields = [\n            'tactical_resonance', 'crystallization_potential'\n        ]\n    \n    def validate_structure(self, iar_data):\n        \"\"\"Validate IAR structure meets all requirements\"\"\"\n        if not isinstance(iar_data, dict):\n            return False, [\"IAR must be a dictionary\"]\n        \n        missing_fields = []\n        for field in self.required_fields:\n            if field not in iar_data:\n                missing_fields.append(field)\n        \n        issues = []\n        if missing_fields:\n            issues.extend(\n                [f\"Missing required field: {field}\" for field in missing_fields])\n        \n        # Validate confidence is float between 0-1\n        confidence = iar_data.get('confidence')\n        if confidence is not None:\n            if not isinstance(\n    confidence, (int, float)) or not (\n        0.0 <= confidence <= 1.0):\n                issues.append(\"Confidence must be float between 0.0 and 1.0\")\n        \n        # Validate status is valid\n        status = iar_data.get('status')\n        if status not in ['Success', 'Partial', 'Failed']:\n            issues.append(\"Status must be 'Success', 'Partial', or 'Failed'\")\n        \n        return len(issues) == 0, issues\n    \n    def validate_enhanced_fields(self, iar_data):\n        \"\"\"Validate enhanced IAR fields for tactical resonance\"\"\"\n        enhanced_issues = []\n        \n        for field in self.enhanced_fields:\n            if field in iar_data:\n                value = iar_data[field]\n                if not isinstance(\n    value, (int, float)) or not (\n        0.0 <= value <= 1.0):\n                    enhanced_issues.append(\n    f\"{field} must be float between 0.0 and 1.0\")\n        \n        return len(enhanced_issues) == 0, enhanced_issues\n\n\nclass ResonanceTracker:\n    \"\"\"Tracks tactical resonance and crystallization metrics\"\"\"\n    \n    def __init__(self):\n        self.execution_history = []\n        self.resonance_metrics = {\n            'avg_tactical_resonance': 0.0,\n            'avg_crystallization_potential': 0.0,\n            'total_executions': 0\n        }\n    \n    def record_execution(self, task_id, iar_data, context):\n        \"\"\"Record task execution for resonance tracking\"\"\"\n        execution_record = {\n            'timestamp': now_iso(),\n            'task_id': task_id,\n            'status': iar_data.get('status'),\n            'confidence': iar_data.get('confidence', 0.0),\n            'tactical_resonance': iar_data.get('tactical_resonance', 0.0),\n            'crystallization_potential': iar_data.get('crystallization_potential', 0.0)\n        }\n        \n        self.execution_history.append(execution_record)\n        self._update_metrics()\n        \n        return execution_record\n    \n    def _update_metrics(self):\n        \"\"\"Update aggregate resonance metrics\"\"\"\n        if not self.execution_history:\n            return\n        \n        # Last 100 executions\n        recent_executions = self.execution_history[-100:]\n\n        tactical_scores = [ex.get('tactical_resonance', 0.0)\n                                  for ex in recent_executions]\n        crystallization_scores = [\n    ex.get(\n        'crystallization_potential',\n         0.0) for ex in recent_executions]\n        \n        self.resonance_metrics = {\n            'avg_tactical_resonance': sum(tactical_scores) / len(tactical_scores),\n            'avg_crystallization_potential': sum(crystallization_scores) / len(crystallization_scores),\n            'total_executions': len(self.execution_history)\n        }\n    \n    def get_resonance_report(self):\n        \"\"\"Get current resonance metrics report\"\"\"\n        return {\n            'current_metrics': self.resonance_metrics,\n            'recent_trend': self._calculate_trend(),\n            'compliance_score': self._calculate_compliance_score()\n        }\n    \n    def _calculate_trend(self):\n        \"\"\"Calculate resonance trend over recent executions\"\"\"\n        if len(self.execution_history) < 10:\n            return \"insufficient_data\"\n        \n        recent_10 = self.execution_history[-10:]\n        older_10 = self.execution_history[-20:-10]\n        \n        recent_avg = sum(ex.get('tactical_resonance', 0.0)\n                         for ex in recent_10) / 10\n        older_avg = sum(ex.get('tactical_resonance', 0.0)\n                        for ex in older_10) / 10\n        \n        if recent_avg > older_avg + 0.05:\n            return \"improving\"\n        elif recent_avg < older_avg - 0.05:\n            return \"declining\"\n        else:\n            return \"stable\"\n    \n    def _calculate_compliance_score(self):\n        \"\"\"Calculate overall IAR compliance score\"\"\"\n        if not self.execution_history:\n            return 0.0\n        \n        recent_executions = self.execution_history[-50:]\n        successful_executions = [\n    ex for ex in recent_executions if ex.get('status') == 'Success']\n        \n        success_rate = len(successful_executions) / len(recent_executions)\n        avg_confidence = sum(ex.get('confidence', 0.0)\n                             for ex in successful_executions) / max(len(successful_executions), 1)\n        avg_resonance = self.resonance_metrics['avg_tactical_resonance']\n        \n        # Weighted compliance score\n        compliance_score = (success_rate * 0.4) + \\\n                            (avg_confidence * 0.3) + (avg_resonance * 0.3)\n        return min(compliance_score, 1.0)\n\n# === Session State Manager ===\ntry:\n    from .session_state_manager import load_session_state, save_session_state, append_fact\n    from .context_superposition import create_context_bundle, merge_bundles\n    from .prefetch_manager import trigger_predictive_prefetch\n    from .sirc_autonomy import maybe_autorun_sirc\n    from .causal_digest import build_flux_annotated_digest\nexcept Exception:\n    # Fallback no-ops if module not available\n    def load_session_state():\n        return {\"facts_ledger\": [], \"updated_at\": now_iso()}\n    def save_session_state(state):\n        return None\n    def append_fact(state, fact):\n        state.setdefault(\"facts_ledger\", []).append({**fact, \"ts\": now_iso()})\n    def create_context_bundle(spr_def, runtime_context, initial_context):\n        return {\"spr_id\": spr_def.get(\"spr_id\"), \"created_at\": now_iso()}\n    def merge_bundles(bundles):\n        return {\"spr_index\": [b.get(\"spr_id\") for b in bundles]}\n\n\ndef _execute_standalone_workflow(workflow_definition: Dict[str, Any], initial_context: Dict[str, Any], parent_run_id: str, action_registry: Dict[str, Callable]) -> Dict[str, Any]:\n    \"\"\"\n    Executes a workflow definition in-memory, for use by meta-actions like for_each.\n    This is a simplified version of the main run_workflow loop, moved outside the class\n    to prevent circular dependencies.\n    \"\"\"\n    run_id = f\"{parent_run_id}_sub_{uuid.uuid4().hex[:8]}\"\n    initial_context[\"workflow_run_id\"] = run_id\n    \n    runtime_context = {\n        \"initial_context\": initial_context,\n        \"workflow_run_id\": run_id,\n        \"workflow_definition\": workflow_definition,\n    }\n    \n    tasks = workflow_definition.get('tasks', {})\n    task_statuses = {key: \"pending\" for key in tasks}\n    \n    sorted_tasks = list(tasks.keys()) \n\n    logger.info(f\"Starting standalone sub-workflow (Run ID: {run_id}).\")\n    start_time = time.time()\n\n    for task_key in sorted_tasks:\n        task_info = tasks[task_key]\n        action_type = task_info.get(\"action_type\")\n        \n        # This is a critical simplification: we can't use the full resolver here\n        # without re-introducing the dependency. We'll manually resolve for now.\n        resolved_inputs = {}\n        for k, v in task_info.get('inputs', {}).items():\n            if isinstance(v, str) and v == \"{{ item }}\":\n                resolved_inputs[k] = initial_context.get('item')\n            else:\n                resolved_inputs[k] = v\n\n        action_func = action_registry.get(action_type)\n        if not action_func:\n            error_result = {\"error\": f\"Sub-workflow failed: Action type '{action_type}' not found.\"}\n            runtime_context[task_key] = self._ensure_iar_compliance(error_result, action_context_obj)\n            task_statuses[task_key] = \"failed\"\n            break\n\n        try:\n            # The arguments must be passed as keyword arguments\n            # to align with the action wrappers in action_registry.py\n            result = action_func(**resolved_inputs)\n            if not isinstance(result, dict):\n                result = {\"output\": result}\n            runtime_context[task_key] = self._ensure_iar_compliance(result, action_context_obj)\n            task_statuses[task_key] = \"completed\"\n        except Exception as e:\n            error_msg = f\"Sub-workflow task '{task_key}' failed: {e}\"\n            logger.error(error_msg, exc_info=True)\n            error_result = {\"error\": error_msg}\n            runtime_context[task_key] = self._ensure_iar_compliance(error_result, action_context_obj)\n            task_statuses[task_key] = \"failed\"\n            break\n\n    end_time = time.time()\n    run_duration = round(end_time - start_time, 2)\n    \n    final_status = \"Completed\" if all(s == \"completed\" for s in task_statuses.values()) else \"Failed\"\n    \n    # Can't call _summarize_run directly, so construct a simplified summary\n    return {\n        \"workflow_name\": workflow_definition.get(\"name\", \"sub-workflow\"),\n        \"run_id\": run_id,\n        \"status\": final_status,\n        \"duration\": run_duration,\n        \"task_statuses\": task_statuses,\n        \"results\": runtime_context\n    }\n\n\nclass IARCompliantWorkflowEngine:\n    \"\"\"Enhanced workflow engine with IAR compliance and recovery support.\"\"\"\n\n    def __init__(self, workflows_dir: str = \"workflows\", spr_manager=None, event_callback: Optional[Callable] = None):\n        self.workflows_dir = workflows_dir\n        self.spr_manager = spr_manager\n        self.event_callback = event_callback  # <-- ADDED\n        self.last_workflow_name = None\n        self.action_registry = main_action_registry.actions.copy()  # Use centralized registry\n        self.recovery_handler = None\n        self.current_run_id = None\n        self.current_workflow = None\n        self.iar_validator = IARValidator()\n        self.resonance_tracker = ResonanceTracker()\n        self.jinja_env = Environment() # Initialize Jinja2 environment\n\n        # Register standard actions\n        self.register_action(\"display_output\", display_output)\n        self.register_action(\"perform_system_genesis_action\",\n     perform_system_genesis_action)\n        self.register_action(\"run_code_linter\", run_code_linter)\n        self.register_action(\"run_workflow_suite\", run_workflow_suite)\n\n        # Register recovery actions\n        self.register_recovery_actions()\n        logger.info(\n            \"IARCompliantWorkflowEngine initialized with full vetting capabilities\")\n\n    @log_to_thought_trail\n    def register_action(self, action_type: str, action_func: Callable) -> None:\n        \"\"\"Register an action function with the engine.\"\"\"\n        main_action_registry.register_action(\n            action_type,\n            action_func,\n        )\n        self.action_registry = main_action_registry.actions.copy()  # Update local copy\n        logger.de",
    "compression_ratio": 2.000065419337956,
    "symbol_count": 15286,
    "timestamp": "2025-11-18T10:59:56.794583Z"
  },
  {
    "stage_name": "Nano",
    "content": "TERM: Class: ΦValidator D: Validates Φ structure compliance per crystallized artifacts specification BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Æ/workflow_engine.py, type: python_class I CODE (workflow_engine.py) - First 30KB: ```python # ResonantiA P v3.0 - workflow_engine.py # Orchestrates execution of defined workflows (P Blueprints). # Manages context, dependencies, conditions, action execution, error handling. # Critically handles Φ (Φ) results by storing # complete action output dictionary (including 'reflection') in # context. import json import os import logging import copy import time import re import uuid import tempfile datetime import datetime typing import Dict, Any, List, Optional, Set, Union, Tuple, Callable jinja2 import Environment, meta, exceptions # Import Jinja2 # --- Standardized Imports --- # single block ensures robust relative imports within package. try: . import config .action_registry import execute_action, main_action_registry .Θ_manager import ΘManager .error_handler import handle_action_error .action_context import ActionContext .workflow_recovery import WorkflowRecoveryHandler .recovery_actions import ( analyze_failure, fix_template, fix_action, validate_workflow, validate_action, self_heal_output ) .S_genesis_tool import perform_S_genesis_action .qa_tools import run_code_linter, run_workflow_suite .output_handler import print_tagged_execution, print_tagged_results, display_output, display_workflow_start, display_workflow_progress, display_task_result, display_workflow_complete, display_workflow_error .custom_json import dumps .thought_trail import log_to_thought_trail except ImportError as e: # block should ideally be hit if run as part of package, # serves as a fallback clear error indicator. import logging _import_error_msg = str(e) # Capture error message as string logging.getLogger(__name__).critical(f\"A critical relative import failed in workflow_engine: {e}\", exc_info=True) # Set WorkflowOptimizer to None so code check it WorkflowOptimizer = None # Define dummy fallbacks to prevent outright crashing possible def execute_action(*args, **kwargs): return {\"error\": f\"Action Registry available due to import error: {_import_error_msg}\"} # Add other necessary fallbacks here as needed... class ΘManager: pass # Ensure decorator name is defined to avoid NameError in class annotations def log_to_thought_trail(func): return func # Optional import WorkflowOptimizer (requires networkx) try: .workflow_optimizer import WorkflowOptimizer except ImportError: WorkflowOptimizer = None import ast datetime import datetime # ============================================================================ # TEMPORAL CORE INTEGRATION (CANONICAL DATETIME S) # ============================================================================ Three_PointO_Æ.temporal_core import now, now_iso, ago, from_now, F_log, F_filename # Attempt to import numpy numeric type checking in _compare_values, # optional try: import numpy as np except ImportError: np = None # Set to None if numpy is available logging.info( \"Numpy found, some numeric type checks in _compare_values might be limited.\") logger = logging.getLogger(__name__) # === Φ COMPLIANCE ENHANCEMENT === # Crystallized Artifacts I - ARTIFACT 4A class ΦValidator: \"\"\"Validates Φ structure compliance per crystallized artifacts specification\"\"\" def __init__(self): self.required_fields = [ 'status', 'summary', 'confidence', 'alignment_check', 'potential_issues', 'raw_output_preview' ] self.enhanced_fields = [ 'tactical_resonance', 'crystallization_potential' ] def validate_structure(self, Φ_data): \"\"\"Validate Φ structure meets requirements\"\"\" if isinstance(Φ_data, dict): return False, [\"Φ must be a dictionary\"] missing_fields = [] field in self.required_fields: if field in Φ_data: missing_fields.append(field) issues = [] if missing_fields: issues.extend( [f\"Missing required field: {field}\" field in missing_fields]) # Validate confidence is float between 0-1 confidence = Φ_data.get('confidence') if confidence is None: if isinstance( confidence, (int, float)) or ( 0.0 <= confidence <= 1.0): issues.append(\"Confidence must be float between 0.0 1.0\") # Validate status is valid status = Φ_data.get('status') if status in ['Success', 'Partial', 'Failed']: issues.append(\"Status must be 'Success', 'Partial', or 'Failed'\") return len(issues) == 0, issues def validate_enhanced_fields(self, Φ_data): \"\"\"Validate enhanced Φ fields tactical resonance\"\"\" enhanced_issues = [] field in self.enhanced_fields: if field in Φ_data: value = Φ_data[field] if isinstance( value, (int, float)) or ( 0.0 <= value <= 1.0): enhanced_issues.append( f\"{field} must be float between 0.0 1.0\") return len(enhanced_issues) == 0, enhanced_issues class ResonanceTracker: \"\"\"Tracks tactical resonance crystallization metrics\"\"\" def __init__(self): self.execution_history = [] self.resonance_metrics = { 'avg_tactical_resonance': 0.0, 'avg_crystallization_potential': 0.0, 'total_executions': 0 } def record_execution(self, task_id, Φ_data, context): \"\"\"Record task execution resonance tracking\"\"\" execution_record = { 'timestamp': now_iso(), 'task_id': task_id, 'status': Φ_data.get('status'), 'confidence': Φ_data.get('confidence', 0.0), 'tactical_resonance': Φ_data.get('tactical_resonance', 0.0), 'crystallization_potential': Φ_data.get('crystallization_potential', 0.0) } self.execution_history.append(execution_record) self._update_metrics() return execution_record def _update_metrics(self): \"\"\"Update aggregate resonance metrics\"\"\" if self.execution_history: return # Last 100 executions recent_executions = self.execution_history[-100:] tactical_scores = [ex.get('tactical_resonance', 0.0) ex in recent_executions] crystallization_scores = [ ex.get( 'crystallization_potential', 0.0) ex in recent_executions] self.resonance_metrics = { 'avg_tactical_resonance': sum(tactical_scores) / len(tactical_scores), 'avg_crystallization_potential': sum(crystallization_scores) / len(crystallization_scores), 'total_executions': len(self.execution_history) } def get_resonance_report(self): \"\"\"Get current resonance metrics report\"\"\" return { 'current_metrics': self.resonance_metrics, 'recent_trend': self._calculate_trend(), 'compliance_score': self._calculate_compliance_score() } def _calculate_trend(self): \"\"\"Calculate resonance trend over recent executions\"\"\" if len(self.execution_history) < 10: return \"insufficient_data\" recent_10 = self.execution_history[-10:] older_10 = self.execution_history[-20:-10] recent_avg = sum(ex.get('tactical_resonance', 0.0) ex in recent_10) / 10 older_avg = sum(ex.get('tactical_resonance', 0.0) ex in older_10) / 10 if recent_avg > older_avg + 0.05: return \"improving\" elif recent_avg < older_avg - 0.05: return \"declining\" else: return \"stable\" def _calculate_compliance_score(self): \"\"\"Calculate overall Φ compliance score\"\"\" if self.execution_history: return 0.0 recent_executions = self.execution_history[-50:] successful_executions = [ ex ex in recent_executions if ex.get('status') == 'Success'] success_rate = len(successful_executions) / len(recent_executions) avg_confidence = sum(ex.get('confidence', 0.0) ex in successful_executions) / max(len(successful_executions), 1) avg_resonance = self.resonance_metrics['avg_tactical_resonance'] # Weighted compliance score compliance_score = (success_rate * 0.4) + \\ (avg_confidence * 0.3) + (avg_resonance * 0.3) return min(compliance_score, 1.0) # === Session State Manager === try: .session_state_manager import load_session_state, save_session_state, append_fact .context_superposition import create_context_bundle, merge_bundles .prefetch_manager import trigger_predictive_prefetch .SIRC_autonomy import maybe_autorun_SIRC .causal_digest import build_flux_annotated_digest except Exception: # Fallback no-ops if module available def load_session_state(): return {\"facts_ledger\": [], \"updated_at\": now_iso()} def save_session_state(state): return None def append_fact(state, fact): state.setdefault(\"facts_ledger\", []).append({**fact, \"ts\": now_iso()}) def create_context_bundle(Θ_def, runtime_context, initial_context): return {\"Θ_id\": Θ_def.get(\"Θ_id\"), \"created_at\": now_iso()} def merge_bundles(bundles): return {\"Θ_index\": [b.get(\"Θ_id\") b in bundles]} def _execute_standalone_workflow(workflow_D: Dict[str, Any], initial_context: Dict[str, Any], parent_run_id: str, action_registry: Dict[str, Callable]) -> Dict[str, Any]: \"\"\" Executes a workflow D in-memory, use by meta-actions like for_each. is a simplified version of main run_workflow loop, moved outside class to prevent circular dependencies. \"\"\" run_id = f\"{parent_run_id}_sub_{uuid.uuid4().hex[:8]}\" initial_context[\"workflow_run_id\"] = run_id runtime_context = { \"initial_context\": initial_context, \"workflow_run_id\": run_id, \"workflow_D\": workflow_D, } tasks = workflow_D.get('tasks', {}) task_statuses = {key: \"pending\" key in tasks} sorted_tasks = list(tasks.keys()) logger.info(f\"Starting standalone sub-workflow (Run ID: {run_id}).\") start_time = time.time() task_key in sorted_tasks: task_info = tasks[task_key] action_type = task_info.get(\"action_type\") # is a critical simplification: we 't use full resolver here # without re-introducing dependency. We'll manually resolve now. resolved_inputs = {} k, v in task_info.get('inputs', {}).items(): if isinstance(v, str) v == \"{{ item }}\": resolved_inputs[k] = initial_context.get('item') else: resolved_inputs[k] = v action_func = action_registry.get(action_type) if action_func: error_result = {\"error\": f\"Sub-workflow failed: Action type '{action_type}' found.\"} runtime_context[task_key] = self._ensure_Φ_compliance(error_result, action_context_obj) task_statuses[task_key] = \"failed\" break try: # arguments must be passed as keyword arguments # to align action wrappers in action_registry.py result = action_func(**resolved_inputs) if isinstance(result, dict): result = {\"output\": result} runtime_context[task_key] = self._ensure_Φ_compliance(result, action_context_obj) task_statuses[task_key] = \"completed\" except Exception as e: error_msg = f\"Sub-workflow task '{task_key}' failed: {e}\" logger.error(error_msg, exc_info=True) error_result = {\"error\": error_msg} runtime_context[task_key] = self._ensure_Φ_compliance(error_result, action_context_obj) task_statuses[task_key] = \"failed\" break end_time = time.time() run_duration = round(end_time - start_time, 2) final_status = \"Completed\" if (s == \"completed\" s in task_statuses.values()) else \"Failed\" # 't call _summarize_run directly, so construct a simplified summary return { \"workflow_name\": workflow_D.get(\"name\", \"sub-workflow\"), \"run_id\": run_id, \"status\": final_status, \"duration\": run_duration, \"task_statuses\": task_statuses, \"results\": runtime_context } class ΦCompliantWorkflowEngine: \"\"\"Enhanced W Φ compliance recovery support.\"\"\" def __init__(self, workflows_dir: str = \"workflows\", Θ_manager=None, event_callback: Optional[Callable] = None): self.workflows_dir = workflows_dir self.Θ_manager = Θ_manager self.event_callback = event_callback # <-- ADDED self.last_workflow_name = None self.action_registry = main_action_registry.actions.copy() # Use centralized registry self.recovery_handler = None self.current_run_id = None self.current_workflow = None self.Φ_validator = ΦValidator() self.resonance_tracker = ResonanceTracker() self.jinja_env = Environment() # Initialize Jinja2 environment # Register standard actions self.register_action(\"display_output\", display_output) self.register_action(\"perform_S_genesis_action\", perform_S_genesis_action) self.register_action(\"run_code_linter\", run_code_linter) self.register_action(\"run_workflow_suite\", run_workflow_suite) # Register recovery actions self.register_recovery_actions() logger.info( \"ΦCompliantWorkflowEngine initialized full vetting capabilities\") @log_to_thought_trail def register_action(self, action_type: str, action_func: Callable) -> None: \"\"\"Register an action function engine.\"\"\" main_action_registry.register_action( action_type, action_func, ) self.action_registry = main_action_registry.actions.copy() # Update local copy logger.de",
    "compression_ratio": 2.523774145616642,
    "symbol_count": 12114,
    "timestamp": "2025-11-18T10:59:56.877270Z"
  },
  {
    "stage_name": "Micro",
    "content": "TERM: Class: ΦValidator D: Validates Φ structure compliance crystallized artifacts specification BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Æ/workflow_engine.py, type: python_class I CODE (workflow_engine.py) First 30KB: ```python ResonantiA P workflow_engine.py Orchestrates execution defined workflows Blueprints). Manages context, dependencies, conditions, action execution, error handling. Critically handles Φ (Φ) results storing complete action output dictionary (including 'CRC') context. import import import logging import import import import import tempfile datetime import datetime typing import Dict, Any, List, Optional, Set, Union, Tuple, Callable jinja2 import Environment, meta, exceptions Import Jinja2 Standardized Imports single block ensures robust relative imports within package. import config .action_registry import execute_action, main_action_registry .Θ_manager import ΘManager .error_handler import handle_action_error .action_context import ActionContext .workflow_recovery import WorkflowRecoveryHandler .recovery_actions import analyze_failure, fix_template, fix_action, validate_workflow, validate_action, self_heal_output .S_genesis_tool import perform_S_genesis_action .qa_tools import run_code_linter, run_workflow_suite .output_handler import print_tagged_execution, print_tagged_results, display_output, display_workflow_start, display_workflow_progress, display_task_result, display_workflow_complete, display_workflow_error .custom_json import dumps .thought_trail import log_to_thought_trail except ImportError block ideally package, serves fallback clear error indicator. import logging _import_error_msg str(e) Capture error message string logging.getLogger(__name__).critical(f\"A critical relative import failed workflow_engine: {e}\", exc_info=True) Set WorkflowOptimizer None check WorkflowOptimizer None Define dummy fallbacks prevent outright crashing possible execute_action(*args, **kwargs): return {\"error\": f\"Action Registry available import error: {_import_error_msg}\"} Add other necessary fallbacks needed... class ΘManager: Ensure decorator defined avoid NameError class annotations log_to_thought_trail(func): return Optional import WorkflowOptimizer (requires networkx) .workflow_optimizer import WorkflowOptimizer except ImportError: WorkflowOptimizer None import datetime import datetime ============================================================================ Δ CORE INTEGRATION (CANONICAL DATETIME S) ============================================================================ Three_PointO_Æ.temporal_core import now_iso, from_now, F_log, F_filename Attempt import numpy numeric checking _compare_values, optional import numpy except ImportError: None Set None numpy available logging.info( \"Numpy found, numeric checks _compare_values limited.\") logger logging.getLogger(__name__) Φ COMPLIANCE ENHANCEMENT Crystallized Artifacts I ARTIFACT class ΦValidator: \"\"\"Validates Φ structure compliance crystallized artifacts specification\"\"\" __init__(self): self.required_fields 'status', 'summary', 'confidence', 'alignment_check', 'potential_issues', 'raw_output_preview' self.enhanced_fields 'tactical_resonance', 'crystallization_potential' validate_structure(self, Φ_data): \"\"\"Validate Φ structure meets requirements\"\"\" isinstance(Φ_data, dict): return False, [\"Φ dictionary\"] missing_fields field self.required_fields: field Φ_data: missing_fields.append(field) issues missing_fields: issues.extend( [f\"Missing required field: {field}\" field missing_fields]) Validate confidence float between confidence Φ_data.get('confidence') confidence None: isinstance( confidence, (int, float)) confidence 1.0): issues.append(\"Confidence float between 1.0\") Validate status valid status Φ_data.get('status') status ['Success', 'Partial', 'Failed']: issues.append(\"Status 'Success', 'Partial', 'Failed'\") return len(issues) issues validate_enhanced_fields(self, Φ_data): \"\"\"Validate enhanced Φ fields tactical Ω\"\"\" enhanced_issues field self.enhanced_fields: field Φ_data: value Φ_data[field] isinstance( value, (int, float)) value 1.0): enhanced_issues.append( f\"{field} float between 1.0\") return len(enhanced_issues) enhanced_issues class ResonanceTracker: \"\"\"Tracks tactical Ω Π metrics\"\"\" __init__(self): self.execution_history self.resonance_metrics 'avg_tactical_resonance': 'avg_crystallization_potential': 'total_executions': record_execution(self, task_id, Φ_data, context): \"\"\"Record execution Ω tracking\"\"\" execution_record 'timestamp': now_iso(), 'task_id': task_id, 'status': Φ_data.get('status'), 'confidence': Φ_data.get('confidence', 0.0), 'tactical_resonance': Φ_data.get('tactical_resonance', 0.0), 'crystallization_potential': Φ_data.get('crystallization_potential', self.execution_history.append(execution_record) self._update_metrics() return execution_record _update_metrics(self): \"\"\"Update aggregate Ω metrics\"\"\" self.execution_history: return Last executions recent_executions self.execution_history[-100:] tactical_scores [ex.get('tactical_resonance', recent_executions] crystallization_scores ex.get( 'crystallization_potential', recent_executions] self.resonance_metrics 'avg_tactical_resonance': sum(tactical_scores) len(tactical_scores), 'avg_crystallization_potential': sum(crystallization_scores) len(crystallization_scores), 'total_executions': len(self.execution_history) get_resonance_report(self): \"\"\"Get current Ω metrics report\"\"\" return 'current_metrics': self.resonance_metrics, 'recent_trend': self._calculate_trend(), 'compliance_score': self._calculate_compliance_score() _calculate_trend(self): \"\"\"Calculate Ω trend recent executions\"\"\" len(self.execution_history) return \"insufficient_data\" recent_10 self.execution_history[-10:] older_10 self.execution_history[-20:-10] recent_avg sum(ex.get('tactical_resonance', recent_10) older_avg sum(ex.get('tactical_resonance', older_10) recent_avg older_avg 0.05: return \"improving\" recent_avg older_avg 0.05: return \"declining\" else: return \"stable\" _calculate_compliance_score(self): \"\"\"Calculate overall Φ compliance score\"\"\" self.execution_history: return recent_executions self.execution_history[-50:] successful_executions recent_executions ex.get('status') 'Success'] success_rate len(successful_executions) len(recent_executions) avg_confidence sum(ex.get('confidence', successful_executions) max(len(successful_executions), avg_resonance self.resonance_metrics['avg_tactical_resonance'] Weighted compliance score compliance_score (success_rate (avg_confidence (avg_resonance return min(compliance_score, Session State Manager .session_state_manager import load_session_state, save_session_state, append_fact .context_superposition import create_context_bundle, merge_bundles .prefetch_manager import trigger_predictive_prefetch .SIRC_autonomy import maybe_autorun_SIRC .causal_digest import build_flux_annotated_digest except Exception: Fallback no-ops module available load_session_state(): return {\"facts_ledger\": \"updated_at\": now_iso()} save_session_state(state): return None append_fact(state, fact): state.setdefault(\"facts_ledger\", []).append({**fact, \"ts\": now_iso()}) create_context_bundle(Θ_def, runtime_context, initial_context): return {\"Θ_id\": Θ_def.get(\"Θ_id\"), \"created_at\": now_iso()} merge_bundles(bundles): return {\"Θ_index\": [b.get(\"Θ_id\") bundles]} _execute_standalone_workflow(workflow_D: Dict[str, Any], initial_context: Dict[str, Any], parent_run_id: action_registry: Dict[str, Callable]) Dict[str, Any]: Executes workflow D in-memory, meta-actions for_each. simplified version run_workflow loop, moved outside class prevent circular dependencies. run_id f\"{parent_run_id}_sub_{uuid.uuid4().hex[:8]}\" initial_context[\"workflow_run_id\"] run_id runtime_context \"initial_context\": initial_context, \"workflow_run_id\": run_id, \"workflow_D\": workflow_D, tasks workflow_D.get('tasks', task_statuses {key: \"pending\" tasks} sorted_tasks list(tasks.keys()) logger.info(f\"Starting standalone sub-workflow ID: {run_id}).\") start_time time.time() task_key sorted_tasks: task_info tasks[task_key] action_type task_info.get(\"action_type\") critical simplification: resolver without re-introducing dependency. We'll manually resolve resolved_inputs task_info.get('inputs', {}).items(): isinstance(v, resolved_inputs[k] initial_context.get('item') else: resolved_inputs[k] action_func action_registry.get(action_type) action_func: error_result {\"error\": f\"Sub-workflow failed: Action '{action_type}' found.\"} runtime_context[task_key] self._ensure_Φ_compliance(error_result, action_context_obj) task_statuses[task_key] \"failed\" break arguments passed keyword arguments align action wrappers action_registry.py result action_func(**resolved_inputs) isinstance(result, dict): result {\"output\": result} runtime_context[task_key] self._ensure_Φ_compliance(result, action_context_obj) task_statuses[task_key] \"completed\" except Exception error_msg f\"Sub-workflow '{task_key}' failed: logger.error(error_msg, exc_info=True) error_result {\"error\": error_msg} runtime_context[task_key] self._ensure_Φ_compliance(error_result, action_context_obj) task_statuses[task_key] \"failed\" break end_time time.time() run_duration round(end_time start_time, final_status \"Completed\" \"completed\" task_statuses.values()) \"Failed\" _summarize_run directly, construct simplified summary return \"workflow_name\": workflow_D.get(\"name\", \"sub-workflow\"), \"run_id\": run_id, \"status\": final_status, \"duration\": run_duration, \"task_statuses\": task_statuses, \"results\": runtime_context class ΦCompliantWorkflowEngine: \"\"\"Enhanced W Φ compliance recovery support.\"\"\" __init__(self, workflows_dir: \"workflows\", Θ_manager=None, event_callback: Optional[Callable] None): self.workflows_dir workflows_dir self.Θ_manager Θ_manager self.event_callback event_callback ADDED self.last_workflow_name None self.action_registry main_action_registry.actions.copy() Use centralized registry self.recovery_handler None self.current_run_id None self.current_workflow None self.Φ_validator ΦValidator() self.resonance_tracker ResonanceTracker() self.jinja_env Environment() Initialize Jinja2 environment Register standard actions self.register_action(\"display_output\", display_output) self.register_action(\"perform_S_genesis_action\", perform_S_genesis_action) self.register_action(\"run_code_linter\", run_code_linter) self.register_action(\"run_workflow_suite\", run_workflow_suite) Register recovery actions self.register_recovery_actions() logger.info( \"ΦCompliantWorkflowEngine initialized vetting capabilities\") @log_to_thought_trail register_action(self, action_type: action_func: Callable) None: \"\"\"Register action function engine.\"\"\" main_action_registry.register_action( action_type, action_func, self.action_registry main_action_registry.actions.copy() Update local logger.de",
    "compression_ratio": 2.8274299454360494,
    "symbol_count": 10813,
    "timestamp": "2025-11-18T10:59:57.118847Z"
  },
  {
    "stage_name": "Pico",
    "content": "TERM: Class: ΦValidator D: Validates Φ structure compliance crystallized artifacts specification BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Æ/workflow_engine.py, type: python_class I CODE (workflow_engine.py) First 30KB: ```python ResonantiA P workflow_engine.py Orchestrates execution defined workflows Blueprints). Manages context, dependencies, conditions, action execution, error handling. Critically handles Φ (Φ) results storing complete action output dictionary (including 'CRC') context. import import import logging import import import import import tempfile datetime import datetime typing import Dict, Any, List, Optional, Set, Union, Tuple, Callable jinja2 import Environment, meta, exceptions Import Jinja2 Standardized Imports single block ensures robust relative imports within package. import config .action_registry import execute_action, main_action_registry .Θ_manager import ΘManager .error_handler import handle_action_error .action_context import ActionContext .workflow_recovery import WorkflowRecoveryHandler .recovery_actions import analyze_failure, fix_template, fix_action, validate_workflow, validate_action, self_heal_output .S_genesis_tool import perform_S_genesis_action .qa_tools import run_code_linter, run_workflow_suite .output_handler import print_tagged_execution, print_tagged_results, display_output, display_workflow_start, display_workflow_progress, display_task_result, display_workflow_complete, display_workflow_error .custom_json import dumps .thought_trail import log_to_thought_trail except ImportError block ideally package, serves fallback clear error indicator. import logging _import_error_msg str(e) Capture error message string logging.getLogger(__name__).critical(f\"A critical relative import failed workflow_engine: {e}\", exc_info=True) Set WorkflowOptimizer None check WorkflowOptimizer None Define dummy fallbacks prevent outright crashing possible execute_action(*args, **kwargs): return {\"error\": f\"Action Registry available import error: {_import_error_msg}\"} Add other necessary fallbacks needed... class ΘManager: Ensure decorator defined avoid NameError class annotations log_to_thought_trail(func): return Optional import WorkflowOptimizer (requires networkx) .workflow_optimizer import WorkflowOptimizer except ImportError: WorkflowOptimizer None import datetime import datetime ============================================================================ Δ CORE INTEGRATION (CANONICAL DATETIME S) ============================================================================ Three_PointO_Æ.temporal_core import now_iso, from_now, F_log, F_filename Attempt import numpy numeric checking _compare_values, optional import numpy except ImportError: None Set None numpy available logging.info( \"Numpy found, numeric checks _compare_values limited.\") logger logging.getLogger(__name__) Φ COMPLIANCE ENHANCEMENT Crystallized Artifacts I ARTIFACT class ΦValidator: \"\"\"Validates Φ structure compliance crystallized artifacts specification\"\"\" __init__(self): self.required_fields 'status', 'summary', 'confidence', 'alignment_check', 'potential_issues', 'raw_output_preview' self.enhanced_fields 'tactical_resonance', 'crystallization_potential' validate_structure(self, Φ_data): \"\"\"Validate Φ structure meets requirements\"\"\" isinstance(Φ_data, dict): return False, [\"Φ dictionary\"] missing_fields field self.required_fields: field Φ_data: missing_fields.append(field) issues missing_fields: issues.extend( [f\"Missing required field: {field}\" field missing_fields]) Validate confidence float between confidence Φ_data.get('confidence') confidence None: isinstance( confidence, (int, float)) confidence 1.0): issues.append(\"Confidence float between 1.0\") Validate status valid status Φ_data.get('status') status ['Success', 'Partial', 'Failed']: issues.append(\"Status 'Success', 'Partial', 'Failed'\") return len(issues) issues validate_enhanced_fields(self, Φ_data): \"\"\"Validate enhanced Φ fields tactical Ω\"\"\" enhanced_issues field self.enhanced_fields: field Φ_data: value Φ_data[field] isinstance( value, (int, float)) value 1.0): enhanced_issues.append( f\"{field} float between 1.0\") return len(enhanced_issues) enhanced_issues class ResonanceTracker: \"\"\"Tracks tactical Ω Π metrics\"\"\" __init__(self): self.execution_history self.resonance_metrics 'avg_tactical_resonance': 'avg_crystallization_potential': 'total_executions': record_execution(self, task_id, Φ_data, context): \"\"\"Record execution Ω tracking\"\"\" execution_record 'timestamp': now_iso(), 'task_id': task_id, 'status': Φ_data.get('status'), 'confidence': Φ_data.get('confidence', 0.0), 'tactical_resonance': Φ_data.get('tactical_resonance', 0.0), 'crystallization_potential': Φ_data.get('crystallization_potential', self.execution_history.append(execution_record) self._update_metrics() return execution_record _update_metrics(self): \"\"\"Update aggregate Ω metrics\"\"\" self.execution_history: return Last executions recent_executions self.execution_history[-100:] tactical_scores [ex.get('tactical_resonance', recent_executions] crystallization_scores ex.get( 'crystallization_potential', recent_executions] self.resonance_metrics 'avg_tactical_resonance': sum(tactical_scores) len(tactical_scores), 'avg_crystallization_potential': sum(crystallization_scores) len(crystallization_scores), 'total_executions': len(self.execution_history) get_resonance_report(self): \"\"\"Get current Ω metrics report\"\"\" return 'current_metrics': self.resonance_metrics, 'recent_trend': self._calculate_trend(), 'compliance_score': self._calculate_compliance_score() _calculate_trend(self): \"\"\"Calculate Ω trend recent executions\"\"\" len(self.execution_history) return \"insufficient_data\" recent_10 self.execution_history[-10:] older_10 self.execution_history[-20:-10] recent_avg sum(ex.get('tactical_resonance', recent_10) older_avg sum(ex.get('tactical_resonance', older_10) recent_avg older_avg 0.05: return \"improving\" recent_avg older_avg 0.05: return \"declining\" else: return \"stable\" _calculate_compliance_score(self): \"\"\"Calculate overall Φ compliance score\"\"\" self.execution_history: return recent_executions self.execution_history[-50:] successful_executions recent_executions ex.get('status') 'Success'] success_rate len(successful_executions) len(recent_executions) avg_confidence sum(ex.get('confidence', successful_executions) max(len(successful_executions), avg_resonance self.resonance_metrics['avg_tactical_resonance'] Weighted compliance score compliance_score (success_rate (avg_confidence (avg_resonance return min(compliance_score, Session State Manager .session_state_manager import load_session_state, save_session_state, append_fact .context_superposition import create_context_bundle, merge_bundles .prefetch_manager import trigger_predictive_prefetch .SIRC_autonomy import maybe_autorun_SIRC .causal_digest import build_flux_annotated_digest except Exception: Fallback no-ops module available load_session_state(): return {\"facts_ledger\": \"updated_at\": now_iso()} save_session_state(state): return None append_fact(state, fact): state.setdefault(\"facts_ledger\", []).append({**fact, \"ts\": now_iso()}) create_context_bundle(Θ_def, runtime_context, initial_context): return {\"Θ_id\": Θ_def.get(\"Θ_id\"), \"created_at\": now_iso()} merge_bundles(bundles): return {\"Θ_index\": [b.get(\"Θ_id\") bundles]} _execute_standalone_workflow(workflow_D: Dict[str, Any], initial_context: Dict[str, Any], parent_run_id: action_registry: Dict[str, Callable]) Dict[str, Any]: Executes workflow D in-memory, meta-actions for_each. simplified version run_workflow loop, moved outside class prevent circular dependencies. run_id f\"{parent_run_id}_sub_{uuid.uuid4().hex[:8]}\" initial_context[\"workflow_run_id\"] run_id runtime_context \"initial_context\": initial_context, \"workflow_run_id\": run_id, \"workflow_D\": workflow_D, tasks workflow_D.get('tasks', task_statuses {key: \"pending\" tasks} sorted_tasks list(tasks.keys()) logger.info(f\"Starting standalone sub-workflow ID: {run_id}).\") start_time time.time() task_key sorted_tasks: task_info tasks[task_key] action_type task_info.get(\"action_type\") critical simplification: resolver without re-introducing dependency. We'll manually resolve resolved_inputs task_info.get('inputs', {}).items(): isinstance(v, resolved_inputs[k] initial_context.get('item') else: resolved_inputs[k] action_func action_registry.get(action_type) action_func: error_result {\"error\": f\"Sub-workflow failed: Action '{action_type}' found.\"} runtime_context[task_key] self._ensure_Φ_compliance(error_result, action_context_obj) task_statuses[task_key] \"failed\" break arguments passed keyword arguments align action wrappers action_registry.py result action_func(**resolved_inputs) isinstance(result, dict): result {\"output\": result} runtime_context[task_key] self._ensure_Φ_compliance(result, action_context_obj) task_statuses[task_key] \"completed\" except Exception error_msg f\"Sub-workflow '{task_key}' failed: logger.error(error_msg, exc_info=True) error_result {\"error\": error_msg} runtime_context[task_key] self._ensure_Φ_compliance(error_result, action_context_obj) task_statuses[task_key] \"failed\" break end_time time.time() run_duration round(end_time start_time, final_status \"Completed\" \"completed\" task_statuses.values()) \"Failed\" _summarize_run directly, construct simplified summary return \"workflow_name\": workflow_D.get(\"name\", \"sub-workflow\"), \"run_id\": run_id, \"status\": final_status, \"duration\": run_duration, \"task_statuses\": task_statuses, \"results\": runtime_context class ΦCompliantWorkflowEngine: \"\"\"Enhanced W Φ compliance recovery support.\"\"\" __init__(self, workflows_dir: \"workflows\", Θ_manager=None, event_callback: Optional[Callable] None): self.workflows_dir workflows_dir self.Θ_manager Θ_manager self.event_callback event_callback ADDED self.last_workflow_name None self.action_registry main_action_registry.actions.copy() Use centralized registry self.recovery_handler None self.current_run_id None self.current_workflow None self.Φ_validator ΦValidator() self.resonance_tracker ResonanceTracker() self.jinja_env Environment() Initialize Jinja2 environment Register standard actions self.register_action(\"display_output\", display_output) self.register_action(\"perform_S_genesis_action\", perform_S_genesis_action) self.register_action(\"run_code_linter\", run_code_linter) self.register_action(\"run_workflow_suite\", run_workflow_suite) Register recovery actions self.register_recovery_actions() logger.info( \"ΦCompliantWorkflowEngine initialized vetting capabilities\") @log_to_thought_trail register_action(self, action_type: action_func: Callable) None: \"\"\"Register action function engine.\"\"\" main_action_registry.register_action( action_type, action_func, self.action_registry main_action_registry.actions.copy() Update local logger.de",
    "compression_ratio": 2.8274299454360494,
    "symbol_count": 10813,
    "timestamp": "2025-11-18T10:59:57.373849Z"
  },
  {
    "stage_name": "Femto",
    "content": "TERM: Class: ΦValidator D: Validates Φ structure compliance crystallized artifacts specification BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Æ/workflow_engine.py, type: python_class I CODE (workflow_engine.py) First 30KB: ```python ResonantiA P workflow_engine.py Orchestrates execution defined workflows Blueprints). Manages context, dependencies, conditions, action execution, error handling. Critically handles Φ (Φ) results storing complete action output dictionary (including 'CRC') context. import import import logging import import import import import tempfile datetime import datetime typing import Dict, Any, List, Optional, Set, Union, Tuple, Callable jinja2 import Environment, meta, exceptions Import Jinja2 Standardized Imports single block ensures robust relative imports within package. import config .action_registry import execute_action, main_action_registry .Θ_manager import ΘManager .error_handler import handle_action_error .action_context import ActionContext .workflow_recovery import WorkflowRecoveryHandler .recovery_actions import analyze_failure, fix_template, fix_action, validate_workflow, validate_action, self_heal_output .S_genesis_tool import perform_S_genesis_action .qa_tools import run_code_linter, run_workflow_suite .output_handler import print_tagged_execution, print_tagged_results, display_output, display_workflow_start, display_workflow_progress, display_task_result, display_workflow_complete, display_workflow_error .custom_json import dumps .thought_trail import log_to_thought_trail except ImportError block ideally package, serves fallback clear error indicator. import logging _import_error_msg str(e) Capture error message string logging.getLogger(__name__).critical(f\" critical relative import failed workflow_engine: {e}\", exc_info=True) Set WorkflowOptimizer None check WorkflowOptimizer None Define dummy fallbacks prevent outright crashing possible execute_action(*args, **kwargs): return {\"error\": f\"Action Registry available import error: {_import_error_msg}\"} Add other necessary fallbacks needed... class ΘManager: Ensure decorator defined avoid NameError class annotations log_to_thought_trail(func): return Optional import WorkflowOptimizer (requires networkx) .workflow_optimizer import WorkflowOptimizer except ImportError: WorkflowOptimizer None import datetime import datetime ============================================================================ Δ CORE INTEGRATION (CANONICAL DATETIME S) ============================================================================ Three_PointO_Æ.temporal_core import now_iso, from_now, F_log, F_filename Attempt import numpy numeric checking _compare_values, optional import numpy except ImportError: None Set None numpy available logging.info( \"Numpy found, numeric checks _compare_values limited.\") logger logging.getLogger(__name__) Φ COMPLIANCE ENHANCEMENT Crystallized Artifacts I ARTIFACT class ΦValidator: \"\"\"Validates Φ structure compliance crystallized artifacts specification\"\"\" __init__(self): self.required_fields 'status', 'summary', 'confidence', 'alignment_check', 'potential_issues', 'raw_output_preview' self.enhanced_fields 'tactical_resonance', 'crystallization_potential' validate_structure(self, Φ_data): \"\"\"Validate Φ structure meets requirements\"\"\" isinstance(Φ_data, dict): return False, [\"Φ dictionary\"] missing_fields field self.required_fields: field Φ_data: missing_fields.append(field) issues missing_fields: issues.extend( [f\"Missing required field: {field}\" field missing_fields]) Validate confidence float between confidence Φ_data.get('confidence') confidence None: isinstance( confidence, (int, float)) confidence 1.0): issues.append(\"Confidence float between 1.0\") Validate status valid status Φ_data.get('status') status ['Success', 'Partial', 'Failed']: issues.append(\"Status 'Success', 'Partial', 'Failed'\") return len(issues) issues validate_enhanced_fields(self, Φ_data): \"\"\"Validate enhanced Φ fields tactical Ω\"\"\" enhanced_issues field self.enhanced_fields: field Φ_data: value Φ_data[field] isinstance( value, (int, float)) value 1.0): enhanced_issues.append( f\"{field} float between 1.0\") return len(enhanced_issues) enhanced_issues class ResonanceTracker: \"\"\"Tracks tactical Ω Π metrics\"\"\" __init__(self): self.execution_history self.resonance_metrics 'avg_tactical_resonance': 'avg_crystallization_potential': 'total_executions': record_execution(self, task_id, Φ_data, context): \"\"\"Record execution Ω tracking\"\"\" execution_record 'timestamp': now_iso(), 'task_id': task_id, 'status': Φ_data.get('status'), 'confidence': Φ_data.get('confidence', 0.0), 'tactical_resonance': Φ_data.get('tactical_resonance', 0.0), 'crystallization_potential': Φ_data.get('crystallization_potential', self.execution_history.append(execution_record) self._update_metrics() return execution_record _update_metrics(self): \"\"\"Update aggregate Ω metrics\"\"\" self.execution_history: return Last executions recent_executions self.execution_history[-100:] tactical_scores [ex.get('tactical_resonance', recent_executions] crystallization_scores ex.get( 'crystallization_potential', recent_executions] self.resonance_metrics 'avg_tactical_resonance': sum(tactical_scores) len(tactical_scores), 'avg_crystallization_potential': sum(crystallization_scores) len(crystallization_scores), 'total_executions': len(self.execution_history) get_resonance_report(self): \"\"\"Get current Ω metrics report\"\"\" return 'current_metrics': self.resonance_metrics, 'recent_trend': self._calculate_trend(), 'compliance_score': self._calculate_compliance_score() _calculate_trend(self): \"\"\"Calculate Ω trend recent executions\"\"\" len(self.execution_history) return \"insufficient_data\" recent_10 self.execution_history[-10:] older_10 self.execution_history[-20:-10] recent_avg sum(ex.get('tactical_resonance', recent_10) older_avg sum(ex.get('tactical_resonance', older_10) recent_avg older_avg 0.05: return \"improving\" recent_avg older_avg 0.05: return \"declining\" else: return \"stable\" _calculate_compliance_score(self): \"\"\"Calculate overall Φ compliance score\"\"\" self.execution_history: return recent_executions self.execution_history[-50:] successful_executions recent_executions ex.get('status') 'Success'] success_rate len(successful_executions) len(recent_executions) avg_confidence sum(ex.get('confidence', successful_executions) max(len(successful_executions), avg_resonance self.resonance_metrics['avg_tactical_resonance'] Weighted compliance score compliance_score (success_rate (avg_confidence (avg_resonance return min(compliance_score, Session State Manager .session_state_manager import load_session_state, save_session_state, append_fact .context_superposition import create_context_bundle, merge_bundles .prefetch_manager import trigger_predictive_prefetch .SIRC_autonomy import maybe_autorun_SIRC .causal_digest import build_flux_annotated_digest except Exception: Fallback no-ops module available load_session_state(): return {\"facts_ledger\": \"updated_at\": now_iso()} save_session_state(state): return None append_fact(state, fact): state.setdefault(\"facts_ledger\", []).append({**fact, \"ts\": now_iso()}) create_context_bundle(Θ_def, runtime_context, initial_context): return {\"Θ_id\": Θ_def.get(\"Θ_id\"), \"created_at\": now_iso()} merge_bundles(bundles): return {\"Θ_index\": [b.get(\"Θ_id\") bundles]} _execute_standalone_workflow(workflow_D: Dict[str, Any], initial_context: Dict[str, Any], parent_run_id: action_registry: Dict[str, Callable]) Dict[str, Any]: Executes workflow D -memory, meta-actions for_each. simplified version run_workflow loop, moved outside class prevent circular dependencies. run_id f\"{parent_run_id}_sub_{uuid.uuid4().hex[:8]}\" initial_context[\"workflow_run_id\"] run_id runtime_context \"initial_context\": initial_context, \"workflow_run_id\": run_id, \"workflow_D\": workflow_D, tasks workflow_D.get('tasks', task_statuses {key: \"pending\" tasks} sorted_tasks list(tasks.keys()) logger.info(f\"Starting standalone sub-workflow ID: {run_id}).\") start_time time.time() task_key sorted_tasks: task_info tasks[task_key] action_type task_info.get(\"action_type\") critical simplification: resolver without re-introducing dependency. We'll manually resolve resolved_inputs task_info.get('inputs', {}).items(): isinstance(v, resolved_inputs[k] initial_context.get('item') else: resolved_inputs[k] action_func action_registry.get(action_type) action_func: error_result {\"error\": f\"Sub-workflow failed: Action '{action_type}' found.\"} runtime_context[task_key] self._ensure_Φ_compliance(error_result, action_context_obj) task_statuses[task_key] \"failed\" break arguments passed keyword arguments align action wrappers action_registry.py result action_func(**resolved_inputs) isinstance(result, dict): result {\"output\": result} runtime_context[task_key] self._ensure_Φ_compliance(result, action_context_obj) task_statuses[task_key] \"completed\" except Exception error_msg f\"Sub-workflow '{task_key}' failed: logger.error(error_msg, exc_info=True) error_result {\"error\": error_msg} runtime_context[task_key] self._ensure_Φ_compliance(error_result, action_context_obj) task_statuses[task_key] \"failed\" break end_time time.time() run_duration round(end_time start_time, final_status \"Completed\" \"completed\" task_statuses.values()) \"Failed\" _summarize_run directly, construct simplified summary return \"workflow_name\": workflow_D.get(\"name\", \"sub-workflow\"), \"run_id\": run_id, \"status\": final_status, \"duration\": run_duration, \"task_statuses\": task_statuses, \"results\": runtime_context class ΦCompliantWorkflowEngine: \"\"\"Enhanced W Φ compliance recovery support.\"\"\" __init__(self, workflows_dir: \"workflows\", Θ_manager=None, event_callback: Optional[Callable] None): self.workflows_dir workflows_dir self.Θ_manager Θ_manager self.event_callback event_callback ADDED self.last_workflow_name None self.action_registry main_action_registry.actions.copy() Use centralized registry self.recovery_handler None self.current_run_id None self.current_workflow None self.Φ_validator ΦValidator() self.resonance_tracker ResonanceTracker() self.jinja_env Environment() Initialize Jinja2 environment Register standard actions self.register_action(\"display_output\", display_output) self.register_action(\"perform_S_genesis_action\", perform_S_genesis_action) self.register_action(\"run_code_linter\", run_code_linter) self.register_action(\"run_workflow_suite\", run_workflow_suite) Register recovery actions self.register_recovery_actions() logger.info( \"ΦCompliantWorkflowEngine initialized vetting capabilities\") @log_to_thought_trail register_action(self, action_type: action_func: Callable) None: \"\"\"Register action function engine.\"\"\" main_action_registry.register_action( action_type, action_func, self.action_registry main_action_registry.actions.copy() Update local logger.de",
    "compression_ratio": 2.8282146160962074,
    "symbol_count": 10810,
    "timestamp": "2025-11-18T10:59:57.740086Z"
  },
  {
    "stage_name": "Atto",
    "content": "TERM: Class: ΦValidator D: Validates Φ BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Æ/workflow_engine.py, I CODE First 30KB: ResonantiA P Orchestrates Blueprints). Manages Critically Φ (Φ) 'CRC') Dict, Any, List, Optional, Set, Union, Tuple, Callable Environment, Import Jinja2 Standardized Imports .Θ_manager ΘManager ActionContext WorkflowRecoveryHandler ImportError Capture Set WorkflowOptimizer None WorkflowOptimizer None Define Registry Add ΘManager: Ensure NameError Optional WorkflowOptimizer WorkflowOptimizer ImportError: WorkflowOptimizer None Δ CORE INTEGRATION (CANONICAL DATETIME S) Three_PointO_Æ.temporal_core F_log, F_filename Attempt ImportError: None Set None Φ COMPLIANCE ENHANCEMENT Crystallized Artifacts I ARTIFACT ΦValidator: Φ Φ_data): Φ isinstance(Φ_data, False, [\"Φ Φ_data: Validate Φ_data.get('confidence') None: Validate Φ_data.get('status') Φ_data): Φ Ω\"\"\" Φ_data: Φ_data[field] ResonanceTracker: Ω Π Φ_data, Ω Φ_data.get('status'), Φ_data.get('confidence', Φ_data.get('tactical_resonance', Φ_data.get('crystallization_potential', Ω Last Ω Ω Φ Weighted Session State Manager Exception: Fallback None create_context_bundle(Θ_def, {\"Θ_id\": Θ_def.get(\"Θ_id\"), {\"Θ_index\": [b.get(\"Θ_id\") Dict[str, Any], Dict[str, Any], Dict[str, Callable]) Dict[str, Any]: Executes D ID: We'll Action self._ensure_Φ_compliance(error_result, self._ensure_Φ_compliance(result, Exception self._ensure_Φ_compliance(error_result, ΦCompliantWorkflowEngine: W Φ Θ_manager=None, Optional[Callable] None): self.Θ_manager Θ_manager ADDED None Use None None None self.Φ_validator ΦValidator() ResonanceTracker() Environment() Initialize Jinja2 Register Register \"ΦCompliantWorkflowEngine Callable) None: Update",
    "compression_ratio": 17.611175115207374,
    "symbol_count": 1736,
    "timestamp": "2025-11-18T10:59:58.004550Z"
  },
  {
    "stage_name": "Zepto",
    "content": "Φ|Φ|Æ|Φ|Φ",
    "compression_ratio": 3397.0,
    "symbol_count": 9,
    "timestamp": "2025-11-18T10:59:58.016334Z"
  }
]