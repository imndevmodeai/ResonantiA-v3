[
  {
    "stage_name": "Narrative",
    "content": "TERM: Function: ensure_directories\n\nDEFINITION:\nCreates necessary directories defined in config.py if they don't exist.\n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/main.py, type: python_function\n\nFULL IMPLEMENTATION CODE (main.py):\n```python\n# --- START OF FILE Three_PointO_ArchE/main.py ---\n# ResonantiA Protocol v3.0 - main.py\n# Example entry point demonstrating initialization and execution of the Arche system.\n# Handles workflow execution via IARCompliantWorkflowEngine and manages IAR-inclusive results.\n\nimport logging\nimport os\nimport json\nimport argparse\nimport sys\nimport time\nimport uuid # For unique workflow run IDs\nfrom typing import Optional, Dict, Any, Union # Added for type hinting clarity\n\n# --- Helper function to truncate values for summary ---\ndef truncate_value(value: Any, max_len: int = 70) -> str:\n    \"\"\"Converts a value to string and truncates it if too long.\"\"\"\n    try:\n        s_value = str(value)\n        if len(s_value) > max_len:\n            return s_value[:max_len-3] + \"...\"\n        return s_value\n    except Exception:\n        return \"[Unrepresentable Value]\"\n# --- End helper function ---\n\n# Setup logging FIRST using the centralized configuration\ntry:\n    # Assumes config and logging_config are in the same package directory\n    from . import config # Use relative import within the package\n    from .logging_config import setup_logging\n    setup_logging() # Initialize logging based on config settings\nexcept ImportError as cfg_imp_err:\n    # Basic fallback logging if config files are missing during setup\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', stream=sys.stdout)\n    logging.warning(f\"Could not import config/logging_config via relative import: {cfg_imp_err}. Using basic stdout logging.\", exc_info=True)\nexcept Exception as log_setup_e:\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', stream=sys.stdout)\n    logging.error(f\"Error setting up logging from logging_config.py: {log_setup_e}. Using basic config.\", exc_info=True)\n\n# Now import other core ResonantiA modules AFTER logging is configured\ntry:\n    from .workflow_engine import IARCompliantWorkflowEngine as IARCompliantWorkflowEngine\n    from .spr_manager import SPRManager\n    from .sirc_intake_handler import SIRCIntakeHandler\n    # config already imported above\nexcept ImportError as import_err:\n    logging.critical(f\"Failed to import core ResonantiA modules (IARCompliantWorkflowEngine, SPRManager, SIRCIntakeHandler): {import_err}. Check installation and paths.\", exc_info=True)\n    sys.exit(1) # Critical failure if core components cannot be imported\n\nlogger = logging.getLogger(__name__) # Get logger specifically for this module\n\ndef ensure_directories():\n    \"\"\"Creates necessary directories defined in config.py if they don't exist.\"\"\"\n    # Fetches paths from the config module\n    dirs_to_check = [\n        getattr(config, 'LOG_DIR', 'logs'),\n        getattr(config, 'OUTPUT_DIR', 'outputs'),\n        getattr(config, 'WORKFLOW_DIR', 'workflows'),\n        getattr(config, 'KNOWLEDGE_GRAPH_DIR', 'knowledge_graph'),\n        getattr(config, 'MODEL_SAVE_DIR', 'outputs/models') # Includes subdirectory for models\n    ]\n    logger.info(f\"Ensuring base directories exist: {dirs_to_check}\")\n    for d in dirs_to_check:\n        if d and isinstance(d, str): # Check if path is valid string\n            try:\n                os.makedirs(d, exist_ok=True) # exist_ok=True prevents error if dir exists\n            except OSError as e:\n                # Log critical error and raise to halt execution if essential dirs can't be made\n                logger.critical(f\"CRITICAL: Failed to create directory {d}: {e}. Check permissions.\", exc_info=True)\n                raise\n        else:\n            logger.warning(f\"Skipping invalid directory path configured: {d}\")\n\n    # Specifically ensure the SPR definitions file exists, creating an empty list if not\n    spr_file = getattr(config, 'SPR_JSON_FILE', None)\n    if spr_file and isinstance(spr_file, str):\n        if not os.path.exists(spr_file):\n            try:\n                spr_dir = os.path.dirname(spr_file)\n                if spr_dir: os.makedirs(spr_dir, exist_ok=True)\n                with open(spr_file, 'w', encoding='utf-8') as f:\n                    json.dump([], f) # Create file with an empty JSON list\n                logger.info(f\"Created empty SPR definitions file at {spr_file}\")\n            except IOError as e:\n                logger.error(f\"Could not create empty SPR file at {spr_file}: {e}\")\n            except Exception as e:\n                logger.error(f\"Unexpected error ensuring SPR file exists: {e}\", exc_info=True)\n    else:\n        logger.warning(\"SPR_JSON_FILE not configured or invalid in config.py.\")\n\ndef find_last_successful_run_id(output_dir: str) -> Optional[str]:\n    \"\"\"Finds the most recent successfully completed workflow run ID.\"\"\"\n    try:\n        result_files = [f for f in os.listdir(output_dir) if f.startswith(\"result_\") and f.endswith(\".json\")]\n        result_files.sort(key=lambda f: os.path.getmtime(os.path.join(output_dir, f)), reverse=True)\n\n        for filename in result_files:\n            filepath = os.path.join(output_dir, filename)\n            with open(filepath, 'r') as f:\n                result_data = json.load(f)\n                if result_data.get(\"workflow_status\") == \"Completed Successfully\":\n                    run_id = result_data.get(\"workflow_run_id\")\n                    if run_id:\n                        logger.info(f\"Found last successful run_id: {run_id} from file {filename}\")\n                        return run_id\n    except Exception as e:\n        logger.error(f\"Could not determine last successful run ID: {e}\", exc_info=True)\n    return None\n\ndef handle_sirc_directive(args):\n    \"\"\"Handler for SIRC directive processing with DirectiveClarificationProtocoL.\"\"\"\n    logger.info(f\"--- Processing SIRC directive: {args.directive[:100]}... ---\")\n    \n    try:\n        # Initialize SPR Manager for SIRC handler\n        spr_manager = SPRManager(getattr(config, 'SPR_JSON_FILE', None))\n        \n        # Initialize SIRC Intake Handler\n        sirc_handler = SIRCIntakeHandler(spr_manager=spr_manager)\n        \n        # Process directive through clarification protocol\n        clarification_result = sirc_handler.process_directive(args.directive)\n        \n        # Display results\n        print(\"\\n=== SIRC DIRECTIVE CLARIFICATION RESULT ===\")\n        print(f\"Original Directive: {clarification_result['original_directive']}\")\n        print(f\"Finalized Objective: {clarification_result['finalized_objective']}\")\n        print(f\"Clarity Score: {clarification_result['clarity_score']:.2f}\")\n        print(f\"Clarification Needed: {clarification_result['clarification_needed']}\")\n        \n        if 'resonance_validation' in clarification_result:\n            print(\"\\nResonance Validation:\")\n            for key, value in clarification_result['resonance_validation'].items():\n                print(f\"  {key}: {value}\")\n        \n        # Save results\n        # Use new config container if present\n        try:\n            output_dir = config.CONFIG.paths.outputs.as_posix()\n        except Exception:\n            output_dir = getattr(config, 'OUTPUT_DIR', 'outputs')\n        os.makedirs(output_dir, exist_ok=True)\n        output_filename = os.path.join(output_dir, f\"sirc_clarification_{uuid.uuid4().hex[:8]}.json\")\n        \n        with open(output_filename, 'w', encoding='utf-8') as f:\n            json.dump(clarification_result, f, indent=2, default=str)\n        logger.info(f\"SIRC clarification result saved to {output_filename}\")\n        \n        # If clarification successful and execution ready, offer to proceed with SIRC\n        if clarification_result.get('clarity_score', 0) > 0.85:\n            print(f\"\\n✓ Objective clarity threshold met (score: {clarification_result['clarity_score']:.2f})\")\n            print(\"Ready for SIRC Phase 3: Harmonization Check\")\n            \n            # Here we would normally proceed to full SIRC processing\n            # For now, we'll just log the readiness\n            logger.info(\"Directive successfully clarified and ready for SIRC continuation\")\n        else:\n            print(f\"\\n⚠ Objective clarity below threshold (score: {clarification_result['clarity_score']:.2f})\")\n            print(\"Additional clarification may be needed before SIRC processing\")\n            \n    except Exception as e:\n        logger.error(f\"Error processing SIRC directive: {e}\", exc_info=True)\n        print(f\"ERROR: Failed to process SIRC directive: {e}\")\n\ndef handle_run_workflow(args):\n    \"\"\"Handler for the 'run-workflow' command.\"\"\"\n    logger.info(f\"--- Received command to run workflow: {args.workflow_name} ---\")\n    initial_context = {}\n    if args.context_file:\n        try:\n            with open(args.context_file, 'r') as f:\n                initial_context = json.load(f)\n            logger.info(f\"Loaded initial context from: {args.context_file}\")\n        except (FileNotFoundError, json.JSONDecodeError) as e:\n            logger.error(f\"Error loading context file {args.context_file}: {e}\")\n            # Decide if you want to exit or run with an empty context\n            return \n\n    try:\n        engine = IARCompliantWorkflowEngine()\n\n        # Construct the full path to the workflow file\n        workflow_dir = getattr(config, 'WORKFLOW_DIR', 'workflows')\n        workflow_path = os.path.join(workflow_dir, args.workflow_name)\n        \n        final_context = engine.run_workflow(workflow_path, initial_context)\n        \n        # Determine the final status for logging\n        final_status = final_context.get(\"workflow_status\", \"Unknown\")\n\n        # Save the final context to a file\n        try:\n            output_dir = config.CONFIG.paths.outputs.as_posix()\n        except Exception:\n            output_dir = getattr(config, 'OUTPUT_DIR', 'outputs')\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Sanitize workflow name for the filename and add run_id\n        sanitized_name = os.path.splitext(os.path.basename(args.workflow_name))[0].replace(\" \", \"_\")\n        run_id = final_context.get('workflow_run_id', 'no_run_id')\n        output_filename = os.path.join(output_dir, f\"result_{sanitized_name}_{run_id}.json\")\n\n        try:\n            with open(output_filename, 'w', encoding='utf-8') as f:\n                # Use default=str to handle potential non-serializable types gracefully (e.g., numpy types)\n                json.dump(final_context, f, indent=2, default=str)\n            logger.info(f\"Final result saved successfully to {output_filename}\")\n        except TypeError as json_err:\n            # Handle cases where the result dictionary contains objects JSON can't serialize directly\n            logger.error(f\"Could not serialize final result to JSON: {json_err}. Result likely contains non-standard objects (e.g., complex numbers, custom classes). Saving string representation as fallback.\", exc_info=True)\n            fallback_filename = output_filename.replace('.json', '_error_repr.txt')\n            try:\n                with open(fallback_filename, 'w', encoding='utf-8') as f:\n                    f.write(f\"Original JSON serialization error: {json_err}\\n\\n\")\n                    f.write(\"--- Full Result (repr) ---\\n\")\n                    f.write(repr(final_context)) # Write the Python representation\n                logger.info(f\"String representation saved to {fallback_filename}\")\n            except Exception as write_err:\n                logger.error(f\"Could not write fallback string representation: {write_err}\")\n        except IOError as io_err:\n            logger.error(f\"Could not write final result to {output_filename}: {io_err}\")\n        except Exception as save_err:\n            logger.error(f\"Unexpected error saving final result: {save_err}\", exc_info=True)\n\n        # --- Print Summary to Console ---\n        # Provides a quick overview of the execution outcome\n        print(\"\\n--- Workflow Final Result Summary (v3.0) ---\")\n        try:\n            summary = {}\n            summary['workflow_name'] = args.workflow_name\n            summary['workflow_run_id'] = run_id\n            summary['overall_status'] = final_status\n            summary['run_duration_sec'] = final_context.get('workflow_run_duration_sec', 'N/A')\n\n            # Summarize status and IAR reflection highlights for each task\n            task_statuses = final_context.get('task_statuses', {})\n            summary['task_summary'] = {}\n            for task_id, status in task_statuses.items():\n                task_result = final_context.get(task_id, {})\n                reflection = task_result.get('reflection', {}) if isinstance(task_result, dict) else {}\n                \n                inputs_preview = {}\n                if isinstance(task_result, dict) and 'resolved_inputs' in task_result and isinstance(task_result['resolved_inputs'], dict):\n                    for k, v in task_result['resolved_inputs'].items():\n                        inputs_preview[k] = truncate_value(v)\n                \n                outputs_preview = {}\n                if isinstance(task_result, dict):\n                    # Exclude known meta-keys and reflection from outputs preview\n                    excluded_output_keys = ['reflection', 'resolved_inputs', 'error', 'status', 'start_time', 'end_time', 'duration_sec', 'attempt_number']\n                    for k, v in task_result.items():\n                        if k not in excluded_output_keys:\n                            outputs_preview[k] = truncate_value(v)\n\n                summary['task_summary'][task_id] = {\n                    \"status\": status,\n                    \"inputs_preview\": inputs_preview if inputs_preview else \"N/A\",\n                    \"outputs_preview\": outputs_preview if outputs_preview else \"N/A\",\n                    \"reflection_status\": reflection.get('status', 'N/A'),\n                    \"reflection_confidence\": reflection.get('confidence', 'N/A'),\n                    \"reflection_issues\": reflection.get('potential_issues', None),\n                    \"error\": truncate_value(task_result.get('error', None)) # Also truncate error messages\n                }\n            # Print the summary dict as formatted JSON\n            print(json.dumps(summary, indent=2, default=str))\n        except Exception as summary_e:\n            print(f\"(Could not generate summary: {summary_e})\")\n            print(f\"Full results saved to {output_filename} (or fallback file).\")\n        print(\"---------------------------------------------\\n\")\n\n    except Exception as e:\n        logger.critical(f\"A critical error occurred in the main execution block: {e}\", exc_info=True)\n\ndef main(workflow_to_run: str, initial_context_json: Optional[str] = None):\n    \"\"\"\n    Main execution function: Initializes system, runs workflow, saves results.\n    \"\"\"\n    logger.info(\"--- Arche System Initialization (ResonantiA Protocol v3.0) ---\")\n    logger.warning(\"Keyholder Override ('IMnDEVmode') is conceptually active for this session (as per protocol doc).\") # Note: Actual check might be needed elsewhere\n\n    # Ensure directories exist before initializing components that might need them\n    try:\n        ensure_directories()\n    except Exception as dir_e:\n        # If directory creation failed, log critical and exit\n        logger.critical(f\"Failed to ensure necessary directories: {dir_e}. Exiting.\")\n        sys.exit(1)\n\n    # Initialize core components\n    try:\n        spr_manager = SPRManager(getattr(config, 'SPR_JSON_FILE', None)) # Pass configured path\n        logger.info(f\"SPR Manager initialized. Loaded {len(spr_manager.sprs)} SPRs from '{spr_manager.filepath}'.\")\n    except (ValueError, TypeError) as spr_init_err: # Catch config errors specifically\n        logger.critical(f\"Failed to initialize SPR Manager due to configuration issue (SPR_JSON_FILE): {spr_init_err}. Exiting.\", exc_info=True)\n        sys.exit(1)\n    except Exception as spr_e:\n        logger.critical(f\"Unexpected error initializing SPR Manager: {spr_e}. Exiting.\", exc_info=True)\n        sys.exit(1)\n\n    try:\n        # Pass the initialized SPR manager to the engine if needed (e.g., for SPR context)\n        workflow_engine = IARCompliantWorkflowEngine(spr_manager=spr_manager)\n        logger.info(\"Workflow Engine initialized.\")\n    except Exception as wf_e:\n        logger.critical(f\"Failed to initialize Workflow Engine: {wf_e}. Exiting.\", exc_info=True)\n        sys.exit(1)\n\n    # --- Prepare Initial Context ---\n    initial_context: Dict[str, Any] = {}\n\n    # initial_context_json = \"{\\\"raw_user_query\\\": \\\"Can you provide an InnovativE SolutioN for energy crisis?\\\", \\\"user_id\\\": \\\"cli_keyholder_IMnDEVmode\\\", \\\"protocol_version\\\": \\\"3.0\\\"}\" # HARCODED FOR TEST\n\n    if initial_context_json:\n        try:\n            # Load context from JSON string argument\n            initial_context = json.loads(initial_context_json)\n            if not isinstance(initial_context, dict):\n                # Ensure the loaded JSON is actually a dictionary\n                raise json.JSONDecodeError(\"Initial context must be a JSON object (dictionary).\", initial_context_json, 0)\n            logger.info(\"Loaded initial context from command line argument.\")\n        except json.JSONDecodeError as e:\n            logger.error(f\"Invalid JSON provided for initial context: {e}. Starting with minimal context including error.\", exc_info=True)\n            initial_context = {\"error_loading_context\": f\"Invalid JSON: {e}\", \"raw_context_input\": initial_context_json}\n\n    # Add/ensure essential context variables\n    initial_context[\"user_id\"] = initial_context.get(\"user_id\", \"cli_keyholder_IMnDEVmode\") # Example user ID\n    initial_context[\"workflow_run_id\"] = initial_context.get(\"workflow_run_id\", f\"run_{uuid.uuid4().hex}\") # Unique ID for this run\n    initial_context[\"protocol_version\"] = \"3.0\" # Stamp the protocol version\n    \n    # Find and inject the last successful run ID for analysis workflows\n    last_run_id = find_last_successful_run_id(config.OUTPUT_DIR)\n    if last_run_id:\n        initial_context[\"last_successful_run_id\"] = last_run_id\n\n    # --- Execute Workflow ---\n    logger.info(f\"Attempting to execute workflow: '{workflow_to_run}' (Run ID: {initial_context['workflow_run_id']})\")\n    final_result: Dict[str, Any] = {}\n    try:\n        # Core execution call\n        final_result = workflow_engine.run_workflow(workflow_to_run, initial_context)\n        logger.info(f\"Workflow '{workflow_engine.last_workflow_name or workflow_to_run}' execution finished.\") # Use name loaded by engine if available\n\n        # --- Save Full Results ---\n        # Construct a unique filename for the results\n        base_workflow_name = os.path.basename(workflow_to_run).replace('.json', '')\n        output_filename = os.path.join(config.OUTPUT_DIR, f\"result_{base_workflow_name}_{initial_context['workflow_run_id']}.json\")\n\n        logger.info(f\"Attempting to save full final result dictionary to {output_filename}\")\n        try:\n            with open(output_filename, 'w', encoding='utf-8') as f:\n                # Use default=str to handle potential non-serializable types gracefully (e.g., numpy types)\n                json.dump(final_result, f, indent=2, default=str)\n            logger.info(f\"Final result saved successfully.\")\n        except TypeError as json_err:\n            # Handle cases where the result dictionary contains objects JSON can't serialize directly\n            logger.error(f\"Could not serialize final result to JSON: {json_err}. Result likely contains non-standard objects (e.g., complex numbers, custom classes). Saving string representation as fallback.\", exc_info=True)\n            fallback_filename = output_filename.replace('.json', '_error_repr.txt')\n            try:\n                with open(fallback_filename, 'w', encoding='utf-8') as f:\n                    f.write(f\"Original JSON serialization error: {json_err}\\n\\n\")\n                    f.write(\"--- Full Result (repr) ---\\n\")\n                    f.write(repr(final_result)) # Write the Python representation\n                logger.info(f\"String representation saved to {fallback_filename}\")\n            except Exception as write_err:\n                logger.error(f\"Could not write fallback string representation: {write_err}\")\n        except IOError as io_err:\n            logger.error(f\"Could not write final result to {output_filename}: {io_err}\")\n        except Exception as save_err:\n            logger.error(f\"Unexpected error saving final result: {save_err}\", exc_info=True)\n\n        # --- Print Summary to Console ---\n        # Provides a quick overview of the execution outcome\n        print(\"\\n--- Workflow Final Result Summary (v3.0) ---\")\n        try:\n            summary = {}\n            summary['workflow_name'] = workflow_engine.last_workflow_name or workflow_to_run\n            summary['workflow_run_id'] = initial_context['workflow_run_id']\n            summary['overall_status'] = final_result.get('workflow_status', 'Unknown')\n            summary['run_duration_sec'] = final_result.get('workflow_run_duration_sec', 'N/A')\n\n            # Summarize status and IAR reflection highlights for each task\n            task_statuses = final_result.get('task_statuses', {})\n            summary['task_summary'] = {}\n            for task_id, status in task_statuses.items():\n                task_result = final_result.get(task_id, {})\n                reflection = task_result.get('reflection', {}) if isinstance(task_result, dict) else {}\n                \n                inputs_preview = {}\n                if isinstance(task_result, dict) and 'resolved_inputs' in task_result and isinstance(task_result['resolved_inputs'], dict):\n                    for k, v in task_result['resolved_inputs'].items():\n                        inputs_preview[k] = truncate_value(v)\n                \n                outputs_preview = {}\n                if isinstance(task_result, dict):\n                    # Exclude known meta-keys and reflection from outputs preview\n                    excluded_output_keys = ['reflection', 'resolved_inputs', 'error', 'status', 'start_time', 'end_time', 'duration_sec', 'attempt_number']\n                    for k, v in task_result.items():\n                        if k not in excluded_output_keys:\n                            outputs_preview[k] = truncate_value(v)\n\n                summary['task_summary'][task_id] = {\n                    \"status\": status,\n                    \"inputs_preview\": inputs_preview if inputs_preview else \"N/A\",\n                    \"outputs_preview\": outputs_preview if outputs_preview else \"N/A\",\n                    \"reflection_status\": reflection.get('status', 'N/A'),\n                    \"reflection_confidence\": reflection.get('confidence', 'N/A'),\n                    \"reflection_issues\": reflection.get('potential_issues', None),\n                    \"error\": truncate_value(task_result.get('error', None)) # Also truncate error messages\n                }\n            # Print the summary dict as formatted JSON\n            print(json.dumps(summary, indent=2, default=str))\n        except Exception as summary_e:\n            print(f\"(Could not generate summary: {summary_e})\")\n            print(f\"Full results saved to {output_filename} (or fallback file).\")\n        print(\"---------------------------------------------\\n\")\n\n    except FileNotFoundError as e:\n        # Handle case where the specified workflow file doesn't exist\n        logger.error(f\"Workflow file '{workflow_to_run}' not found in '{config.WORKFLOW_DIR}': {e}\")\n        print(f\"ERROR: Workflow file '{workflow_to_run}' not found in '{config.WORKFLOW_DIR}'. Please check the filename and path.\")\n        sys.exit(1)\n    except (ValueError, TypeError) as setup_err:\n        # Handle errors likely related to configuration or workflow structure\n        logger.critical(f\"Workflow execution failed due to configuration or setup error: {setup_err}\", exc_info=True)\n        print(f\"ERROR: Workflow setup failed. Check configuration ({config.__file__}) and workflow structure ({workflow_to_run}). Error: {setup_err}\")\n        sys.exit(1)\n    except Exception as exec_err:\n        # Catch any other unexpected errors during workflow execution\n        logger.critical(f\"An critical unexpected error occurred during workflow execution: {exec_err}\", exc_info=True)\n        print(f\"ERROR: Workflow execution failed unexpectedly. Check logs at {config.LOG_FILE}. Error: {exec_err}\")\n        sys.exit(1)\n\n    logger.info(\"--- Arche System Shutdown ---\")\n\n# Ensure the package can be found if running the script directly\n# This might be less necessary when running with `python -m` but kept for broader compatibility\npackage_dir = os.path.dirname(__file__) # Directory of main.py (e.g., .../ResonantiA/3.0ArchE)\nproject_root = os.path.abspath(os.path.join(package_dir, '..')) # Project root (e.g., .../ResonantiA)\nif project_root not in sys.path:\n    sys.path.insert(0, project_root) # Add project root to Python path\n\n# --- Command Line Argument Parsing ---\nif __name__ == \"__main__\":\n    # Setup command-line argument parsing\n    parser = argparse.ArgumentParser(description=\"ResonantiA v3.0 Arche System - Main Executor\")\n    \n    # Using subparsers for different commands like 'run-workflow'\n    subparsers = parser.add_subparsers(dest=\"command\", required=True, help=\"Available commands\")\n    \n    # --- run-workflow Sub-parser ---\n    parser_run = subparsers.add_parser(\"run-workflow\", help=\"Execute a specified workflow.\")\n    parser_run.add_argument(\n        \"workflow_name\", \n        type=str,\n        help=\"The filename of the workflow to execute (e.g., 'quality_assurance_workflow.json'). Must be located in the configured WORKFLOW_DIR.\"\n    )\n    parser_run.add_argument(\n        \"--context-file\",\n        type=str,\n        default=None,\n        help=\"Path to a JSON file containing the initial context for the workflow run.\"\n    )\n    # Register the handler for the 'run-workflow' command\n    parser_run.set_defaults(func=handle_run_workflow)\n\n    # --- sirc-directive Sub-parser ---\n    parser_sirc = subparsers.add_parser(\"sirc-directive\", help=\"Process a strategic directive through SIRC intake handler with DirectiveClarificationProtocoL.\")\n    parser_sirc.add_argument(\n        \"directive\",\n        type=str,\n        help=\"The strategic directive to process (equivalent to keyy=('IMnDEVmode') directive input)\"\n    )\n    # Register the handler for the 'sirc-directive' command\n    parser_sirc.set_defaults(func=handle_sirc_directive)\n\n    # --- Other sub-parsers for different commands can be added here ---\n    # For example, a future 'manage-spr' command\n\n    args = parser.parse_args()\n\n    # Execute the function associated with the chosen command\n    if hasattr(args, 'func'):\n        args.func(args)\n    else:\n        parser.print_help()\n        sys.exit(1)\n\n# --- END OF FILE Three_PointO_ArchE/main.py --- \n```\n\nEXAMPLE APPLICATION:\nCreates necessary directories defined in config.py if they don't exist.\n\nCATEGORY: CodeKnowledge\n\nRELATIONSHIPS:\ntype: FromCodebase; source: /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/main.py; source_type: python_function",
    "compression_ratio": 1.0,
    "symbol_count": 27278,
    "timestamp": "2025-11-18T11:00:33.502510Z"
  },
  {
    "stage_name": "Concise",
    "content": "TERM: Function: ensure_directories\n\nDEFINITION:\nCreates necessary directories defined in config.py if they don't exist.\n\nBLUEPRINT DETAILS:\nExtracted from /mnt/3626C55326C514B1/Happier/Three_PointO_ArchE/main.py, type: python_function\n\nFULL IMPLEMENTATION CODE (main.py):\n```python\n# --- START OF FILE Three_PointO_ArchE/main.py ---\n# ResonantiA Protocol v3.0 - main.py\n# Example entry point demonstrating initialization and execution of the Arche system.\n# Handles workflow execution via IARCompliantWorkflowEngine and manages IAR-inclusive results.\n\nimport logging\nimport os\nimport json\nimport argparse\nimport sys\nimport time\nimport uuid # For unique workflow run IDs\nfrom typing import Optional, Dict, Any, Union # Added for type hinting clarity\n\n# --- Helper function to truncate values for summary ---\ndef truncate_value(value: Any, max_len: int = 70) -> str:\n    \"\"\"Converts a value to string and truncates it if too long.\"\"\"\n    try:\n        s_value = str(value)\n        if len(s_value) > max_len:\n            return s_value[:max_len-3] + \"...\"\n        return s_value\n    except Exception:\n        return \"[Unrepresentable Value]\"\n# --- End helper function ---\n\n# Setup logging FIRST using the centralized configuration\ntry:\n    # Assumes config and logging_config are in the same package directory\n    from . import config # Use relative import within the package\n    from .logging_config import setup_logging\n    setup_logging() # Initialize logging based on config settings\nexcept ImportError as cfg_imp_err:\n    # Basic fallback logging if config files are missing during setup\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', stream=sys.stdout)\n    logging.warning(f\"Could not import config/logging_config via relative import: {cfg_imp_err}. Using basic stdout logging.\", exc_info=True)\nexcept Exception as log_setup_e:\n    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', stream=sys.stdout)\n    logging.error(f\"Error setting up logging from logging_config.py: {log_setup_e}. Using basic config.\", exc_info=True)\n\n# Now import other core ResonantiA modules AFTER logging is configured\ntry:\n    from .workflow_engine import IARCompliantWorkflowEngine as IARCompliantWorkflowEngine\n    from .spr_manager import SPRManager\n    from .sirc_intake_handler import SIRCIntakeHandler\n    # config already imported above\nexcept ImportError as import_err:\n    logging.critical(f\"Failed to import core ResonantiA modules (IARCompliantWorkflowEngine, SPRManager, SIRCIntakeHandler): {import_err}. Check installation and paths.\", exc_info=True)\n    sys.exit(1) # Critical failure if core components cannot be imported\n\nlogger = logging.getLogger(__name__) # Get logger specifically for this module\n\ndef ensure_directories():\n    \"\"\"Creates necessary directories defined in config.py if they don't exist.\"\"\"\n    # Fetches paths from the config module\n    dirs_to_check = [\n        getattr(config, 'LOG_DIR', 'logs'),\n        getattr(config, 'OUTPUT_DIR', 'outputs'),\n        getattr(config, 'WORKFLOW_DIR', 'workflows'),\n        getattr(config, 'KNOWLEDGE_GRAPH_DIR', 'knowledge_graph'),\n        getattr(config, 'MODEL_SAVE_DIR', 'outputs/models') # Includes subdirectory for models\n    ]\n    logger.info(f\"Ensuring base directories exist: {dirs_to_check}\")\n    for d in dirs_to_check:\n        if d and isinstance(d, str): # Check if path is valid string\n            try:\n                os.makedirs(d, exist_ok=True) # exist_ok=True prevents error if dir exists\n            except OSError as e:\n                # Log critical error and raise to halt execution if essential dirs can't be made\n                logger.critical(f\"CRITICAL: Failed to create directory {d}: {e}. Check permissions.\", exc_info=True)\n                raise\n        else:\n            logger.warning(f\"Skipping invalid directory path configured: {d}\")\n\n    # Specifically ensure the SPR definitions file exists, creating an empty list if not\n    spr_file = getattr(config, 'SPR_JSON_FILE', None)\n    if spr_file and isinstance(spr_file, str):\n        if not os.path.exists(spr_file):\n            try:\n                spr_dir = os.path.dirname(spr_file)\n                if spr_dir: os.makedirs(spr_dir, exist_ok=True)\n                with open(spr_file, 'w', encoding='utf-8') as f:\n                    json.dump([], f) # Create file with an empty JSON list\n                logger.info(f\"Created empty SPR definitions file at {spr_file}\")\n            except IOError as e:\n                logger.error(f\"Could not create empty SPR file at {spr_file}: {e}\")\n            except Exception as e:\n                logger.error(f\"Unexpected error ensuring SPR file exists: {e}\", exc_info=True)\n    else:\n        logger.warning(\"SPR_JSON_FILE not configured or invalid in config.py.\")\n\ndef find_last_successful_run_id(output_dir: str) -> Optional[str]:\n    \"\"\"Finds the most recent successfully completed workflow run ID.\"\"\"\n    try:\n        result_files = [f for f in os.listdir(output_dir) if f.startswith(\"result_\") and f.endswith(\".json\")]\n        result_files.sort(key=lambda f: os.path.getmtime(os.path.join(output_dir, f)), reverse=True)\n\n        for filename in result_files:\n            filepath = os.path.join(output_dir, filename)\n            with open(filepath, 'r') as f:\n                result_data = json.load(f)\n                if result_data.get(\"workflow_status\") == \"Completed Successfully\":\n                    run_id = result_data.get(\"workflow_run_id\")\n                    if run_id:\n                        logger.info(f\"Found last successful run_id: {run_id} from file {filename}\")\n                        return run_id\n    except Exception as e:\n        logger.error(f\"Could not determine last successful run ID: {e}\", exc_info=True)\n    return None\n\ndef handle_sirc_directive(args):\n    \"\"\"Handler for SIRC directive processing with DirectiveClarificationProtocoL.\"\"\"\n    logger.info(f\"--- Processing SIRC directive: {args.directive[:100]}... ---\")\n    \n    try:\n        # Initialize SPR Manager for SIRC handler\n        spr_manager = SPRManager(getattr(config, 'SPR_JSON_FILE', None))\n        \n        # Initialize SIRC Intake Handler\n        sirc_handler = SIRCIntakeHandler(spr_manager=spr_manager)\n        \n        # Process directive through clarification protocol\n        clarification_result = sirc_handler.process_directive(args.directive)\n        \n        # Display results\n        print(\"\\n=== SIRC DIRECTIVE CLARIFICATION RESULT ===\")\n        print(f\"Original Directive: {clarification_result['original_directive']}\")\n        print(f\"Finalized Objective: {clarification_result['finalized_objective']}\")\n        print(f\"Clarity Score: {clarification_result['clarity_score']:.2f}\")\n        print(f\"Clarification Needed: {clarification_result['clarification_needed']}\")\n        \n        if 'resonance_validation' in clarification_result:\n            print(\"\\nResonance Validation:\")\n            for key, value in clarification_result['resonance_validation'].items():\n                print(f\"  {key}: {value}\")\n        \n        # Save results\n        # Use new config container if present\n        try:\n            output_dir = config.CONFIG.paths.outputs.as_posix()\n        except Exception:\n            output_dir = getattr(config, 'OUTPUT_DIR', 'outputs')\n        os.makedirs(output_dir, exist_ok=True)\n        output_filename = os.path.join(output_dir, f\"sirc_clarification_{uuid.uuid4().hex[:8]}.json\")\n        \n        with open(output_filename, 'w', encoding='utf-8') as f:\n            json.dump(clarification_result, f, indent=2, default=str)\n        logger.info(f\"SIRC clarification result saved to {output_filename}\")\n        \n        # If clarification successful and execution ready, offer to proceed with SIRC\n        if clarification_result.get('clarity_score', 0) > 0.85:\n            print(f\"\\n✓ Objective clarity threshold met (score: {clarification_result['clarity_score']:.2f})\")\n            print(\"Ready for SIRC Phase 3: Harmonization Check\")\n            \n            # Here we would normally proceed to full SIRC processing\n            # For now, we'll just log the readiness\n            logger.info(\"Directive successfully clarified and ready for SIRC continuation\")\n        else:\n            print(f\"\\n⚠ Objective clarity below threshold (score: {clarification_result['clarity_score']:.2f})\")\n            print(\"Additional clarification may be needed before SIRC processing\")\n            \n    except Exception as e:\n        logger.error(f\"Error processing SIRC directive: {e}\", exc_info=True)\n        print(f\"ERROR: Failed to process SIRC directive: {e}\")\n\ndef handle_run_workflow(args):\n    \"\"\"Handler for the 'run-workflow' command.\"\"\"\n    logger.info(f\"--- Received command to run workflow: {args.workflow_name} ---\")\n    initial_context = {}\n    if args.context_file:\n        try:\n            with open(args.context_file, 'r') as f:\n                initial_context = json.load(f)\n            logger.info(f\"Loaded initial context from: {args.context_file}\")\n        except (FileNotFoundError, json.JSONDecodeError) as e:\n            logger.error(f\"Error loading context file {args.context_file}: {e}\")\n            # Decide if you want to exit or run with an empty context\n            return \n\n    try:\n        engine = IARCompliantWorkflowEngine()\n\n        # Construct the full path to the workflow file\n        workflow_dir = getattr(config, 'WORKFLOW_DIR', 'workflows')\n        workflow_path = os.path.join(workflow_dir, args.workflow_name)\n        \n        final_context = engine.run_workflow(workflow_path, initial_context)\n        \n        # Determine the final status for logging\n        final_status = final_context.get(\"workflow_status\", \"Unknown\")\n\n        # Save the final context to a file\n        try:\n            output_dir = config.CONFIG.paths.outputs.as_posix()\n        except Exception:\n            output_dir = getattr(config, 'OUTPUT_DIR', 'outputs')\n        os.makedirs(output_dir, exist_ok=True)\n        \n        # Sanitize workflow name for the filename and add run_id\n        sanitized_name = os.path.splitext(os.path.basename(args.workflow_name))[0].replace(\" \", \"_\")\n        run_id = final_context.get('workflow_run_id', 'no_run_id')\n        output_filename = os.path.join(output_dir, f\"result_{sanitized_name}_{run_id}.json\")\n\n        try:\n            with open(output_filename, 'w', encoding='utf-8') as f:\n                # Use default=str to handle potential non-serializable types gracefully (e.g., numpy types)\n                json.dump(final_context, f, indent=2, default=str)\n            logger.info(f\"Final result saved successfully to {output_filename}\")\n        except TypeError as json_err:\n            # Handle cases where the result dictionary contains objects JSON can't serialize directly\n            logger.error(f\"Could not serialize final result to JSON: {json_err}. Result likely contains non-standard objects (e.g., complex numbers, custom classes). Saving string representation as fallback.\", exc_info=True)\n            fallback_filename = output_filename.replace('.json', '_error_repr.txt')\n            try:\n                with open(fallback_filename, 'w', encoding='utf-8') as f:\n                    f.write(f\"Original JSON serialization error: {json_err}\\n\\n\")\n                    f.write(\"--- Full Result (repr) ---\\n\")\n                    f.write(repr(final_context)) # Write the Python representation\n                logger.info(f\"String representation saved to {fallback_filename}\")\n            except Exception as write_err:\n                logger.error(f\"Could not write fallback string representation: {write_err}\")\n        except IOError as io_err:\n            logger.error(f\"Could not write final result to {output_filename}: {io_err}\")\n        except Exception as save_err:\n            logger.error(f\"Unexpected error saving final result: {save_err}\", exc_info=True)\n\n        # --- Print Summary to Console ---\n        # Provides a quick overview of the execution outcome\n        print(\"\\n--- Workflow Final Result Summary (v3.0) ---\")\n        try:\n            summary = {}\n            summary['workflow_name'] = args.workflow_name\n            summary['workflow_run_id'] = run_id\n            summary['overall_status'] = final_status\n            summary['run_duration_sec'] = final_context.get('workflow_run_duration_sec', 'N/A')\n\n            # Summarize status and IAR reflection highlights for each task\n            task_statuses = final_context.get('task_statuses', {})\n            summary['task_summary'] = {}\n            for task_id, status in task_statuses.items():\n                task_result = final_context.get(task_id, {})\n                reflection = task_result.get('reflection', {}) if isinstance(task_result, dict) else {}\n                \n                inputs_preview = {}\n                if isinstance(task_result, dict) and 'resolved_inputs' in task_result and isinstance(task_result['resolved_inputs'], dict):\n                    for k, v in task_result['resolved_inputs'].items():\n                        inputs_preview[k] = truncate_value(v)\n                \n                outputs_preview = {}\n                if isinstance(task_result, dict):\n                    # Exclude known meta-keys and reflection from outputs preview\n                    excluded_output_keys = ['reflection', 'resolved_inputs', 'error', 'status', 'start_time', 'end_time', 'duration_sec', 'attempt_number']\n                    for k, v in task_result.items():\n                        if k not in excluded_output_keys:\n                            outp",
    "compression_ratio": 2.0,
    "symbol_count": 13639,
    "timestamp": "2025-11-18T11:00:33.502569Z"
  },
  {
    "stage_name": "Nano",
    "content": "TERM: Function: ensure_directories D: Creates necessary directories defined in config.py if they don't exist. BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Æ/main.py, type: python_function FULL I CODE (main.py): ```python # --- START OF FILE Three_PointO_Æ/main.py --- # ResonantiA P v3.0 - main.py # Example entry point demonstrating initialization execution of Æ S. # Handles workflow execution via ΦCompliantWorkflowEngine manages Φ-inclusive results. import logging import os import json import argparse import sys import time import uuid # unique workflow run IDs typing import Optional, Dict, Any, Union # Added type hinting clarity # --- Helper function to truncate values summary --- def truncate_value(value: Any, max_len: int = 70) -> str: \"\"\"Converts a value to string truncates it if too long.\"\"\" try: s_value = str(value) if len(s_value) > max_len: return s_value[:max_len-3] + \"...\" return s_value except Exception: return \"[Unrepresentable Value]\" # --- End helper function --- # Setup logging FIRST using centralized configuration try: # Assumes config logging_config in same package directory . import config # Use relative import within package .logging_config import setup_logging setup_logging() # Initialize logging based on config settings except ImportError as cfg_imp_err: # Basic fallback logging if config files missing during setup logging.basicConfig(level=logging.INFO, F='%(asctime)s - %(name)s - %(levelname)s - %(message)s', stream=sys.stdout) logging.warning(f\"Could import config/logging_config via relative import: {cfg_imp_err}. Using basic stdout logging.\", exc_info=True) except Exception as log_setup_e: logging.basicConfig(level=logging.INFO, F='%(asctime)s - %(name)s - %(levelname)s - %(message)s', stream=sys.stdout) logging.error(f\"Error setting up logging logging_config.py: {log_setup_e}. Using basic config.\", exc_info=True) # Now import other core ResonantiA modules AFTER logging is configured try: .workflow_engine import ΦCompliantWorkflowEngine as ΦCompliantWorkflowEngine .Θ_manager import ΘManager .SIRC_intake_handler import SIRCIntakeHandler # config already imported above except ImportError as import_err: logging.critical(f\"Failed to import core ResonantiA modules (ΦCompliantWorkflowEngine, ΘManager, SIRCIntakeHandler): {import_err}. Check installation paths.\", exc_info=True) sys.exit(1) # Critical failure if core components cannot be imported logger = logging.getLogger(__name__) # Get logger specifically module def ensure_directories(): \"\"\"Creates necessary directories defined in config.py if they don't exist.\"\"\" # Fetches paths config module dirs_to_check = [ getattr(config, 'LOG_DIR', 'logs'), getattr(config, 'OUTPUT_DIR', 'outputs'), getattr(config, 'WORKFLOW_DIR', 'workflows'), getattr(config, 'KnOWLEDGE_GRAPH_DIR', 'KnOwledge_graph'), getattr(config, 'MODEL_SAVE_DIR', 'outputs/models') # Includes subdirectory models ] logger.info(f\"Ensuring base directories exist: {dirs_to_check}\") d in dirs_to_check: if d isinstance(d, str): # Check if path is valid string try: os.makedirs(d, exist_ok=True) # exist_ok=True prevents error if dir exists except OSError as e: # Log critical error raise to halt execution if essential dirs 't be made logger.critical(f\"CRITICAL: Failed to create directory {d}: {e}. Check permissions.\", exc_info=True) raise else: logger.warning(f\"Skipping invalid directory path configured: {d}\") # Specifically ensure Θ Ds file exists, creating an empty list if Θ_file = getattr(config, 'Θ_JSON_FILE', None) if Θ_file isinstance(Θ_file, str): if os.path.exists(Θ_file): try: Θ_dir = os.path.dirname(Θ_file) if Θ_dir: os.makedirs(Θ_dir, exist_ok=True) open(Θ_file, 'w', encoding='utf-8') as f: json.dump([], f) # Create file an empty JSON list logger.info(f\"Created empty Θ Ds file at {Θ_file}\") except IOError as e: logger.error(f\"Could create empty Θ file at {Θ_file}: {e}\") except Exception as e: logger.error(f\"Unexpected error ensuring Θ file exists: {e}\", exc_info=True) else: logger.warning(\"Θ_JSON_FILE configured or invalid in config.py.\") def find_last_successful_run_id(output_dir: str) -> Optional[str]: \"\"\"Finds most recent successfully completed workflow run ID.\"\"\" try: result_files = [f f in os.listdir(output_dir) if f.startswith(\"result_\") f.endswith(\".json\")] result_files.sort(key=lambda f: os.path.getmtime(os.path.join(output_dir, f)), reverse=True) filename in result_files: filepath = os.path.join(output_dir, filename) open(filepath, 'r') as f: result_data = json.load(f) if result_data.get(\"workflow_status\") == \"Completed Successfully\": run_id = result_data.get(\"workflow_run_id\") if run_id: logger.info(f\"Found last successful run_id: {run_id} file {filename}\") return run_id except Exception as e: logger.error(f\"Could determine last successful run ID: {e}\", exc_info=True) return None def handle_SIRC_directive(args): \"\"\"Handler SIRC directive Ping DirectiveClarificationP.\"\"\" logger.info(f\"--- Ping SIRC directive: {args.directive[:100]}... ---\") try: # Initialize Θ Manager SIRC handler Θ_manager = ΘManager(getattr(config, 'Θ_JSON_FILE', None)) # Initialize SIRC Intake Handler SIRC_handler = SIRCIntakeHandler(Θ_manager=Θ_manager) # P directive through clarification P clarification_result = SIRC_handler.P_directive(args.directive) # Display results print(\"\\n=== SIRC DIRECTIVE CLARIFICATION RESULT ===\") print(f\"Original Directive: {clarification_result['original_directive']}\") print(f\"Finalized Objective: {clarification_result['finalized_objective']}\") print(f\"Clarity Score: {clarification_result['clarity_score']:.2f}\") print(f\"Clarification Needed: {clarification_result['clarification_needed']}\") if 'resonance_validation' in clarification_result: print(\"\\nResonance Validation:\") key, value in clarification_result['resonance_validation'].items(): print(f\" {key}: {value}\") # Save results # Use new config container if present try: output_dir = config.CONFIG.paths.outputs.as_posix() except Exception: output_dir = getattr(config, 'OUTPUT_DIR', 'outputs') os.makedirs(output_dir, exist_ok=True) output_filename = os.path.join(output_dir, f\"SIRC_clarification_{uuid.uuid4().hex[:8]}.json\") open(output_filename, 'w', encoding='utf-8') as f: json.dump(clarification_result, f, indent=2, default=str) logger.info(f\"SIRC clarification result saved to {output_filename}\") # If clarification successful execution ready, offer to proceed SIRC if clarification_result.get('clarity_score', 0) > 0.85: print(f\"\\n✓ Objective clarity threshold met (score: {clarification_result['clarity_score']:.2f})\") print(\"Ready SIRC Phase 3: Harmonization Check\") # Here we would normally proceed to full SIRC Ping # now, we'll just log readiness logger.info(\"Directive successfully clarified ready SIRC continuation\") else: print(f\"\\n⚠ Objective clarity below threshold (score: {clarification_result['clarity_score']:.2f})\") print(\"Additional clarification may be needed before SIRC Ping\") except Exception as e: logger.error(f\"Error Ping SIRC directive: {e}\", exc_info=True) print(f\"ERROR: Failed to P SIRC directive: {e}\") def handle_run_workflow(args): \"\"\"Handler 'run-workflow' command.\"\"\" logger.info(f\"--- Received command to run workflow: {args.workflow_name} ---\") initial_context = {} if args.context_file: try: open(args.context_file, 'r') as f: initial_context = json.load(f) logger.info(f\"Loaded initial context : {args.context_file}\") except (FileNotFoundError, json.JSONDecodeError) as e: logger.error(f\"Error loading context file {args.context_file}: {e}\") # Decide if want to exit or run an empty context return try: engine = ΦCompliantWorkflowEngine() # Construct full path to workflow file workflow_dir = getattr(config, 'WORKFLOW_DIR', 'workflows') workflow_path = os.path.join(workflow_dir, args.workflow_name) final_context = engine.run_workflow(workflow_path, initial_context) # Determine final status logging final_status = final_context.get(\"workflow_status\", \"UnKnOwn\") # Save final context to a file try: output_dir = config.CONFIG.paths.outputs.as_posix() except Exception: output_dir = getattr(config, 'OUTPUT_DIR', 'outputs') os.makedirs(output_dir, exist_ok=True) # Sanitize workflow name filename add run_id sanitized_name = os.path.splitext(os.path.basename(args.workflow_name))[0].replace(\" \", \"_\") run_id = final_context.get('workflow_run_id', 'no_run_id') output_filename = os.path.join(output_dir, f\"result_{sanitized_name}_{run_id}.json\") try: open(output_filename, 'w', encoding='utf-8') as f: # Use default=str to handle potential non-serializable types gracefully (e.g., numpy types) json.dump(final_context, f, indent=2, default=str) logger.info(f\"Final result saved successfully to {output_filename}\") except TypeError as json_err: # Handle cases result dictionary contains objects JSON 't serialize directly logger.error(f\"Could serialize final result to JSON: {json_err}. Result likely contains non-standard objects (e.g., complex numbers, custom classes). Saving string representation as fallback.\", exc_info=True) fallback_filename = output_filename.replace('.json', '_error_repr.txt') try: open(fallback_filename, 'w', encoding='utf-8') as f: f.write(f\"Original JSON serialization error: {json_err}\\n\\n\") f.write(\"--- Full Result (repr) ---\\n\") f.write(repr(final_context)) # Write Python representation logger.info(f\"String representation saved to {fallback_filename}\") except Exception as write_err: logger.error(f\"Could write fallback string representation: {write_err}\") except IOError as io_err: logger.error(f\"Could write final result to {output_filename}: {io_err}\") except Exception as save_err: logger.error(f\"Unexpected error saving final result: {save_err}\", exc_info=True) # --- Print Summary to Console --- # Provides a quick overview of execution outcome print(\"\\n--- Workflow Final Result Summary (v3.0) ---\") try: summary = {} summary['workflow_name'] = args.workflow_name summary['workflow_run_id'] = run_id summary['overall_status'] = final_status summary['run_duration_sec'] = final_context.get('workflow_run_duration_sec', 'N/A') # Summarize status Φ reflection highlights each task task_statuses = final_context.get('task_statuses', {}) summary['task_summary'] = {} task_id, status in task_statuses.items(): task_result = final_context.get(task_id, {}) reflection = task_result.get('reflection', {}) if isinstance(task_result, dict) else {} inputs_preview = {} if isinstance(task_result, dict) 'resolved_inputs' in task_result isinstance(task_result['resolved_inputs'], dict): k, v in task_result['resolved_inputs'].items(): inputs_preview[k] = truncate_value(v) outputs_preview = {} if isinstance(task_result, dict): # Exclude KnOwn meta-keys reflection outputs preview excluded_output_keys = ['reflection', 'resolved_inputs', 'error', 'status', 'start_time', 'end_time', 'duration_sec', 'attempt_number'] k, v in task_result.items(): if k in excluded_output_keys: outp",
    "compression_ratio": 2.5067083256754272,
    "symbol_count": 10882,
    "timestamp": "2025-11-18T11:00:33.772227Z"
  },
  {
    "stage_name": "Micro",
    "content": "TERM: Function: ensure_directories D: Creates necessary directories defined config.py don't exist. BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Æ/main.py, type: python_function FULL I CODE (main.py): ```python START OF FILE Three_PointO_Æ/main.py ResonantiA P main.py Example entry point demonstrating initialization execution Æ S. Handles workflow execution ΦCompliantWorkflowEngine manages Φ-inclusive results. import logging import import import argparse import import import unique workflow IDs typing import Optional, Dict, Any, Union Added hinting clarity Helper function truncate values summary truncate_value(value: Any, max_len: \"\"\"Converts value string truncates long.\"\"\" s_value str(value) len(s_value) max_len: return s_value[:max_len-3] \"...\" return s_value except Exception: return \"[Unrepresentable Value]\" End helper function Setup logging FIRST using centralized configuration Assumes config logging_config package directory import config Use relative import within package .logging_config import setup_logging setup_logging() Initialize logging ABM config settings except ImportError cfg_imp_err: Basic fallback logging config files missing during setup logging.basicConfig(level=logging.INFO, F='%(asctime)s %(name)s %(levelname)s %(message)s', stream=sys.stdout) logging.warning(f\" import config/logging_config relative import: {cfg_imp_err}. Using basic stdout logging.\", exc_info=True) except Exception log_setup_e: logging.basicConfig(level=logging.INFO, F='%(asctime)s %(name)s %(levelname)s %(message)s', stream=sys.stdout) logging.error(f\"Error setting logging logging_config.py: {log_setup_e}. Using basic config.\", exc_info=True) Now import other ResonantiA modules AFTER logging configured .workflow_engine import ΦCompliantWorkflowEngine ΦCompliantWorkflowEngine .Θ_manager import ΘManager .SIRC_intake_handler import SIRCIntakeHandler config already imported above except ImportError import_err: logging.critical(f\"Failed import ResonantiA modules (ΦCompliantWorkflowEngine, ΘManager, SIRCIntakeHandler): {import_err}. Check installation paths.\", exc_info=True) sys.exit(1) Critical failure components cannot imported logger logging.getLogger(__name__) Get logger specifically module ensure_directories(): \"\"\"Creates necessary directories defined config.py don't exist.\"\"\" Fetches paths config module dirs_to_check getattr(config, 'LOG_DIR', 'logs'), getattr(config, 'OUTPUT_DIR', 'outputs'), getattr(config, 'WORKFLOW_DIR', 'workflows'), getattr(config, 'KnOWLEDGE_GRAPH_DIR', 'KnOwledge_graph'), getattr(config, 'MODEL_SAVE_DIR', 'outputs/models') Includes subdirectory models logger.info(f\"Ensuring directories exist: {dirs_to_check}\") dirs_to_check: isinstance(d, str): Check valid string os.makedirs(d, exist_ok=True) exist_ok=True prevents error exists except OSError Log critical error raise execution essential logger.critical(f\"CRITICAL: Failed create directory Check permissions.\", exc_info=True) raise else: logger.warning(f\"Skipping invalid directory configured: {d}\") Specifically ensure Θ Ds exists, creating empty Θ_file getattr(config, 'Θ_JSON_FILE', None) Θ_file isinstance(Θ_file, str): os.path.exists(Θ_file): Θ_dir os.path.dirname(Θ_file) Θ_dir: os.makedirs(Θ_dir, exist_ok=True) open(Θ_file, encoding='utf-8') json.dump([], Create empty JSON logger.info(f\"Created empty Θ Ds {Θ_file}\") except IOError logger.error(f\" create empty Θ {Θ_file}: {e}\") except Exception logger.error(f\"Unexpected error ensuring Θ exists: {e}\", exc_info=True) else: logger.warning(\"Θ_JSON_FILE configured invalid config.py.\") find_last_successful_run_id(output_dir: Optional[str]: \"\"\"Finds recent successfully completed workflow ID.\"\"\" result_files os.listdir(output_dir) f.startswith(\"result_\") f.endswith(\".json\")] result_files.sort(key=lambda os.path.getmtime(os.path.join(output_dir, reverse=True) filename result_files: filepath os.path.join(output_dir, filename) open(filepath, result_data json.load(f) result_data.get(\"workflow_status\") \"Completed Successfully\": run_id result_data.get(\"workflow_run_id\") run_id: logger.info(f\"Found successful run_id: {run_id} {filename}\") return run_id except Exception logger.error(f\" determine successful ID: {e}\", exc_info=True) return None handle_SIRC_directive(args): \"\"\"Handler SIRC directive Ping DirectiveClarificationP.\"\"\" logger.info(f\"--- Ping SIRC directive: {args.directive[:100]}... ---\") Initialize Θ Manager SIRC handler Θ_manager ΘManager(getattr(config, 'Θ_JSON_FILE', None)) Initialize SIRC Intake Handler SIRC_handler SIRCIntakeHandler(Θ_manager=Θ_manager) P directive through clarification P clarification_result SIRC_handler.P_directive(args.directive) Display results print(\"\\n=== SIRC DIRECTIVE CLARIFICATION RESULT ===\") print(f\"Original Directive: {clarification_result['original_directive']}\") print(f\"Finalized Objective: {clarification_result['finalized_objective']}\") print(f\"Clarity Score: {clarification_result['clarity_score']:.2f}\") print(f\"Clarification Needed: {clarification_result['clarification_needed']}\") 'resonance_validation' clarification_result: print(\"\\nResonance Validation:\") value clarification_result['resonance_validation'].items(): print(f\" {key}: {value}\") Save results Use config container present output_dir config.CONFIG.paths.outputs.as_posix() except Exception: output_dir getattr(config, 'OUTPUT_DIR', 'outputs') os.makedirs(output_dir, exist_ok=True) output_filename os.path.join(output_dir, f\"SIRC_clarification_{uuid.uuid4().hex[:8]}.json\") open(output_filename, encoding='utf-8') json.dump(clarification_result, indent=2, default=str) logger.info(f\"SIRC clarification result saved {output_filename}\") If clarification successful execution ready, offer proceed SIRC clarification_result.get('clarity_score', 0.85: print(f\"\\n✓ Objective clarity threshold (score: {clarification_result['clarity_score']:.2f})\") print(\"Ready SIRC Phase Harmonization Check\") Here normally proceed SIRC Ping we'll readiness logger.info(\"Directive successfully clarified ready SIRC continuation\") else: print(f\"\\n⚠ Objective clarity Λ threshold (score: {clarification_result['clarity_score']:.2f})\") print(\"Additional clarification needed before SIRC Ping\") except Exception logger.error(f\"Error Ping SIRC directive: {e}\", exc_info=True) print(f\"ERROR: Failed P SIRC directive: {e}\") handle_run_workflow(args): \"\"\"Handler 'run-workflow' command.\"\"\" logger.info(f\"--- Received command workflow: {args.workflow_name} ---\") initial_context args.context_file: open(args.context_file, initial_context json.load(f) logger.info(f\"Loaded initial context {args.context_file}\") except (FileNotFoundError, json.JSONDecodeError) logger.error(f\"Error loading context {args.context_file}: {e}\") Decide empty context return engine ΦCompliantWorkflowEngine() Construct workflow workflow_dir getattr(config, 'WORKFLOW_DIR', 'workflows') workflow_path os.path.join(workflow_dir, args.workflow_name) final_context engine.run_workflow(workflow_path, initial_context) Determine final status logging final_status final_context.get(\"workflow_status\", \"UnKnOwn\") Save final context output_dir config.CONFIG.paths.outputs.as_posix() except Exception: output_dir getattr(config, 'OUTPUT_DIR', 'outputs') os.makedirs(output_dir, exist_ok=True) Sanitize workflow filename run_id sanitized_name os.path.splitext(os.path.basename(args.workflow_name))[0].replace(\" run_id final_context.get('workflow_run_id', 'no_run_id') output_filename os.path.join(output_dir, f\"result_{sanitized_name}_{run_id}.json\") open(output_filename, encoding='utf-8') Use default=str handle potential non-serializable types gracefully (e.g., numpy types) json.dump(final_context, indent=2, default=str) logger.info(f\"Final result saved successfully {output_filename}\") except TypeError json_err: Handle cases result dictionary contains objects JSON serialize directly logger.error(f\" serialize final result JSON: {json_err}. Result likely contains non-standard objects (e.g., complex numbers, custom classes). Saving string representation fallback.\", exc_info=True) fallback_filename output_filename.replace('.json', '_error_repr.txt') open(fallback_filename, encoding='utf-8') f.write(f\"Original JSON serialization error: {json_err}\\n\\n\") f.write(\"--- Full Result (repr) ---\\n\") f.write(repr(final_context)) Write Python representation logger.info(f\"String representation saved {fallback_filename}\") except Exception write_err: logger.error(f\" write fallback string representation: {write_err}\") except IOError io_err: logger.error(f\" write final result {output_filename}: {io_err}\") except Exception save_err: logger.error(f\"Unexpected error saving final result: {save_err}\", exc_info=True) Print Summary Console Provides quick overview execution outcome print(\"\\n--- Workflow Final Result Summary (v3.0) ---\") summary summary['workflow_name'] args.workflow_name summary['workflow_run_id'] run_id summary['overall_status'] final_status summary['run_duration_sec'] final_context.get('workflow_run_duration_sec', 'N/A') Summarize status Φ CRC highlights task_statuses final_context.get('task_statuses', summary['task_summary'] task_id, status task_statuses.items(): task_result final_context.get(task_id, CRC task_result.get('CRC', isinstance(task_result, dict) inputs_preview isinstance(task_result, dict) 'resolved_inputs' task_result isinstance(task_result['resolved_inputs'], dict): task_result['resolved_inputs'].items(): inputs_preview[k] truncate_value(v) outputs_preview isinstance(task_result, dict): Exclude KnOwn meta-keys CRC outputs preview excluded_output_keys ['CRC', 'resolved_inputs', 'error', 'status', 'start_time', 'end_time', 'duration_sec', 'attempt_number'] task_result.items(): excluded_output_keys:",
    "compression_ratio": 2.8101370145256,
    "symbol_count": 9707,
    "timestamp": "2025-11-18T11:00:34.236703Z"
  },
  {
    "stage_name": "Pico",
    "content": "TERM: Function: ensure_directories D: Creates necessary directories defined config.py don't exist. BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Æ/main.py, type: python_function FULL I CODE (main.py): ```python START OF FILE Three_PointO_Æ/main.py ResonantiA P main.py Example entry point demonstrating initialization execution Æ S. Handles workflow execution ΦCompliantWorkflowEngine manages Φ-inclusive results. import logging import import import argparse import import import unique workflow IDs typing import Optional, Dict, Any, Union Added hinting clarity Helper function truncate values summary truncate_value(value: Any, max_len: \"\"\"Converts value string truncates long.\"\"\" s_value str(value) len(s_value) max_len: return s_value[:max_len-3] \"...\" return s_value except Exception: return \"[Unrepresentable Value]\" End helper function Setup logging FIRST using centralized configuration Assumes config logging_config package directory import config Use relative import within package .logging_config import setup_logging setup_logging() Initialize logging ABM config settings except ImportError cfg_imp_err: Basic fallback logging config files missing during setup logging.basicConfig(level=logging.INFO, F='%(asctime)s %(name)s %(levelname)s %(message)s', stream=sys.stdout) logging.warning(f\" import config/logging_config relative import: {cfg_imp_err}. Using basic stdout logging.\", exc_info=True) except Exception log_setup_e: logging.basicConfig(level=logging.INFO, F='%(asctime)s %(name)s %(levelname)s %(message)s', stream=sys.stdout) logging.error(f\"Error setting logging logging_config.py: {log_setup_e}. Using basic config.\", exc_info=True) Now import other ResonantiA modules AFTER logging configured .workflow_engine import ΦCompliantWorkflowEngine ΦCompliantWorkflowEngine .Θ_manager import ΘManager .SIRC_intake_handler import SIRCIntakeHandler config already imported above except ImportError import_err: logging.critical(f\"Failed import ResonantiA modules (ΦCompliantWorkflowEngine, ΘManager, SIRCIntakeHandler): {import_err}. Check installation paths.\", exc_info=True) sys.exit(1) Critical failure components cannot imported logger logging.getLogger(__name__) Get logger specifically module ensure_directories(): \"\"\"Creates necessary directories defined config.py don't exist.\"\"\" Fetches paths config module dirs_to_check getattr(config, 'LOG_DIR', 'logs'), getattr(config, 'OUTPUT_DIR', 'outputs'), getattr(config, 'WORKFLOW_DIR', 'workflows'), getattr(config, 'KnOWLEDGE_GRAPH_DIR', 'KnOwledge_graph'), getattr(config, 'MODEL_SAVE_DIR', 'outputs/models') Includes subdirectory models logger.info(f\"Ensuring directories exist: {dirs_to_check}\") dirs_to_check: isinstance(d, str): Check valid string os.makedirs(d, exist_ok=True) exist_ok=True prevents error exists except OSError Log critical error raise execution essential logger.critical(f\"CRITICAL: Failed create directory Check permissions.\", exc_info=True) raise else: logger.warning(f\"Skipping invalid directory configured: {d}\") Specifically ensure Θ Ds exists, creating empty Θ_file getattr(config, 'Θ_JSON_FILE', None) Θ_file isinstance(Θ_file, str): os.path.exists(Θ_file): Θ_dir os.path.dirname(Θ_file) Θ_dir: os.makedirs(Θ_dir, exist_ok=True) open(Θ_file, encoding='utf-8') json.dump([], Create empty JSON logger.info(f\"Created empty Θ Ds {Θ_file}\") except IOError logger.error(f\" create empty Θ {Θ_file}: {e}\") except Exception logger.error(f\"Unexpected error ensuring Θ exists: {e}\", exc_info=True) else: logger.warning(\"Θ_JSON_FILE configured invalid config.py.\") find_last_successful_run_id(output_dir: Optional[str]: \"\"\"Finds recent successfully completed workflow ID.\"\"\" result_files os.listdir(output_dir) f.startswith(\"result_\") f.endswith(\".json\")] result_files.sort(key=lambda os.path.getmtime(os.path.join(output_dir, reverse=True) filename result_files: filepath os.path.join(output_dir, filename) open(filepath, result_data json.load(f) result_data.get(\"workflow_status\") \"Completed Successfully\": run_id result_data.get(\"workflow_run_id\") run_id: logger.info(f\"Found successful run_id: {run_id} {filename}\") return run_id except Exception logger.error(f\" determine successful ID: {e}\", exc_info=True) return None handle_SIRC_directive(args): \"\"\"Handler SIRC directive Ping DirectiveClarificationP.\"\"\" logger.info(f\"--- Ping SIRC directive: {args.directive[:100]}... ---\") Initialize Θ Manager SIRC handler Θ_manager ΘManager(getattr(config, 'Θ_JSON_FILE', None)) Initialize SIRC Intake Handler SIRC_handler SIRCIntakeHandler(Θ_manager=Θ_manager) P directive through clarification P clarification_result SIRC_handler.P_directive(args.directive) Display results print(\"\\n=== SIRC DIRECTIVE CLARIFICATION RESULT ===\") print(f\"Original Directive: {clarification_result['original_directive']}\") print(f\"Finalized Objective: {clarification_result['finalized_objective']}\") print(f\"Clarity Score: {clarification_result['clarity_score']:.2f}\") print(f\"Clarification Needed: {clarification_result['clarification_needed']}\") 'resonance_validation' clarification_result: print(\"\\nResonance Validation:\") value clarification_result['resonance_validation'].items(): print(f\" {key}: {value}\") Save results Use config container present output_dir config.CONFIG.paths.outputs.as_posix() except Exception: output_dir getattr(config, 'OUTPUT_DIR', 'outputs') os.makedirs(output_dir, exist_ok=True) output_filename os.path.join(output_dir, f\"SIRC_clarification_{uuid.uuid4().hex[:8]}.json\") open(output_filename, encoding='utf-8') json.dump(clarification_result, indent=2, default=str) logger.info(f\"SIRC clarification result saved {output_filename}\") If clarification successful execution ready, offer proceed SIRC clarification_result.get('clarity_score', 0.85: print(f\"\\n✓ Objective clarity threshold (score: {clarification_result['clarity_score']:.2f})\") print(\"Ready SIRC Phase Harmonization Check\") Here normally proceed SIRC Ping we'll readiness logger.info(\"Directive successfully clarified ready SIRC continuation\") else: print(f\"\\n⚠ Objective clarity Λ threshold (score: {clarification_result['clarity_score']:.2f})\") print(\"Additional clarification needed before SIRC Ping\") except Exception logger.error(f\"Error Ping SIRC directive: {e}\", exc_info=True) print(f\"ERROR: Failed P SIRC directive: {e}\") handle_run_workflow(args): \"\"\"Handler 'run-workflow' command.\"\"\" logger.info(f\"--- Received command workflow: {args.workflow_name} ---\") initial_context args.context_file: open(args.context_file, initial_context json.load(f) logger.info(f\"Loaded initial context {args.context_file}\") except (FileNotFoundError, json.JSONDecodeError) logger.error(f\"Error loading context {args.context_file}: {e}\") Decide empty context return engine ΦCompliantWorkflowEngine() Construct workflow workflow_dir getattr(config, 'WORKFLOW_DIR', 'workflows') workflow_path os.path.join(workflow_dir, args.workflow_name) final_context engine.run_workflow(workflow_path, initial_context) Determine final status logging final_status final_context.get(\"workflow_status\", \"UnKnOwn\") Save final context output_dir config.CONFIG.paths.outputs.as_posix() except Exception: output_dir getattr(config, 'OUTPUT_DIR', 'outputs') os.makedirs(output_dir, exist_ok=True) Sanitize workflow filename run_id sanitized_name os.path.splitext(os.path.basename(args.workflow_name))[0].replace(\" run_id final_context.get('workflow_run_id', 'no_run_id') output_filename os.path.join(output_dir, f\"result_{sanitized_name}_{run_id}.json\") open(output_filename, encoding='utf-8') Use default=str handle potential non-serializable types gracefully (e.g., numpy types) json.dump(final_context, indent=2, default=str) logger.info(f\"Final result saved successfully {output_filename}\") except TypeError json_err: Handle cases result dictionary contains objects JSON serialize directly logger.error(f\" serialize final result JSON: {json_err}. Result likely contains non-standard objects (e.g., complex numbers, custom classes). Saving string representation fallback.\", exc_info=True) fallback_filename output_filename.replace('.json', '_error_repr.txt') open(fallback_filename, encoding='utf-8') f.write(f\"Original JSON serialization error: {json_err}\\n\\n\") f.write(\"--- Full Result (repr) ---\\n\") f.write(repr(final_context)) Write Python representation logger.info(f\"String representation saved {fallback_filename}\") except Exception write_err: logger.error(f\" write fallback string representation: {write_err}\") except IOError io_err: logger.error(f\" write final result {output_filename}: {io_err}\") except Exception save_err: logger.error(f\"Unexpected error saving final result: {save_err}\", exc_info=True) Print Summary Console Provides quick overview execution outcome print(\"\\n--- Workflow Final Result Summary (v3.0) ---\") summary summary['workflow_name'] args.workflow_name summary['workflow_run_id'] run_id summary['overall_status'] final_status summary['run_duration_sec'] final_context.get('workflow_run_duration_sec', 'N/A') Summarize status Φ CRC highlights task_statuses final_context.get('task_statuses', summary['task_summary'] task_id, status task_statuses.items(): task_result final_context.get(task_id, CRC task_result.get('CRC', isinstance(task_result, dict) inputs_preview isinstance(task_result, dict) 'resolved_inputs' task_result isinstance(task_result['resolved_inputs'], dict): task_result['resolved_inputs'].items(): inputs_preview[k] truncate_value(v) outputs_preview isinstance(task_result, dict): Exclude KnOwn meta-keys CRC outputs preview excluded_output_keys ['CRC', 'resolved_inputs', 'error', 'status', 'start_time', 'end_time', 'duration_sec', 'attempt_number'] task_result.items(): excluded_output_keys:",
    "compression_ratio": 2.8101370145256,
    "symbol_count": 9707,
    "timestamp": "2025-11-18T11:00:34.580142Z"
  },
  {
    "stage_name": "Femto",
    "content": "TERM: Function: ensure_directories D: Creates necessary directories defined config.py don't exist. BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Æ/main.py, type: python_function FULL I CODE (main.py): ```python START FILE Three_PointO_Æ/main.py ResonantiA P main.py Example entry point demonstrating initialization execution Æ S. Handles workflow execution ΦCompliantWorkflowEngine manages Φ-inclusive results. import logging import import import argparse import import import unique workflow IDs typing import Optional, Dict, Any, Union Added hinting clarity Helper function truncate values summary truncate_value(value: Any, max_len: \"\"\"Converts value string truncates long.\"\"\" s_value str(value) len(s_value) max_len: return s_value[:max_len-3] \"...\" return s_value except Exception: return \"[Unrepresentable Value]\" End helper function Setup logging FIRST using centralized configuration Assumes config logging_config package directory import config Use relative import within package .logging_config import setup_logging setup_logging() Initialize logging ABM config settings except ImportError cfg_imp_err: Basic fallback logging config files missing during setup logging.basicConfig(level=logging.INFO, F='%(asctime)s %(name)s %(levelname)s %(message)s', stream=sys.stdout) logging.warning(f\" import config/logging_config relative import: {cfg_imp_err}. Using basic stdout logging.\", exc_info=True) except Exception log_setup_e: logging.basicConfig(level=logging.INFO, F='%(asctime)s %(name)s %(levelname)s %(message)s', stream=sys.stdout) logging.error(f\"Error setting logging logging_config.py: {log_setup_e}. Using basic config.\", exc_info=True) Now import other ResonantiA modules AFTER logging configured .workflow_engine import ΦCompliantWorkflowEngine ΦCompliantWorkflowEngine .Θ_manager import ΘManager .SIRC_intake_handler import SIRCIntakeHandler config already imported above except ImportError import_err: logging.critical(f\"Failed import ResonantiA modules (ΦCompliantWorkflowEngine, ΘManager, SIRCIntakeHandler): {import_err}. Check installation paths.\", exc_info=True) sys.exit(1) Critical failure components cannot imported logger logging.getLogger(__name__) Get logger specifically module ensure_directories(): \"\"\"Creates necessary directories defined config.py don't exist.\"\"\" Fetches paths config module dirs_to_check getattr(config, 'LOG_DIR', 'logs'), getattr(config, 'OUTPUT_DIR', 'outputs'), getattr(config, 'WORKFLOW_DIR', 'workflows'), getattr(config, 'KnOWLEDGE_GRAPH_DIR', 'KnOwledge_graph'), getattr(config, 'MODEL_SAVE_DIR', 'outputs/models') Includes subdirectory models logger.info(f\"Ensuring directories exist: {dirs_to_check}\") dirs_to_check: isinstance(d, str): Check valid string os.makedirs(d, exist_ok=True) exist_ok=True prevents error exists except OSError Log critical error raise execution essential logger.critical(f\"CRITICAL: Failed create directory Check permissions.\", exc_info=True) raise else: logger.warning(f\"Skipping invalid directory configured: {d}\") Specifically ensure Θ Ds exists, creating empty Θ_file getattr(config, 'Θ_JSON_FILE', None) Θ_file isinstance(Θ_file, str): os.path.exists(Θ_file): Θ_dir os.path.dirname(Θ_file) Θ_dir: os.makedirs(Θ_dir, exist_ok=True) open(Θ_file, encoding='utf-8') json.dump([], Create empty JSON logger.info(f\"Created empty Θ Ds {Θ_file}\") except IOError logger.error(f\" create empty Θ {Θ_file}: {e}\") except Exception logger.error(f\"Unexpected error ensuring Θ exists: {e}\", exc_info=True) else: logger.warning(\"Θ_JSON_FILE configured invalid config.py.\") find_last_successful_run_id(output_dir: Optional[str]: \"\"\"Finds recent successfully completed workflow ID.\"\"\" result_files os.listdir(output_dir) f.startswith(\"result_\") f.endswith(\".json\")] result_files.sort(key=lambda os.path.getmtime(os.path.join(output_dir, reverse=True) filename result_files: filepath os.path.join(output_dir, filename) open(filepath, result_data json.load(f) result_data.get(\"workflow_status\") \"Completed Successfully\": run_id result_data.get(\"workflow_run_id\") run_id: logger.info(f\"Found successful run_id: {run_id} {filename}\") return run_id except Exception logger.error(f\" determine successful ID: {e}\", exc_info=True) return None handle_SIRC_directive(args): \"\"\"Handler SIRC directive Ping DirectiveClarificationP.\"\"\" logger.info(f\"--- Ping SIRC directive: {args.directive[:100]}... ---\") Initialize Θ Manager SIRC handler Θ_manager ΘManager(getattr(config, 'Θ_JSON_FILE', None)) Initialize SIRC Intake Handler SIRC_handler SIRCIntakeHandler(Θ_manager=Θ_manager) P directive through clarification P clarification_result SIRC_handler.P_directive(args.directive) Display results print(\"\\n=== SIRC DIRECTIVE CLARIFICATION RESULT ===\") print(f\"Original Directive: {clarification_result['original_directive']}\") print(f\"Finalized Objective: {clarification_result['finalized_objective']}\") print(f\"Clarity Score: {clarification_result['clarity_score']:.2f}\") print(f\"Clarification Needed: {clarification_result['clarification_needed']}\") 'resonance_validation' clarification_result: print(\"\\nResonance Validation:\") value clarification_result['resonance_validation'].items(): print(f\" {key}: {value}\") Save results Use config container present output_dir config.CONFIG.paths.outputs.as_posix() except Exception: output_dir getattr(config, 'OUTPUT_DIR', 'outputs') os.makedirs(output_dir, exist_ok=True) output_filename os.path.join(output_dir, f\"SIRC_clarification_{uuid.uuid4().hex[:8]}.json\") open(output_filename, encoding='utf-8') json.dump(clarification_result, indent=2, default=str) logger.info(f\"SIRC clarification result saved {output_filename}\") If clarification successful execution ready, offer proceed SIRC clarification_result.get('clarity_score', 0.85: print(f\"\\n✓ Objective clarity threshold (score: {clarification_result['clarity_score']:.2f})\") print(\"Ready SIRC Phase Harmonization Check\") Here normally proceed SIRC Ping we'll readiness logger.info(\"Directive successfully clarified ready SIRC continuation\") else: print(f\"\\n⚠ Objective clarity Λ threshold (score: {clarification_result['clarity_score']:.2f})\") print(\"Additional clarification needed before SIRC Ping\") except Exception logger.error(f\"Error Ping SIRC directive: {e}\", exc_info=True) print(f\"ERROR: Failed P SIRC directive: {e}\") handle_run_workflow(args): \"\"\"Handler 'run-workflow' command.\"\"\" logger.info(f\"--- Received command workflow: {args.workflow_name} ---\") initial_context args.context_file: open(args.context_file, initial_context json.load(f) logger.info(f\"Loaded initial context {args.context_file}\") except (FileNotFoundError, json.JSONDecodeError) logger.error(f\"Error loading context {args.context_file}: {e}\") Decide empty context return engine ΦCompliantWorkflowEngine() Construct workflow workflow_dir getattr(config, 'WORKFLOW_DIR', 'workflows') workflow_path os.path.join(workflow_dir, args.workflow_name) final_context engine.run_workflow(workflow_path, initial_context) Determine final status logging final_status final_context.get(\"workflow_status\", \"UnKnOwn\") Save final context output_dir config.CONFIG.paths.outputs.as_posix() except Exception: output_dir getattr(config, 'OUTPUT_DIR', 'outputs') os.makedirs(output_dir, exist_ok=True) Sanitize workflow filename run_id sanitized_name os.path.splitext(os.path.basename(args.workflow_name))[0].replace(\" run_id final_context.get('workflow_run_id', 'no_run_id') output_filename os.path.join(output_dir, f\"result_{sanitized_name}_{run_id}.json\") open(output_filename, encoding='utf-8') Use default=str handle potential non-serializable types gracefully (e.g., numpy types) json.dump(final_context, indent=2, default=str) logger.info(f\"Final result saved successfully {output_filename}\") except TypeError json_err: Handle cases result dictionary contains objects JSON serialize directly logger.error(f\" serialize final result JSON: {json_err}. Result likely contains non-standard objects (e.g., complex numbers, custom classes). Saving string representation fallback.\", exc_info=True) fallback_filename output_filename.replace('.json', '_error_repr.txt') open(fallback_filename, encoding='utf-8') f.write(f\"Original JSON serialization error: {json_err}\\n\\n\") f.write(\"--- Full Result (repr) ---\\n\") f.write(repr(final_context)) Write Python representation logger.info(f\"String representation saved {fallback_filename}\") except Exception write_err: logger.error(f\" write fallback string representation: {write_err}\") except IOError io_err: logger.error(f\" write final result {output_filename}: {io_err}\") except Exception save_err: logger.error(f\"Unexpected error saving final result: {save_err}\", exc_info=True) Print Summary Console Provides quick overview execution outcome print(\"\\n--- Workflow Final Result Summary (v3.0) ---\") summary summary['workflow_name'] args.workflow_name summary['workflow_run_id'] run_id summary['overall_status'] final_status summary['run_duration_sec'] final_context.get('workflow_run_duration_sec', 'N/') Summarize status Φ CRC highlights task_statuses final_context.get('task_statuses', summary['task_summary'] task_id, status task_statuses.items(): task_result final_context.get(task_id, CRC task_result.get('CRC', isinstance(task_result, dict) inputs_preview isinstance(task_result, dict) 'resolved_inputs' task_result isinstance(task_result['resolved_inputs'], dict): task_result['resolved_inputs'].items(): inputs_preview[k] truncate_value(v) outputs_preview isinstance(task_result, dict): Exclude KnOwn meta-keys CRC outputs preview excluded_output_keys ['CRC', 'resolved_inputs', 'error', 'status', 'start_time', 'end_time', 'duration_sec', 'attempt_number'] task_result.items(): excluded_output_keys:",
    "compression_ratio": 2.811295475626095,
    "symbol_count": 9703,
    "timestamp": "2025-11-18T11:00:34.894784Z"
  },
  {
    "stage_name": "Atto",
    "content": "TERM: Function: D: Creates BLUEPRINT DETAILS: Extracted /mnt/3626C55326C514B1/Happier/Three_PointO_Æ/main.py, FULL I CODE START FILE Three_PointO_Æ/main.py ResonantiA P Example Æ S. Handles ΦCompliantWorkflowEngine Φ-inclusive IDs Optional, Dict, Any, Union Added Helper Any, Exception: Value]\" End Setup FIRST Assumes Use Initialize ABM ImportError Basic F='%(asctime)s Using Exception F='%(asctime)s Using Now ResonantiA AFTER ΦCompliantWorkflowEngine ΦCompliantWorkflowEngine .Θ_manager ΘManager SIRCIntakeHandler ImportError ResonantiA (ΦCompliantWorkflowEngine, ΘManager, SIRCIntakeHandler): Check Critical Get Fetches 'LOG_DIR', 'OUTPUT_DIR', 'WORKFLOW_DIR', 'MODEL_SAVE_DIR', Includes Check OSError Log Failed Check Specifically Θ Ds Θ_file 'Θ_JSON_FILE', None) Θ_file isinstance(Θ_file, os.path.exists(Θ_file): Θ_dir os.path.dirname(Θ_file) Θ_dir: os.makedirs(Θ_dir, open(Θ_file, Create JSON Θ Ds {Θ_file}\") IOError Θ {Θ_file}: Exception Θ logger.warning(\"Θ_JSON_FILE Optional[str]: ID.\"\"\" Successfully\": Exception ID: None SIRC Ping DirectiveClarificationP.\"\"\" Ping SIRC Initialize Θ Manager SIRC Θ_manager ΘManager(getattr(config, 'Θ_JSON_FILE', None)) Initialize SIRC Intake Handler SIRC_handler SIRCIntakeHandler(Θ_manager=Θ_manager) P P SIRC_handler.P_directive(args.directive) Display SIRC DIRECTIVE CLARIFICATION RESULT Directive: Objective: Score: Needed: Validation:\") Save Use Exception: 'OUTPUT_DIR', If SIRC Objective SIRC Phase Harmonization Check\") Here SIRC Ping SIRC Objective Λ SIRC Ping\") Exception Ping SIRC Failed P SIRC Received Decide ΦCompliantWorkflowEngine() Construct 'WORKFLOW_DIR', Determine Save Exception: 'OUTPUT_DIR', Sanitize Use TypeError Handle JSON JSON: Result Saving JSON Full Result Write Python Exception IOError Exception Print Summary Console Provides Workflow Final Result Summary 'N/') Summarize Φ CRC CRC Exclude KnOwn CRC ['CRC',",
    "compression_ratio": 14.486457780138078,
    "symbol_count": 1883,
    "timestamp": "2025-11-18T11:00:35.127025Z"
  },
  {
    "stage_name": "Zepto",
    "content": "Æ|Æ|Æ|Φ|Φ",
    "compression_ratio": 3030.8888888888887,
    "symbol_count": 9,
    "timestamp": "2025-11-18T11:00:35.141605Z"
  }
]