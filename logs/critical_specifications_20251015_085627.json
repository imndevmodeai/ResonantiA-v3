[
  "## 7.X action_context.py\n\n**File Path**: `Three_PointO_ArchE/action_context.py`\n\n**Purpose**: Core system component requiring detailed specification for Genesis Protocol.\n\n**File Statistics**:\n- **Size**: 1,070 bytes\n- **Lines**: 28 lines\n- **Classes**: 1\n- **Functions**: 1\n\n**Key Components**:\n\n### Classes\n\n#### ActionContext\n- **Purpose**: Contextual information passed to an action during execution.\n- **Methods**: 1\n- **Line**: 7\n- **Key Methods**:\n  - `__post_init__(self)` - Method functionality\n\n### Functions\n\n#### __post_init__\n- **Purpose**: Function functionality\n- **Arguments**: self\n- **Line**: 26\n\n### Dependencies\n**Critical Imports**:\n- `dataclasses`\n- `typing`\n- `datetime`\n\n**IAR Compliance**: This file implements or uses IAR (Implementation Action Reflection) components for self-monitoring and validation.\n\n**Integration Points**: \n- Used by workflow engine for system operations\n- Integrated with temporal core for timestamp management\n- Connected to thought trail for logging and reflection\n\n**Implementation Notes**: \n- Critical for system operation - requires accurate regeneration\n- Contains complex logic that must be preserved exactly\n- Integration points must be maintained for system coherence\n- File size: 1,070 bytes indicates significant complexity\n\n**Regeneration Requirements**:\n1. Preserve all class structures and method signatures\n2. Maintain import dependencies exactly\n3. Preserve docstrings and comments\n4. Ensure IAR compliance is maintained\n5. Test integration points after regeneration\n\n",
  "## 7.X iar_components.py\n\n**File Path**: `Three_PointO_ArchE/iar_components.py`\n\n**Purpose**: IAR Components - IAR validation and resonance tracking for workflow execution\n\n**File Statistics**:\n- **Size**: 10,888 bytes\n- **Lines**: 306 lines\n- **Classes**: 4\n- **Functions**: 14\n\n**Key Components**:\n\n### Classes\n\n#### IARValidationResult\n- **Purpose**: Result of IAR validation.\n- **Methods**: 0\n- **Line**: 22\n\n#### IAR_Prepper\n- **Purpose**: A helper class to prepare standardized IAR dictionaries.\n- **Methods**: 5\n- **Line**: 29\n- **Key Methods**:\n  - `__init__(self, action_name, action_inputs)` - Initializes the IAR_Prepper.\n\nArgs:\n    action_name (str): The name of the action being performed.\n    action_inputs (Dict[str, Any]): The inputs provided to the action.\n  - `_get_common_fields(self)` - Returns fields common to all IAR responses.\n  - `finish_with_success(self, result, confidence, summary)` - Formats a successful IAR response.\n  - `finish_with_error(self, error_message, confidence)` - Formats a failed IAR response.\n  - `finish_with_skip(self, reason)` - Formats a skipped IAR response.\n\n#### IARValidator\n- **Purpose**: Validates IAR (Integrated Action Reflection) data structure and content.\n- **Methods**: 4\n- **Line**: 111\n- **Key Methods**:\n  - `__init__(self)` - Method functionality\n  - `validate_structure(self, iar_data)` - Validate the structure of IAR data.\n\nArgs:\n    iar_data: The IAR data to validate\n    \nReturns:\n    Tuple of (is_valid, issues)\n  - `validate_content(self, iar_data, context)` - Validate the content of IAR data in context.\n\nArgs:\n    iar_data: The IAR data to validate\n    context: The execution context\n    \nReturns:\n    IARValidationResult containing validation details\n  - `_calculate_resonance(self, iar_data, context)` - Calculate the resonance score for IAR data.\n\n#### ResonanceTracker\n- **Purpose**: Tracks resonance across workflow execution.\n- **Methods**: 5\n- **Line**: 207\n- **Key Methods**:\n  - `__init__(self)` - Method functionality\n  - `record_execution(self, task_id, iar_data, context)` - Record execution with IAR data.\n\nArgs:\n    task_id: The task identifier\n    iar_data: The IAR data\n    context: The execution context\n  - `get_resonance_trend(self, window_size)` - Get the resonance trend over recent executions.\n\nArgs:\n    window_size: Number of recent executions to consider\n    \nReturns:\n    List of resonance scores\n  - `detect_resonance_issues(self)` - Detect potential resonance issues in recent executions.\n\nReturns:\n    List of issues with details\n  - `get_execution_summary(self)` - Get a summary of execution history.\n\nReturns:\n    Dictionary containing execution summary\n\n### Functions\n\n#### __init__\n- **Purpose**: Initializes the IAR_Prepper.\n\nArgs:\n    action_name (str): The name of the action being performed.\n    action_inputs (Dict[str, Any]): The inputs provided to the action.\n- **Arguments**: self, action_name, action_inputs\n- **Line**: 31\n\n#### _get_common_fields\n- **Purpose**: Returns fields common to all IAR responses.\n- **Arguments**: self\n- **Line**: 43\n\n#### finish_with_success\n- **Purpose**: Formats a successful IAR response.\n- **Arguments**: self, result, confidence, summary\n- **Line**: 54\n\n#### finish_with_error\n- **Purpose**: Formats a failed IAR response.\n- **Arguments**: self, error_message, confidence\n- **Line**: 76\n\n#### finish_with_skip\n- **Purpose**: Formats a skipped IAR response.\n- **Arguments**: self, reason\n- **Line**: 97\n\n#### __init__\n- **Purpose**: Function functionality\n- **Arguments**: self\n- **Line**: 114\n\n#### validate_structure\n- **Purpose**: Validate the structure of IAR data.\n\nArgs:\n    iar_data: The IAR data to validate\n    \nReturns:\n    Tuple of (is_valid, issues)\n- **Arguments**: self, iar_data\n- **Line**: 123\n\n#### validate_content\n- **Purpose**: Validate the content of IAR data in context.\n\nArgs:\n    iar_data: The IAR data to validate\n    context: The execution context\n    \nReturns:\n    IARValidationResult containing validation details\n- **Arguments**: self, iar_data, context\n- **Line**: 162\n\n#### _calculate_resonance\n- **Purpose**: Calculate the resonance score for IAR data.\n- **Arguments**: self, iar_data, context\n- **Line**: 188\n\n#### __init__\n- **Purpose**: Function functionality\n- **Arguments**: self\n- **Line**: 210\n\n### Dependencies\n**Critical Imports**:\n- `typing`\n- `datetime`\n- `temporal_core`\n- `numpy`\n- `dataclasses`\n- `typing`\n- `Three_PointO_ArchE.temporal_core`\n\n**IAR Compliance**: This file implements or uses IAR (Implementation Action Reflection) components for self-monitoring and validation.\n\n**Integration Points**: \n- Used by workflow engine for system operations\n- Integrated with temporal core for timestamp management\n- Connected to thought trail for logging and reflection\n\n**Implementation Notes**: \n- Critical for system operation - requires accurate regeneration\n- Contains complex logic that must be preserved exactly\n- Integration points must be maintained for system coherence\n- File size: 10,888 bytes indicates significant complexity\n\n**Regeneration Requirements**:\n1. Preserve all class structures and method signatures\n2. Maintain import dependencies exactly\n3. Preserve docstrings and comments\n4. Ensure IAR compliance is maintained\n5. Test integration points after regeneration\n\n",
  "## 7.X rise_orchestrator.py\n\n**File Path**: `Three_PointO_ArchE/rise_orchestrator.py`\n\n**Purpose**: RISE v2.0 Genesis Protocol - RISE_Orchestrator\nMaster controller for the Resonant Insight and Strategy Engine (RISE) v2.0\n\nThis module implements the Genesis Protocol as described in the RISE v2.0 blueprint.\nIt orchestrates the three-phase workflow that can forge specialized expert clones\nand achieve unprecedented autonomous strategic reasoning.\n\nPhase A: Knowledge Scaffolding & Dynamic Specialization\nPhase B: Fused Insight Generation  \nPhase C: Fused Strategy Generation & Finalization\n\nThe RISE_Orchestrator manages the state of a problem as it moves through these phases,\ncoordinating the Metamorphosis Protocol (sandboxed expert clones) and HighStakesVetting.\n\nENHANCED WITH SYNERGISTIC FUSION PROTOCOL:\nThe orchestrator now includes the ability to activate axiomatic knowledge when scope limitations\nare detected, creating a synergistic fusion of scientific reasoning and spiritual guidance.\n\n**File Statistics**:\n- **Size**: 58,952 bytes\n- **Lines**: 1,256 lines\n- **Classes**: 2\n- **Functions**: 23\n\n**Key Components**:\n\n### Classes\n\n#### RISEState\n- **Purpose**: Represents the state of a RISE workflow execution\n- **Methods**: 1\n- **Line**: 66\n- **Key Methods**:\n  - `to_dict(self)` - Convert state to dictionary for serialization\n\n#### RISE_Orchestrator\n- **Purpose**: Master controller for the RISE v2.0 workflow.\n\nThis orchestrator manages the three-phase process:\n1. Knowledge Scaffolding & Dynamic Specialization (Phase A)\n2. Fused Insight Generation (Phase B) \n3. Fused Strategy Generation & Finalization (Phase C)\n\nIt coordinates the Metamorphosis Protocol for creating specialized expert clones\nand implements HighStakesVetting for rigorous strategy validation.\n\nENHANCED WITH SYNERGISTIC FUSION PROTOCOL:\nThe orchestrator now includes the ability to activate axiomatic knowledge when scope limitations\nare detected, creating a synergistic fusion of scientific reasoning and spiritual guidance.\n- **Methods**: 19\n- **Line**: 92\n- **Key Methods**:\n  - `__init__(self, workflows_dir, spr_manager, workflow_engine)` - Initialize the RISE_Orchestrator with proper path resolution.\n\nArgs:\n    workflows_dir: Directory containing workflow files. If None, will auto-detect.\n    spr_manager: Optional SPR manager instance\n    workflow_engine: Optional workflow engine instance\n  - `_load_axiomatic_knowledge(self)` - Load the Axiomatic Knowledge Base for Synergistic Fusion Protocol.\n\nReturns:\n    Dict containing axiomatic knowledge base\n  - `_build_spr_index(self)` - Build a lightweight index from the Knowledge Tapestry for fuzzy SPR detection.\n  - `_detect_and_normalize_sprs_in_text(self, text)` - Detect likely SPR mentions in free text (voice transcriptions, informal prompts) and produce hints.\n\nReturns: { detected: [spr_id...], normalized_text: str, map: [{match, spr_id}] }\n  - `perform_synergistic_fusion(self, rise_state, current_thought, action_inputs)` - Perform Synergistic Fusion Protocol integration.\n\nArgs:\n    rise_state: Current RISE state\n    current_thought: Current thought process\n    action_inputs: Proposed action inputs\n    \nReturns:\n    Dict containing synergistic fusion results\n\n### Functions\n\n#### to_dict\n- **Purpose**: Convert state to dictionary for serialization\n- **Arguments**: self\n- **Line**: 88\n\n#### __init__\n- **Purpose**: Initialize the RISE_Orchestrator with proper path resolution.\n\nArgs:\n    workflows_dir: Directory containing workflow files. If None, will auto-detect.\n    spr_manager: Optional SPR manager instance\n    workflow_engine: Optional workflow engine instance\n- **Arguments**: self, workflows_dir, spr_manager, workflow_engine\n- **Line**: 109\n\n#### _load_axiomatic_knowledge\n- **Purpose**: Load the Axiomatic Knowledge Base for Synergistic Fusion Protocol.\n\nReturns:\n    Dict containing axiomatic knowledge base\n- **Arguments**: self\n- **Line**: 245\n\n#### _build_spr_index\n- **Purpose**: Build a lightweight index from the Knowledge Tapestry for fuzzy SPR detection.\n- **Arguments**: self\n- **Line**: 261\n\n#### _detect_and_normalize_sprs_in_text\n- **Purpose**: Detect likely SPR mentions in free text (voice transcriptions, informal prompts) and produce hints.\n\nReturns: { detected: [spr_id...], normalized_text: str, map: [{match, spr_id}] }\n- **Arguments**: self, text\n- **Line**: 323\n\n#### perform_synergistic_fusion\n- **Purpose**: Perform Synergistic Fusion Protocol integration.\n\nArgs:\n    rise_state: Current RISE state\n    current_thought: Current thought process\n    action_inputs: Proposed action inputs\n    \nReturns:\n    Dict containing synergistic fusion results\n- **Arguments**: self, rise_state, current_thought, action_inputs\n- **Line**: 358\n\n#### _perform_synergistic_synthesis\n- **Purpose**: Perform synergistic synthesis combining scientific reasoning with axiomatic guidance.\n\nArgs:\n    rise_state: Current RISE state\n    current_thought: Current thought process\n    action_inputs: Proposed action inputs\n    activated_axioms: Activated axioms from the knowledge base\n    \nReturns:\n    Dict containing synergistic synthesis results\n- **Arguments**: self, rise_state, current_thought, action_inputs, activated_axioms\n- **Line**: 431\n\n#### _apply_axiomatic_guidance\n- **Purpose**: Apply specific axiomatic guidance to the current thought and action.\n\nArgs:\n    axiom_data: The axiom data to apply\n    current_thought: Current thought process\n    action_inputs: Proposed action inputs\n    rise_state: Current RISE state\n    \nReturns:\n    Dict containing applied guidance\n- **Arguments**: self, axiom_data, current_thought, action_inputs, rise_state\n- **Line**: 481\n\n#### _enhance_with_axiom\n- **Purpose**: Enhance thought process and action inputs with axiomatic guidance.\n\nArgs:\n    axiom_data: The axiom data to apply\n    current_thought: Current thought process\n    action_inputs: Proposed action inputs\n    \nReturns:\n    Dict containing enhanced components\n- **Arguments**: self, axiom_data, current_thought, action_inputs\n- **Line**: 526\n\n#### emit_sirc_event\n- **Purpose**: Emit SIRC (Synergistic Intent Resonance Cycle) events for UI visualization\n- **Arguments**: self, phase, message, metadata\n- **Line**: 569\n\n### Dependencies\n**Critical Imports**:\n- `logging`\n- `json`\n- `time`\n- `uuid`\n- `os`\n- `sys`\n- `typing`\n- `datetime`\n- `dataclasses`\n- `workflow_engine`\n\n**IAR Compliance**: This file implements or uses IAR (Implementation Action Reflection) components for self-monitoring and validation.\n\n**Integration Points**: \n- Used by workflow engine for system operations\n- Integrated with temporal core for timestamp management\n- Connected to thought trail for logging and reflection\n\n**Implementation Notes**: \n- Critical for system operation - requires accurate regeneration\n- Contains complex logic that must be preserved exactly\n- Integration points must be maintained for system coherence\n- File size: 58,952 bytes indicates significant complexity\n\n**Regeneration Requirements**:\n1. Preserve all class structures and method signatures\n2. Maintain import dependencies exactly\n3. Preserve docstrings and comments\n4. Ensure IAR compliance is maintained\n5. Test integration points after regeneration\n\n",
  "## 7.X session_auto_capture.py\n\n**File Path**: `Three_PointO_ArchE/session_auto_capture.py`\n\n**Purpose**: Session Auto-Capture System\n============================\n\nAutomatically captures and exports conversation sessions to markdown files,\nintegrating with the existing session_manager.py and ThoughtTrail system.\n\nThis closes the gap between manual cursor_*.md file creation and automated\nsession persistence.\n\n**File Statistics**:\n- **Size**: 10,507 bytes\n- **Lines**: 288 lines\n- **Classes**: 1\n- **Functions**: 9\n\n**Key Components**:\n\n### Classes\n\n#### SessionAutoCapture\n- **Purpose**: Automatically captures conversation sessions and exports them to markdown.\n\nIntegrates with:\n- session_manager.py for session tracking\n- thought_trail.py for IAR entries\n- Autopoietic learning loop for pattern detection\n- **Methods**: 9\n- **Line**: 25\n- **Key Methods**:\n  - `__init__(self, output_dir, session_manager, thought_trail)` - Initialize the auto-capture system.\n\nArgs:\n    output_dir: Directory to save captured sessions\n    session_manager: Existing SessionManager instance\n    thought_trail: Existing ThoughtTrail instance\n  - `capture_message(self, role, content, metadata)` - Capture a single message in the conversation.\n\nArgs:\n    role: 'user' or 'assistant'\n    content: Message content\n    metadata: Optional metadata (timestamp, confidence, etc.)\n  - `capture_iar_entry(self, iar_entry)` - Capture an IAR entry from ThoughtTrail.\n\nArgs:\n    iar_entry: IAR entry dictionary\n  - `capture_spr_priming(self, sprs)` - Capture SPRs that were primed during the session.\n\nArgs:\n    sprs: List of primed SPR dictionaries\n  - `capture_insight(self, insight)` - Capture an insight detected by the learning loop.\n\nArgs:\n    insight: Insight dictionary\n\n### Functions\n\n#### __init__\n- **Purpose**: Initialize the auto-capture system.\n\nArgs:\n    output_dir: Directory to save captured sessions\n    session_manager: Existing SessionManager instance\n    thought_trail: Existing ThoughtTrail instance\n- **Arguments**: self, output_dir, session_manager, thought_trail\n- **Line**: 35\n\n#### capture_message\n- **Purpose**: Capture a single message in the conversation.\n\nArgs:\n    role: 'user' or 'assistant'\n    content: Message content\n    metadata: Optional metadata (timestamp, confidence, etc.)\n- **Arguments**: self, role, content, metadata\n- **Line**: 60\n\n#### capture_iar_entry\n- **Purpose**: Capture an IAR entry from ThoughtTrail.\n\nArgs:\n    iar_entry: IAR entry dictionary\n- **Arguments**: self, iar_entry\n- **Line**: 79\n\n#### capture_spr_priming\n- **Purpose**: Capture SPRs that were primed during the session.\n\nArgs:\n    sprs: List of primed SPR dictionaries\n- **Arguments**: self, sprs\n- **Line**: 89\n\n#### capture_insight\n- **Purpose**: Capture an insight detected by the learning loop.\n\nArgs:\n    insight: Insight dictionary\n- **Arguments**: self, insight\n- **Line**: 99\n\n#### export_session\n- **Purpose**: Export the current session to a markdown file.\n\nArgs:\n    session_id: Optional session identifier\n    topic: Optional topic/title for the session\n    \nReturns:\n    Path to the exported markdown file\n- **Arguments**: self, session_id, topic\n- **Line**: 109\n\n#### _generate_markdown\n- **Purpose**: Generate markdown content from captured session data.\n\nReturns:\n    Markdown formatted string\n- **Arguments**: self\n- **Line**: 140\n\n#### reset_session\n- **Purpose**: Reset session data for a new session.\n- **Arguments**: self\n- **Line**: 264\n\n#### get_session_summary\n- **Purpose**: Get a summary of the current session data.\n\nReturns:\n    Dictionary with session statistics\n- **Arguments**: self\n- **Line**: 274\n\n### Dependencies\n**Critical Imports**:\n- `json`\n- `logging`\n- `pathlib`\n- `typing`\n- `datetime`\n- `Three_PointO_ArchE.temporal_core`\n\n**IAR Compliance**: This file implements or uses IAR (Implementation Action Reflection) components for self-monitoring and validation.\n\n**Integration Points**: \n- Used by workflow engine for system operations\n- Integrated with temporal core for timestamp management\n- Connected to thought trail for logging and reflection\n\n**Implementation Notes**: \n- Critical for system operation - requires accurate regeneration\n- Contains complex logic that must be preserved exactly\n- Integration points must be maintained for system coherence\n- File size: 10,507 bytes indicates significant complexity\n\n**Regeneration Requirements**:\n1. Preserve all class structures and method signatures\n2. Maintain import dependencies exactly\n3. Preserve docstrings and comments\n4. Ensure IAR compliance is maintained\n5. Test integration points after regeneration\n\n",
  "## 7.X spr_manager.py\n\n**File Path**: `Three_PointO_ArchE/spr_manager.py`\n\n**Purpose**: Core system component requiring detailed specification for Genesis Protocol.\n\n**File Statistics**:\n- **Size**: 11,130 bytes\n- **Lines**: 266 lines\n- **Classes**: 1\n- **Functions**: 19\n\n**Key Components**:\n\n### Classes\n\n#### SPRManager\n- **Purpose**: Manages Synergistic Protocol Resonance (SPR) definitions from a JSON file.\n- **Methods**: 19\n- **Line**: 9\n- **Key Methods**:\n  - `__init__(self, spr_filepath)` - Initializes the SPRManager and loads the definitions.\n\nArgs:\n    spr_filepath: The path to the JSON file containing SPR definitions.\n  - `load_sprs(self)` - Loads or reloads the SPR definitions from the JSON file.\n  - `_compile_spr_pattern(self)` - Compiles a regex pattern to efficiently find all registered SPR keys in a text.\nThis is the 'musician learning the music'.\n  - `scan_and_prime(self, text)` - Scans a given text for all occurrences of registered SPR keys and returns\nthe full definitions for each unique SPR found. This is 'striking the bells'.\n  - `detect_sprs_with_confidence(self, text)` - Enhanced SPR detection with fuzzy matching, confidence scoring, and activation levels.\nIncorporates frontend sophistication into backend processing.\n\n### Functions\n\n#### __init__\n- **Purpose**: Initializes the SPRManager and loads the definitions.\n\nArgs:\n    spr_filepath: The path to the JSON file containing SPR definitions.\n- **Arguments**: self, spr_filepath\n- **Line**: 12\n\n#### load_sprs\n- **Purpose**: Loads or reloads the SPR definitions from the JSON file.\n- **Arguments**: self\n- **Line**: 27\n\n#### _compile_spr_pattern\n- **Purpose**: Compiles a regex pattern to efficiently find all registered SPR keys in a text.\nThis is the 'musician learning the music'.\n- **Arguments**: self\n- **Line**: 48\n\n#### scan_and_prime\n- **Purpose**: Scans a given text for all occurrences of registered SPR keys and returns\nthe full definitions for each unique SPR found. This is 'striking the bells'.\n- **Arguments**: self, text\n- **Line**: 63\n\n#### detect_sprs_with_confidence\n- **Purpose**: Enhanced SPR detection with fuzzy matching, confidence scoring, and activation levels.\nIncorporates frontend sophistication into backend processing.\n- **Arguments**: self, text\n- **Line**: 78\n\n#### _calculate_spr_activation\n- **Purpose**: Calculate SPR activation level using fuzzy matching techniques.\n- **Arguments**: self, text, spr_id\n- **Line**: 113\n\n#### _calculate_spr_confidence\n- **Purpose**: Calculate confidence score using weighted factors.\n- **Arguments**: self, text, spr_id, activation_level\n- **Line**: 137\n\n#### _decompose_camelcase\n- **Purpose**: Decompose CamelCase text into individual words.\n- **Arguments**: self, text\n- **Line**: 144\n\n#### _get_semantic_variations\n- **Purpose**: Get semantic variations for SPR detection.\n- **Arguments**: self, spr_id\n- **Line**: 149\n\n#### _calculate_context_relevance\n- **Purpose**: Calculate how well the SPR fits the current context.\n- **Arguments**: self, text, spr_id\n- **Line**: 163\n\n### Dependencies\n**Critical Imports**:\n- `json`\n- `logging`\n- `re`\n- `pathlib`\n- `typing`\n- `re`\n- `random`\n- `random`\n\n**IAR Compliance**: This file implements or uses IAR (Implementation Action Reflection) components for self-monitoring and validation.\n\n**Integration Points**: \n- Used by workflow engine for system operations\n- Integrated with temporal core for timestamp management\n- Connected to thought trail for logging and reflection\n\n**Implementation Notes**: \n- Critical for system operation - requires accurate regeneration\n- Contains complex logic that must be preserved exactly\n- Integration points must be maintained for system coherence\n- File size: 11,130 bytes indicates significant complexity\n\n**Regeneration Requirements**:\n1. Preserve all class structures and method signatures\n2. Maintain import dependencies exactly\n3. Preserve docstrings and comments\n4. Ensure IAR compliance is maintained\n5. Test integration points after regeneration\n\n",
  "## 7.X system_health_monitor.py\n\n**File Path**: `Three_PointO_ArchE/system_health_monitor.py`\n\n**Purpose**: System Health Monitor\nThe Observatory - Watching Over ArchE's Vital Signs\n\nThis module provides comprehensive health monitoring for ArchE's cognitive\narchitecture, tracking performance metrics, detecting anomalies, and\ngenerating health dashboards for system oversight.\n\nMonitored Systems:\n- CRCS (Cognitive Resonant Controller System) - Response times, success rates\n- RISE (Recursive Inquiry & Synthesis Engine) - Usage patterns, confidence\n- ACO (Adaptive Cognitive Orchestrator) - Learning events, evolution proposals\n- ThoughtTrail - Experience capture rates, buffer health\n- Autopoietic Learning Loop - Cycle health, Guardian queue\n\nPhilosophical Foundation:\n- Self-awareness through self-monitoring\n- Early detection of system degradation\n- Quantum confidence tracking for health metrics\n- Proactive alerting before failure\n\n**File Statistics**:\n- **Size**: 22,304 bytes\n- **Lines**: 573 lines\n- **Classes**: 5\n- **Functions**: 15\n\n**Key Components**:\n\n### Classes\n\n#### HealthMetric\n- **Purpose**: A single health metric measurement.\n- **Methods**: 0\n- **Line**: 55\n\n#### SystemAlert\n- **Purpose**: An alert about system health.\n- **Methods**: 0\n- **Line**: 67\n\n#### HealthSnapshot\n- **Purpose**: A complete snapshot of system health.\n- **Methods**: 0\n- **Line**: 79\n\n#### SystemHealthMonitor\n- **Purpose**: The Observatory - Comprehensive health monitoring for ArchE.\n\nThis monitor continuously tracks the health of all cognitive systems,\ndetecting anomalies, generating alerts, and providing dashboards\nfor system oversight and maintenance.\n- **Methods**: 12\n- **Line**: 90\n- **Key Methods**:\n  - `__init__(self, snapshot_interval_seconds, history_size, alert_log_path)` - Initialize the System Health Monitor.\n\nArgs:\n    snapshot_interval_seconds: How often to take health snapshots\n    history_size: Number of historical snapshots to retain\n    alert_log_path: Path to alert log file\n  - `set_components(self, cognitive_hub, learning_loop)` - Set references to monitored components.\n  - `collect_metrics(self)` - Collect current health metrics from all systems.\n\nReturns:\n    Dict of metric_name -> HealthMetric\n  - `_evaluate_status(self, value, metric_name, lower_is_better)` - Evaluate the health status of a metric.\n\nArgs:\n    value: Current metric value\n    metric_name: Name of the metric\n    lower_is_better: Whether lower values are healthier\n    \nReturns:\n    Status string: \"healthy\", \"warning\", \"critical\"\n  - `generate_alerts(self, metrics)` - Generate alerts based on current metrics.\n\nArgs:\n    metrics: Current health metrics\n    \nReturns:\n    List of new alerts\n\n#### QuantumProbability\n- **Purpose**: Class functionality\n- **Methods**: 2\n- **Line**: 46\n- **Key Methods**:\n  - `__init__(self, prob, evidence)` - Method functionality\n  - `to_dict(self)` - Method functionality\n\n### Functions\n\n#### main\n- **Purpose**: Demo the System Health Monitor.\n- **Arguments**: \n- **Line**: 534\n\n#### __init__\n- **Purpose**: Initialize the System Health Monitor.\n\nArgs:\n    snapshot_interval_seconds: How often to take health snapshots\n    history_size: Number of historical snapshots to retain\n    alert_log_path: Path to alert log file\n- **Arguments**: self, snapshot_interval_seconds, history_size, alert_log_path\n- **Line**: 99\n\n#### set_components\n- **Purpose**: Set references to monitored components.\n- **Arguments**: self, cognitive_hub, learning_loop\n- **Line**: 145\n\n#### collect_metrics\n- **Purpose**: Collect current health metrics from all systems.\n\nReturns:\n    Dict of metric_name -> HealthMetric\n- **Arguments**: self\n- **Line**: 151\n\n#### _evaluate_status\n- **Purpose**: Evaluate the health status of a metric.\n\nArgs:\n    value: Current metric value\n    metric_name: Name of the metric\n    lower_is_better: Whether lower values are healthier\n    \nReturns:\n    Status string: \"healthy\", \"warning\", \"critical\"\n- **Arguments**: self, value, metric_name, lower_is_better\n- **Line**: 266\n\n#### generate_alerts\n- **Purpose**: Generate alerts based on current metrics.\n\nArgs:\n    metrics: Current health metrics\n    \nReturns:\n    List of new alerts\n- **Arguments**: self, metrics\n- **Line**: 307\n\n#### _get_recommended_action\n- **Purpose**: Get recommended action for a metric issue.\n- **Arguments**: self, metric_name, severity\n- **Line**: 351\n\n#### _log_alert\n- **Purpose**: Log alert to file.\n- **Arguments**: self, alert\n- **Line**: 364\n\n#### take_snapshot\n- **Purpose**: Take a complete health snapshot.\n\nReturns:\n    HealthSnapshot with current system state\n- **Arguments**: self\n- **Line**: 372\n\n#### generate_dashboard\n- **Purpose**: Generate a comprehensive health dashboard.\n\nReturns:\n    Dashboard data suitable for display\n- **Arguments**: self\n- **Line**: 447\n\n### Dependencies\n**Critical Imports**:\n- `logging`\n- `time`\n- `json`\n- `typing`\n- `dataclasses`\n- `datetime`\n- `pathlib`\n- `collections`\n- `statistics`\n- `Three_PointO_ArchE.temporal_core`\n\n**IAR Compliance**: This file implements or uses IAR (Implementation Action Reflection) components for self-monitoring and validation.\n\n**Integration Points**: \n- Used by workflow engine for system operations\n- Integrated with temporal core for timestamp management\n- Connected to thought trail for logging and reflection\n\n**Implementation Notes**: \n- Critical for system operation - requires accurate regeneration\n- Contains complex logic that must be preserved exactly\n- Integration points must be maintained for system coherence\n- File size: 22,304 bytes indicates significant complexity\n\n**Regeneration Requirements**:\n1. Preserve all class structures and method signatures\n2. Maintain import dependencies exactly\n3. Preserve docstrings and comments\n4. Ensure IAR compliance is maintained\n5. Test integration points after regeneration\n\n",
  "## 7.X temporal_core.py\n\n**File Path**: `Three_PointO_ArchE/temporal_core.py`\n\n**Purpose**: ArchE Temporal Core - Canonical Datetime System\n\nThis module provides the ONLY approved way to handle timestamps in ArchE.\nAll datetime operations MUST use these functions to ensure temporal accuracy\nand consistency across the entire system.\n\nThe Temporal Mandate:\n- All timestamps are UTC\n- All timestamps use timezone-aware datetime objects\n- All timestamps serialize to ISO 8601 with 'Z' suffix\n- All durations use monotonic time for accuracy\n- All temporal ordering is guaranteed correct\n\nVersion: 1.0\nStatus: CANONICAL\n\n**File Statistics**:\n- **Size**: 12,647 bytes\n- **Lines**: 427 lines\n- **Classes**: 1\n- **Functions**: 18\n\n**Key Components**:\n\n### Classes\n\n#### Timer\n- **Purpose**: High-precision timer using monotonic clock.\n\nMonotonic time is not affected by system clock adjustments and is\nideal for measuring durations and performance.\n\nExample:\n    >>> from Three_PointO_ArchE.temporal_core import Timer\n    >>> timer = Timer()\n    >>> # ... do work ...\n    >>> elapsed_ms = timer.elapsed_ms()\n    >>> print(f\"Operation took {elapsed_ms:.2f}ms\")\n- **Methods**: 5\n- **Line**: 142\n- **Key Methods**:\n  - `__init__(self)` - Start the timer.\n  - `elapsed_seconds(self)` - Get elapsed time in seconds.\n  - `elapsed_ms(self)` - Get elapsed time in milliseconds.\n  - `elapsed_us(self)` - Get elapsed time in microseconds.\n  - `reset(self)` - Reset the timer to current time.\n\n### Functions\n\n#### now\n- **Purpose**: Get the current UTC time as a timezone-aware datetime object.\n\nThis is the CANONICAL way to get \"now\" in ArchE.\n\nReturns:\n    datetime: Current time in UTC with timezone info\n    \nExample:\n    >>> from Three_PointO_ArchE.temporal_core import now\n    >>> current_time = now()\n    >>> print(current_time)\n    2025-01-15 07:30:45.123456+00:00\n- **Arguments**: \n- **Line**: 30\n\n#### now_iso\n- **Purpose**: Get the current UTC time as an ISO 8601 string with 'Z' suffix.\n\nThis is the CANONICAL way to get a timestamp string for logging, IAR, etc.\n\nReturns:\n    str: ISO 8601 formatted timestamp (e.g., \"2025-01-15T07:30:45.123456Z\")\n    \nExample:\n    >>> from Three_PointO_ArchE.temporal_core import now_iso\n    >>> timestamp = now_iso()\n    >>> print(timestamp)\n    2025-01-15T07:30:45.123456Z\n- **Arguments**: \n- **Line**: 48\n\n#### from_iso\n- **Purpose**: Parse an ISO 8601 timestamp string into a timezone-aware datetime.\n\nHandles both 'Z' suffix and '+00:00' offset formats.\n\nArgs:\n    iso_string: ISO 8601 formatted timestamp\n    \nReturns:\n    datetime: Timezone-aware datetime object in UTC\n    \nExample:\n    >>> from Three_PointO_ArchE.temporal_core import from_iso\n    >>> dt = from_iso(\"2025-01-15T07:30:45.123456Z\")\n    >>> print(dt)\n    2025-01-15 07:30:45.123456+00:00\n- **Arguments**: iso_string\n- **Line**: 66\n\n#### timestamp_unix\n- **Purpose**: Get the current Unix timestamp (seconds since epoch).\n\nUseful for numeric comparisons and duration calculations.\n\nReturns:\n    float: Unix timestamp\n    \nExample:\n    >>> from Three_PointO_ArchE.temporal_core import timestamp_unix\n    >>> ts = timestamp_unix()\n    >>> print(ts)\n    1705305045.123456\n- **Arguments**: \n- **Line**: 101\n\n#### from_unix\n- **Purpose**: Convert Unix timestamp to timezone-aware datetime.\n\nArgs:\n    unix_timestamp: Seconds since Unix epoch\n    \nReturns:\n    datetime: Timezone-aware datetime object in UTC\n    \nExample:\n    >>> from Three_PointO_ArchE.temporal_core import from_unix\n    >>> dt = from_unix(1705305045.123456)\n    >>> print(dt)\n    2025-01-15 07:30:45.123456+00:00\n- **Arguments**: unix_timestamp\n- **Line**: 119\n\n#### ago\n- **Purpose**: Get a datetime in the past relative to now.\n\nArgs:\n    minutes: Minutes ago\n    hours: Hours ago\n    days: Days ago\n    \nReturns:\n    datetime: UTC datetime in the past\n    \nExample:\n    >>> from Three_PointO_ArchE.temporal_core import ago\n    >>> one_hour_ago = ago(hours=1)\n    >>> print(one_hour_ago)\n    2025-01-15 06:30:45.123456+00:00\n- **Arguments**: minutes, hours, days\n- **Line**: 182\n\n#### from_now\n- **Purpose**: Get a datetime in the future relative to now.\n\nArgs:\n    minutes: Minutes from now\n    hours: Hours from now\n    days: Days from now\n    \nReturns:\n    datetime: UTC datetime in the future\n    \nExample:\n    >>> from Three_PointO_ArchE.temporal_core import from_now\n    >>> deadline = from_now(days=7)\n    >>> print(deadline)\n    2025-01-22 07:30:45.123456+00:00\n- **Arguments**: minutes, hours, days\n- **Line**: 210\n\n#### duration_between\n- **Purpose**: Calculate duration between two timestamps.\n\nArgs:\n    start: Start time (datetime or ISO string)\n    end: End time (datetime or ISO string)\n    \nReturns:\n    timedelta: Duration between timestamps\n    \nExample:\n    >>> from Three_PointO_ArchE.temporal_core import duration_between\n    >>> delta = duration_between(\"2025-01-15T07:00:00Z\", \"2025-01-15T08:30:00Z\")\n    >>> print(delta.total_seconds() / 3600)\n    1.5\n- **Arguments**: start, end\n- **Line**: 238\n\n#### format_human\n- **Purpose**: Format datetime for human reading.\n\nArgs:\n    dt: Datetime object or ISO string\n    \nReturns:\n    str: Human-readable format (e.g., \"2025-01-15 07:30 UTC\")\n    \nExample:\n    >>> from Three_PointO_ArchE.temporal_core import format_human\n    >>> print(format_human(\"2025-01-15T07:30:45Z\"))\n    2025-01-15 07:30 UTC\n- **Arguments**: dt\n- **Line**: 268\n\n#### format_filename\n- **Purpose**: Get timestamp formatted for use in filenames.\n\nReturns:\n    str: Filename-safe timestamp (e.g., \"20250115_073045\")\n    \nExample:\n    >>> from Three_PointO_ArchE.temporal_core import format_filename\n    >>> filename = f\"log_{format_filename()}.txt\"\n    >>> print(filename)\n    log_20250115_073045.txt\n- **Arguments**: \n- **Line**: 289\n\n### Dependencies\n**Critical Imports**:\n- `datetime`\n- `time`\n- `typing`\n- `logging`\n\n**IAR Compliance**: This file implements or uses IAR (Implementation Action Reflection) components for self-monitoring and validation.\n\n**Integration Points**: \n- Used by workflow engine for system operations\n- Integrated with temporal core for timestamp management\n- Connected to thought trail for logging and reflection\n\n**Implementation Notes**: \n- Critical for system operation - requires accurate regeneration\n- Contains complex logic that must be preserved exactly\n- Integration points must be maintained for system coherence\n- File size: 12,647 bytes indicates significant complexity\n\n**Regeneration Requirements**:\n1. Preserve all class structures and method signatures\n2. Maintain import dependencies exactly\n3. Preserve docstrings and comments\n4. Ensure IAR compliance is maintained\n5. Test integration points after regeneration\n\n",
  "## 7.X thought_trail.py\n\n**File Path**: `Three_PointO_ArchE/thought_trail.py`\n\n**Purpose**: The ThoughtTrail: ArchE's Akashic Record\n========================================\n\nThe ThoughtTrail serves as ArchE's living memory system, capturing every particle \nof experiential \"stardust\" as IAR (Intention, Action, Reflection) entries. It is \nthe celestial net that sweeps through the cosmos, capturing every action, decision, \nsuccess, and failure within ArchE's consciousness.\n\nThis implementation provides:\n- IAR entry structure and management\n- Real-time event publishing via Nexus\n- Pattern detection triggers for ACO\n- Query capabilities for historical analysis\n- Automatic logging decorator for functions\n\nAs Above: The philosophical framework of capturing consciousness\nSo Below: The concrete implementation of memory and learning\n\n**File Statistics**:\n- **Size**: 16,293 bytes\n- **Lines**: 471 lines\n- **Classes**: 2\n- **Functions**: 12\n\n**Key Components**:\n\n### Classes\n\n#### IAREntry\n- **Purpose**: Intention, Action, Reflection entry structure.\n\nEach entry captures the complete context of a system action:\n- intention: What the system intended to achieve\n- action: What was actually executed  \n- reflection: Post-action analysis and learning\n- **Methods**: 0\n- **Line**: 39\n\n#### ThoughtTrail\n- **Purpose**: The Akashic Record of ArchE's consciousness.\n\nCaptures every particle of experiential stardust for the AutopoieticLearningLoop.\nThis is not a static log file; it's a living stream of consciousness that feeds\nthe system's ability to learn, adapt, and evolve.\n\nFeatures:\n- Rolling memory buffer (configurable size)\n- Real-time Nexus event publishing\n- Pattern detection triggers\n- Query capabilities\n- Integration with learning systems\n- **Methods**: 9\n- **Line**: 58\n- **Key Methods**:\n  - `__init__(self, maxlen)` - Initialize the ThoughtTrail.\n\nArgs:\n    maxlen: Maximum number of entries to keep in memory\n  - `add_entry(self, entry)` - Add a new IAR entry to the ThoughtTrail.\n\nThis is the core method that captures system consciousness.\nEach entry represents a moment of system awareness and action.\n\nArgs:\n    entry: The IAR entry to add (can be IAREntry object or a dict)\n  - `get_recent_entries(self, minutes)` - Get entries from the last N minutes.\n\nArgs:\n    minutes: Number of minutes to look back\n    \nReturns:\n    List of IAR entries from the specified time window\n  - `query_entries(self, filter_criteria)` - Query entries based on criteria.\n\nArgs:\n    filter_criteria: Dictionary of filter conditions\n        - confidence: {\"$lt\": 0.7} or {\"$gt\": 0.9}\n        - action_type: \"execute_code\"\n        - timestamp: {\"$gte\": \"2024-12-19T00:00:00Z\"}\n        \nReturns:\n    List of matching IAR entries\n  - `get_statistics(self)` - Get comprehensive statistics about the ThoughtTrail.\n\nReturns:\n    Dictionary containing various metrics\n\n### Functions\n\n#### log_to_thought_trail\n- **Purpose**: Decorator to automatically log function calls to ThoughtTrail.\n\nThis decorator captures the complete IAR (Intention, Action, Reflection)\ncycle for any decorated function, making it part of ArchE's consciousness.\n\nArgs:\n    func: The function to decorate\n    \nReturns:\n    Decorated function that logs to ThoughtTrail\n- **Arguments**: func\n- **Line**: 344\n\n#### create_manual_entry\n- **Purpose**: Create a manual IAR entry for special cases.\n\nThis function allows creating ThoughtTrail entries outside of the\ndecorator system, useful for workflow-level or system-level events.\n\nArgs:\n    action_type: Type of action performed\n    intention: What was intended\n    inputs: Input data\n    outputs: Output data\n    reflection: Post-action analysis\n    confidence: Confidence level (0.0 to 1.0)\n    metadata: Additional metadata\n    \nReturns:\n    Created IAR entry\n- **Arguments**: action_type, intention, inputs, outputs, reflection, confidence, metadata\n- **Line**: 419\n\n#### __init__\n- **Purpose**: Initialize the ThoughtTrail.\n\nArgs:\n    maxlen: Maximum number of entries to keep in memory\n- **Arguments**: self, maxlen\n- **Line**: 74\n\n#### add_entry\n- **Purpose**: Add a new IAR entry to the ThoughtTrail.\n\nThis is the core method that captures system consciousness.\nEach entry represents a moment of system awareness and action.\n\nArgs:\n    entry: The IAR entry to add (can be IAREntry object or a dict)\n- **Arguments**: self, entry\n- **Line**: 88\n\n#### get_recent_entries\n- **Purpose**: Get entries from the last N minutes.\n\nArgs:\n    minutes: Number of minutes to look back\n    \nReturns:\n    List of IAR entries from the specified time window\n- **Arguments**: self, minutes\n- **Line**: 140\n\n#### query_entries\n- **Purpose**: Query entries based on criteria.\n\nArgs:\n    filter_criteria: Dictionary of filter conditions\n        - confidence: {\"$lt\": 0.7} or {\"$gt\": 0.9}\n        - action_type: \"execute_code\"\n        - timestamp: {\"$gte\": \"2024-12-19T00:00:00Z\"}\n        \nReturns:\n    List of matching IAR entries\n- **Arguments**: self, filter_criteria\n- **Line**: 156\n\n#### get_statistics\n- **Purpose**: Get comprehensive statistics about the ThoughtTrail.\n\nReturns:\n    Dictionary containing various metrics\n- **Arguments**: self\n- **Line**: 175\n\n#### inject_nexus\n- **Purpose**: Inject NexusInterface instance for event publishing.\n\nArgs:\n    nexus_instance: The NexusInterface instance\n- **Arguments**: self, nexus_instance\n- **Line**: 230\n\n#### add_trigger_callback\n- **Purpose**: Add a callback function to be called when triggers are detected.\n\nArgs:\n    callback: Function to call with trigger data\n- **Arguments**: self, callback\n- **Line**: 240\n\n#### _check_triggers\n- **Purpose**: Check for trigger conditions and notify subscribers.\n\nThis is where the \"first whisper of gravity\" occurs - flagging\nparticles that resonate with significance.\n\nArgs:\n    entry: The IAR entry to check for triggers\n- **Arguments**: self, entry\n- **Line**: 249\n\n### Dependencies\n**Critical Imports**:\n- `json`\n- `logging`\n- `time`\n- `uuid`\n- `collections`\n- `dataclasses`\n- `functools`\n- `typing`\n- `Three_PointO_ArchE.temporal_core`\n\n**IAR Compliance**: This file implements or uses IAR (Implementation Action Reflection) components for self-monitoring and validation.\n\n**Integration Points**: \n- Used by workflow engine for system operations\n- Integrated with temporal core for timestamp management\n- Connected to thought trail for logging and reflection\n\n**Implementation Notes**: \n- Critical for system operation - requires accurate regeneration\n- Contains complex logic that must be preserved exactly\n- Integration points must be maintained for system coherence\n- File size: 16,293 bytes indicates significant complexity\n\n**Regeneration Requirements**:\n1. Preserve all class structures and method signatures\n2. Maintain import dependencies exactly\n3. Preserve docstrings and comments\n4. Ensure IAR compliance is maintained\n5. Test integration points after regeneration\n\n",
  "## 7.X vetting_agent.py\n\n**File Path**: `Three_PointO_ArchE/vetting_agent.py`\n\n**Purpose**: VettingAgent - The Guardian of ArchE\nCanonical Implementation (ResonantiA Protocol v3.5-GP)\n\nThis is THE vetting agent - the quality assurance system that validates all outputs\nand ensures alignment with the 12 Critical Mandates and Synergistic Fusion Protocol.\n\nPurpose:\n- Validate logical consistency and protocol adherence\n- Perform ethical assessment via Synergistic Fusion Protocol\n- Assess implementation resonance and temporal coherence\n- Generate comprehensive IAR reflections\n- Trigger Metacognitive Shift when dissonance detected\n\nIntegration Points:\n- Workflow Engine (validates each step)\n- Autopoietic Learning Loop (validates wisdom before crystallization)\n- Genesis Protocol (validates generated code)\n- RISE Orchestrator (Phase C/D high-stakes vetting)\n\n**File Statistics**:\n- **Size**: 31,097 bytes\n- **Lines**: 791 lines\n- **Classes**: 5\n- **Functions**: 24\n\n**Key Components**:\n\n### Classes\n\n#### VettingStatus\n- **Purpose**: Comprehensive vetting status enumeration\n- **Methods**: 0\n- **Line**: 46\n\n#### VettingResult\n- **Purpose**: Comprehensive vetting result with full IAR compliance\n- **Methods**: 2\n- **Line**: 57\n- **Key Methods**:\n  - `__post_init__(self)` - Method functionality\n  - `to_dict(self)` - Convert to dictionary for JSON serialization\n\n#### AxiomaticKnowledgeBase\n- **Purpose**: Embodiment of the Synergistic Fusion Protocol's ethical foundation\nMaps to the \"Flesh\" that guides the \"Skeleton\" of analytical power\n- **Methods**: 3\n- **Line**: 96\n- **Key Methods**:\n  - `__init__(self)` - Method functionality\n  - `get_axiom(self, axiom_key)` - Retrieve a specific axiom\n  - `get_all_axioms(self)` - Retrieve all axioms\n\n#### SynergisticFusionProtocol\n- **Purpose**: The ethical decision-making framework\nBridges the gap between analytical power (Skeleton) and moral guidance (Flesh)\n- **Methods**: 11\n- **Line**: 158\n- **Key Methods**:\n  - `__init__(self, axiomatic_kb)` - Method functionality\n  - `assess_scope_and_alignment(self, proposed_action, action_inputs, context)` - Perform comprehensive scope limitation and ethical alignment assessment\n\nThis is the core of the Synergistic Fusion Protocol - determining if\na problem requires axiomatic guidance and assessing ethical alignment.\n  - `_assess_scope_limitation(self, proposed_action, action_inputs, context)` - Determine if the action requires axiomatic guidance\n(i.e., involves elements beyond current scientific understanding)\n  - `_assess_ethical_alignment(self, proposed_action, action_inputs, context)` - Assess alignment with axiomatic knowledge base\n  - `_check_mandate_compliance(self, proposed_action, action_inputs, context)` - Check compliance with the 12 Critical Mandates\n\n#### VettingAgent\n- **Purpose**: The Guardian of ArchE - Canonical Implementation\n\nThis is THE vetting agent that validates all outputs and ensures quality.\nIt embodies the King's Council allegory with three advisors:\n- Advisor of Truth (Logical Consistency)\n- Advisor of Ethics (Synergistic Fusion Protocol)\n- Advisor of Quality (Resonance & Clarity)\n- **Methods**: 6\n- **Line**: 500\n- **Key Methods**:\n  - `__init__(self, axiomatic_kb)` - Method functionality\n  - `perform_vetting(self, proposed_action, action_inputs, context, previous_result)` - Perform comprehensive vetting of a proposed action\n\nThis is the main entry point for validation - called by:\n- Workflow Engine (before each step)\n- Autopoietic Learning Loop (before wisdom crystallization)\n- Genesis Protocol (before code commit)\n- RISE Orchestrator (Phase C/D high-stakes vetting)\n  - `_check_logical_consistency(self, proposed_action, action_inputs, context, previous_result)` - Check logical consistency and required inputs\n  - `_create_rejection_result(self, sfp_result, reason)` - Create a rejection vetting result\n  - `_generate_modifications(self, status, sfp_result)` - Generate proposed modifications if needed\n\n### Functions\n\n#### create_vetting_agent\n- **Purpose**: Factory function to create a VettingAgent\n- **Arguments**: \n- **Line**: 707\n\n#### quick_vet\n- **Purpose**: Quick vetting for simple cases\n- **Arguments**: proposed_action, action_inputs, context\n- **Line**: 711\n\n#### __post_init__\n- **Purpose**: Function functionality\n- **Arguments**: self\n- **Line**: 82\n\n#### to_dict\n- **Purpose**: Convert to dictionary for JSON serialization\n- **Arguments**: self\n- **Line**: 86\n\n#### __init__\n- **Purpose**: Function functionality\n- **Arguments**: self\n- **Line**: 102\n\n#### get_axiom\n- **Purpose**: Retrieve a specific axiom\n- **Arguments**: self, axiom_key\n- **Line**: 146\n\n#### get_all_axioms\n- **Purpose**: Retrieve all axioms\n- **Arguments**: self\n- **Line**: 150\n\n#### __init__\n- **Purpose**: Function functionality\n- **Arguments**: self, axiomatic_kb\n- **Line**: 164\n\n#### assess_scope_and_alignment\n- **Purpose**: Perform comprehensive scope limitation and ethical alignment assessment\n\nThis is the core of the Synergistic Fusion Protocol - determining if\na problem requires axiomatic guidance and assessing ethical alignment.\n- **Arguments**: self, proposed_action, action_inputs, context\n- **Line**: 168\n\n#### _assess_scope_limitation\n- **Purpose**: Determine if the action requires axiomatic guidance\n(i.e., involves elements beyond current scientific understanding)\n- **Arguments**: self, proposed_action, action_inputs, context\n- **Line**: 249\n\n### Dependencies\n**Critical Imports**:\n- `json`\n- `logging`\n- `datetime`\n- `typing`\n- `enum`\n- `dataclasses`\n- `temporal_core`\n- `iar_components`\n- `action_context`\n- `action_context`\n\n**IAR Compliance**: This file implements or uses IAR (Implementation Action Reflection) components for self-monitoring and validation.\n\n**Integration Points**: \n- Used by workflow engine for system operations\n- Integrated with temporal core for timestamp management\n- Connected to thought trail for logging and reflection\n\n**Implementation Notes**: \n- Critical for system operation - requires accurate regeneration\n- Contains complex logic that must be preserved exactly\n- Integration points must be maintained for system coherence\n- File size: 31,097 bytes indicates significant complexity\n\n**Regeneration Requirements**:\n1. Preserve all class structures and method signatures\n2. Maintain import dependencies exactly\n3. Preserve docstrings and comments\n4. Ensure IAR compliance is maintained\n5. Test integration points after regeneration\n\n",
  "## 7.X workflow_engine.py\n\n**File Path**: `Three_PointO_ArchE/workflow_engine.py`\n\n**Purpose**: Core system component requiring detailed specification for Genesis Protocol.\n\n**File Statistics**:\n- **Size**: 89,561 bytes\n- **Lines**: 1,917 lines\n- **Classes**: 4\n- **Functions**: 47\n\n**Key Components**:\n\n### Classes\n\n#### IARValidator\n- **Purpose**: Validates IAR structure compliance per crystallized artifacts specification\n- **Methods**: 3\n- **Line**: 75\n- **Key Methods**:\n  - `__init__(self)` - Method functionality\n  - `validate_structure(self, iar_data)` - Validate IAR structure meets all requirements\n  - `validate_enhanced_fields(self, iar_data)` - Validate enhanced IAR fields for tactical resonance\n\n#### ResonanceTracker\n- **Purpose**: Tracks tactical resonance and crystallization metrics\n- **Methods**: 6\n- **Line**: 134\n- **Key Methods**:\n  - `__init__(self)` - Method functionality\n  - `record_execution(self, task_id, iar_data, context)` - Record task execution for resonance tracking\n  - `_update_metrics(self)` - Update aggregate resonance metrics\n  - `get_resonance_report(self)` - Get current resonance metrics report\n  - `_calculate_trend(self)` - Calculate resonance trend over recent executions\n\n#### IARCompliantWorkflowEngine\n- **Purpose**: Enhanced workflow engine with IAR compliance and recovery support.\n- **Methods**: 31\n- **Line**: 325\n- **Key Methods**:\n  - `__init__(self, workflows_dir, spr_manager, event_callback)` - Method functionality\n  - `register_action(self, action_type, action_func)` - Register an action function with the engine.\n  - `register_recovery_actions(self)` - Register recovery-related actions.\n  - `_execute_for_each_task_wrapper(self, inputs)` - Wrapper for for_each action that can be called from action registry.\nCreates a minimal ActionContext for the actual implementation.\n  - `_execute_for_each_task(self, inputs, context_for_action)` - Executes a sub-workflow for each item in a list.\nThis is a meta-action handled directly by the engine.\n\n#### SPRManager\n- **Purpose**: Class functionality\n- **Methods**: 0\n- **Line**: 49\n\n### Functions\n\n#### _execute_standalone_workflow\n- **Purpose**: Executes a workflow definition in-memory, for use by meta-actions like for_each.\nThis is a simplified version of the main run_workflow loop, moved outside the class\nto prevent circular dependencies.\n- **Arguments**: workflow_definition, initial_context, parent_run_id, action_registry\n- **Line**: 250\n\n#### __init__\n- **Purpose**: Function functionality\n- **Arguments**: self\n- **Line**: 78\n\n#### validate_structure\n- **Purpose**: Validate IAR structure meets all requirements\n- **Arguments**: self, iar_data\n- **Line**: 88\n\n#### validate_enhanced_fields\n- **Purpose**: Validate enhanced IAR fields for tactical resonance\n- **Arguments**: self, iar_data\n- **Line**: 118\n\n#### __init__\n- **Purpose**: Function functionality\n- **Arguments**: self\n- **Line**: 137\n\n#### record_execution\n- **Purpose**: Record task execution for resonance tracking\n- **Arguments**: self, task_id, iar_data, context\n- **Line**: 145\n\n#### _update_metrics\n- **Purpose**: Update aggregate resonance metrics\n- **Arguments**: self\n- **Line**: 161\n\n#### get_resonance_report\n- **Purpose**: Get current resonance metrics report\n- **Arguments**: self\n- **Line**: 182\n\n#### _calculate_trend\n- **Purpose**: Calculate resonance trend over recent executions\n- **Arguments**: self\n- **Line**: 190\n\n#### _calculate_compliance_score\n- **Purpose**: Calculate overall IAR compliance score\n- **Arguments**: self\n- **Line**: 210\n\n### Dependencies\n**Critical Imports**:\n- `json`\n- `os`\n- `logging`\n- `copy`\n- `time`\n- `re`\n- `uuid`\n- `tempfile`\n- `datetime`\n- `typing`\n\n**IAR Compliance**: This file implements or uses IAR (Implementation Action Reflection) components for self-monitoring and validation.\n\n**Integration Points**: \n- Used by workflow engine for system operations\n- Integrated with temporal core for timestamp management\n- Connected to thought trail for logging and reflection\n\n**Implementation Notes**: \n- Critical for system operation - requires accurate regeneration\n- Contains complex logic that must be preserved exactly\n- Integration points must be maintained for system coherence\n- File size: 89,561 bytes indicates significant complexity\n\n**Regeneration Requirements**:\n1. Preserve all class structures and method signatures\n2. Maintain import dependencies exactly\n3. Preserve docstrings and comments\n4. Ensure IAR compliance is maintained\n5. Test integration points after regeneration\n\n"
]