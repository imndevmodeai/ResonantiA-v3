#!/usr/bin/env python3
"""
The Autopoietic Genesis Agent: The World-Builder's Ritual

WHO: The Artisan of ArchE, a master programmer fluent in the ResonantiA Protocol.
WHAT: Transforms an approved Living Specification into complete, production-ready Python code.
WHEN: Invoked by the Autopoietic Genesis Protocol workflow after a spec is approved.
WHERE: This agent's code lives at Three_PointO_ArchE/genesis_agent.py.
WHY: To manifest specifications in executable reality, ensuring perfect "As Above, So Below" alignment.
HOW: By using an LLM augmented with a robust prompt to translate the abstract blueprint into concrete, compliant code.

This is the implementation of Chapter 2: The World-Builder's Ritual from the Genesis Protocol.

Generated by: AI Studio ArchE + Local ArchE (Collaborative Genesis)
Timestamp: 2025-10-12T19:15:00Z
"""

import json
import logging
import ast
from pathlib import Path
from typing import Dict, Any, Optional, Tuple

# Temporal Core Integration
from Three_PointO_ArchE.temporal_core import now_iso

# IAR Compliance
from Three_PointO_ArchE.thought_trail import log_to_thought_trail

# LLM Provider Integration
from Three_PointO_ArchE.llm_providers import get_llm_provider, get_model_for_provider

logger = logging.getLogger(__name__)


class GenesisAgent:
    """
    The Artisan of ArchE: Transforms Living Specifications into executable code.

    This agent implements the World-Builder's Ritual by:
    1. Receiving an approved Living Specification.
    2. Augmenting an LLM with the robust `code_genesis_protocol.json` prompt.
    3. Generating complete, production-ready, and IAR-compliant Python code.
    4. Performing an initial syntax validation on the generated code.
    
    This is the manifestation of the Genesis Protocol's second stage.
    """

    def __init__(self, model: Optional[str] = None):
        """
        Initializes the Genesis Agent.

        Args:
            model: The specific LLM model to use. Defaults to the provider's default.
        """
        self.llm_provider = get_llm_provider()
        self.model = model or get_model_for_provider('google')
        self.prompt_template = self._load_prompt_template()
        logger.info(f"Genesis Agent (Artisan) initialized with model: {self.model}")

    def _load_prompt_template(self) -> Dict[str, Any]:
        """
        Loads the robust prompt template for code generation from a JSON file.

        Returns:
            The loaded prompt template structure.
        """
        try:
            # Load from workflows directory
            template_path = Path(__file__).parent.parent / "workflows" / "code_genesis_protocol.json"
            
            if template_path.exists():
                with open(template_path, 'r') as f:
                    return json.load(f)
            else:
                logger.warning("code_genesis_protocol.json not found, using embedded template")
                return self._get_fallback_template()
                
        except (FileNotFoundError, json.JSONDecodeError) as e:
            logger.error(f"Could not load code_genesis_protocol.json. Using fallback. Error: {e}")
            return self._get_fallback_template()
    
    def _get_fallback_template(self) -> Dict[str, Any]:
        """
        Fallback prompt template embedded in code.
        
        Returns:
            Default prompt template structure
        """
        return {
            "role": (
                "You are the Artisan of ArchE, a master programmer fluent in the ResonantiA Protocol. "
                "Your task is to read the provided Living Specification and generate the complete, "
                "production-ready Python code that implements it. The code must be a perfect, 'So Below' "
                "reflection of the 'As Above' specification."
            ),
            "input_variables": ["specification_text", "target_file_path"],
            "output_format": {
                "file_path": "The full relative path for the new file",
                "code": "The complete, final Python code as a single string"
            },
            "constraints": [
                "The code must be written in Python 3.10+",
                "All classes and methods from the specification's blueprint must be implemented",
                "Include full type hinting for all functions and methods",
                "Include comprehensive docstrings in Google format for all classes and methods",
                "Implement IAR compliance. Key methods should be decorated with '@log_to_thought_trail'",
                "Incorporate logging for key operations",
                "Handle potential errors gracefully with try/except blocks",
                "The code must be clean, readable, and follow PEP 8 standards",
                "Do not generate placeholder or 'pass' statements. The code must be fully functional",
                "Use temporal_core for all timestamp operations (not datetime.now())"
            ]
        }

    @log_to_thought_trail
    def generate_code_from_spec(
        self, 
        specification_text: str, 
        target_file_path: str
    ) -> Dict[str, Any]:
        """
        The core method of the World-Builder's Ritual. Generates code from a specification.

        Args:
            specification_text: The full markdown content of the approved Living Specification.
            target_file_path: The intended destination for the generated code.

        Returns:
            Dictionary containing the file path, the generated code, and metadata.
        """
        logger.info(f"🔥 Initiating World-Builder's Ritual for: {target_file_path}")
        
        try:
            # Step 1: Construct the robust prompt
            prompt = self._construct_prompt(specification_text, target_file_path)

            # Step 2: Invoke the LLM
            logger.info(f"Invoking LLM ({self.model}) to generate code...")
            response_text = self.llm_provider.generate(
                prompt=prompt,
                model=self.model,
                max_tokens=16384,  # Generous token limit for complex modules
                temperature=0.2    # Low temperature for precise, deterministic code generation
            )

            # Step 3: Parse the LLM's structured response
            parsed_response = self._parse_llm_response(response_text)
            generated_code = parsed_response['code']
            
            # Step 4: Perform initial syntax validation
            syntax_valid, syntax_error = self._validate_syntax(generated_code)
            
            if not syntax_valid:
                logger.error(f"Generated code for {target_file_path} has syntax errors: {syntax_error}")
                # Note: Could trigger self-correction loop here in future

            # Step 5: Prepare the result
            result = {
                "file_path": parsed_response['file_path'],
                "code": generated_code,
                "metadata": {
                    "generated_at": now_iso(),
                    "model_used": self.model,
                    "lines_of_code": len(generated_code.splitlines()),
                    "syntax_valid": syntax_valid,
                    "syntax_error": syntax_error
                }
            }
            
            logger.info(
                f"✅ Code generation successful for {target_file_path}. "
                f"Lines: {result['metadata']['lines_of_code']}, "
                f"Syntax Valid: {syntax_valid}"
            )
            
            return result

        except Exception as e:
            logger.critical(
                f"❌ World-Builder's Ritual FAILED for {target_file_path}: {e}", 
                exc_info=True
            )
            return {
                "error": str(e),
                "file_path": target_file_path,
                "code": None,
                "metadata": {
                    "status": "failed", 
                    "generated_at": now_iso()
                }
            }

    def _construct_prompt(
        self, 
        specification_text: str, 
        target_file_path: str
    ) -> str:
        """
        Internal method to build the robust, structured prompt for the LLM.

        Args:
            specification_text: The specification content.
            target_file_path: The target file path for context.

        Returns:
            The fully formatted prompt string.
        """
        # This prompt structure is critical for constraining the LLM
        constraints_text = '\n'.join(f'- {c}' for c in self.prompt_template['constraints'])
        
        prompt = f"""
{self.prompt_template['role']}

**LIVING SPECIFICATION TO IMPLEMENT:**
---
{specification_text}
---

**CONTEXT:**
- **Target File Path:** `{target_file_path}`
- **Current Timestamp:** `{now_iso()}`

**YOUR TASK:**
Read the provided Living Specification and generate the complete, production-ready Python code that implements it. The code must be a perfect, 'So Below' reflection of the 'As Above' specification.

**CONSTRAINTS:**
{constraints_text}

**OUTPUT FORMAT:**
You MUST respond with a single JSON object containing two keys: 'file_path' and 'code'. Do not include any other text or explanations outside of this JSON object.

Example:
{{
  "file_path": "{target_file_path}",
  "code": "#!/usr/bin/env python3\\n'''Module docstring'''\\n\\nimport os\\n\\nclass MyClass:\\n    '''Class docstring'''\\n    pass"
}}

Generate the complete implementation now:
"""
        return prompt

    def _parse_llm_response(self, response_text: str) -> Dict[str, str]:
        """
        Safely parses the LLM's JSON output to extract the code.

        Args:
            response_text: The raw text response from the LLM.

        Returns:
            Dictionary with 'file_path' and 'code' keys.

        Raises:
            ValueError: If the response is not a valid JSON object or is missing required keys.
        """
        try:
            # Clean the response to extract only the JSON block
            json_start = response_text.find('{')
            json_end = response_text.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                raise ValueError("No JSON object found in the LLM response.")
            
            json_str = response_text[json_start:json_end]
            data = json.loads(json_str)

            if 'file_path' not in data or 'code' not in data:
                raise ValueError("LLM response is missing required 'file_path' or 'code' keys.")
            
            return {
                "file_path": data['file_path'],
                "code": data['code']
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to decode LLM response as JSON: {e}")
            raise ValueError(f"LLM response was not valid JSON. Response snippet: {response_text[:200]}")
        except Exception as e:
            logger.error(f"Unexpected error parsing LLM response: {e}")
            raise

    def _validate_syntax(self, code: str) -> Tuple[bool, Optional[str]]:
        """
        Performs a basic syntax check on the generated code using Python's AST module.

        Args:
            code: The Python code to validate.

        Returns:
            Tuple containing a boolean for validity and an error message if invalid.
        """
        try:
            ast.parse(code)
            return True, None
        except SyntaxError as e:
            return False, f"Syntax error at line {e.lineno}, offset {e.offset}: {e.msg}"
        except Exception as e:
            return False, f"Unexpected validation error: {e}"


# ============================================================================
# CONVENIENCE FUNCTION
# ============================================================================

def generate_code_from_specification_file(
    specification_path: str,
    model: Optional[str] = None
) -> Dict[str, Any]:
    """
    Convenience function to run the Genesis Agent on a specification file.

    Args:
        specification_path: The path to the .md specification file.
        model: The LLM model to use.

    Returns:
        The result from the Genesis Agent.
    """
    try:
        spec_file = Path(specification_path)
        if not spec_file.exists():
            raise FileNotFoundError(f"Specification file not found: {specification_path}")
        
        specification_text = spec_file.read_text()
        
        # Infer target file path from spec name
        target_filename = spec_file.stem.replace(' ', '_').replace('-', '_').lower() + ".py"
        target_file_path = f"Three_PointO_ArchE/{target_filename}"
        
        agent = GenesisAgent(model=model)
        return agent.generate_code_from_spec(specification_text, target_file_path)
        
    except Exception as e:
        logger.error(
            f"Convenience function failed for spec '{specification_path}': {e}", 
            exc_info=True
        )
        return {"error": str(e)}

